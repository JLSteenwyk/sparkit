```json
{
  "proposal_id": "SPARKIT-HIT-001",
  "title": "Hypothesis-Iterative Triangulation: Evidence-Conditioned Answer Hypothesis Refinement with Citation-Graph Gap Filling and Cross-Encoder Re-ranking",
  "distinctive_angle": "Generate grounded answer hypotheses after initial retrieval (not before), drive a dedicated gap-filling retrieval round using hypothesis-conditioned queries, re-rank all candidate passages with a cross-encoder instead of token-overlap, build a per-hypothesis evidence ledger, and replace the generic claim-cluster synthesis prompt with a structured hypothesis evaluation court that forces explicit evidence adjudication per candidate before committing to a final answer.",
  "summary": "SPARKIT retrieves evidence through intent-based rounds (primary, methods, adversarial, reference) and synthesizes via claim clustering and section summaries. For hard STEM QA this misses three critical capabilities: (1) targeted retrieval conditioned on specific candidate answers rather than generic topic coverage, (2) passage relevance scoring that captures semantic entailment rather than keyword co-occurrence, and (3) synthesis structured around hypothesis evaluation rather than summarization under self-imposed organization pressure. This proposal adds: a post-round-1 hypothesis generator using a fast cheap model, a hypothesis-gap retrieval round that queries each candidate answer for confirming and contradicting evidence, a Semantic Scholar 1-hop citation-graph expander for the top-3 retrieved papers, a cross-encoder re-ranker to replace _record_relevance_score() token-overlap, a per-hypothesis evidence ledger builder, and a restructured synthesis prompt that presents pre-organized support/contradict tables per hypothesis and asks the model to adjudicate before selecting the winner.",
  "reasoning": "Hard benchmark questions such as HLE STEM require distinguishing near-identical candidate answers that differ in subtle empirical details. SPARKIT's current _record_relevance_score() (engine.py) ranks records by title/abstract token overlap with the query, which cannot distinguish a paper about 'protein folding energy barriers' from one about 'protein folding kinetic pathways' when the query is 'protein folding mechanisms'. A cross-encoder scores the question-passage pair jointly, capturing entailment direction - this single change consistently moves gold-evidence from rank 8-12 to rank 1-3 in RAG benchmarks, which directly controls what enters the synthesis prompt. The hypothesis generator is novel because it runs after round-1 retrieval, meaning hypotheses are grounded in actual retrieved evidence rather than LLM priors alone, reducing the risk of generating unanchored candidates. Hypothesis-conditioned gap queries fix a systematic gap in SPARKIT's current plan: the adversarial intent round retrieves contradicting evidence for the question generally, but not evidence that specifically contradicts each candidate answer - a much more targeted signal. The citation-graph expander targets the 'follow-up study that corrected the original result' pattern common in hard research questions: the exact measurement or retraction that resolves ambiguity is often in papers that cite the retrieved papers, one hop deep. Semantic Scholar's graph API is already authenticated in adapters.py, making this low-friction. Finally, the hypothesis evaluation court synthesis prompt eliminates the organizational burden placed on the LLM at inference time: instead of 'here are evidence clusters, please synthesize', it presents 'here is evidence for H1 vs against H1, score it; same for H2, H3; pick the winner'. This structured format reduces hallucination under ambiguity, which is the primary failure mode on hard QA.",
  "expected_impact": {
    "accuracy_delta_pct_points": 4.5,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add cross-encoder passage re-ranker in retrieval_service: after _select_records_for_ingestion() scores candidates, call a cross-encoder (Cohere re-rank API or local cross-encoder/ms-marco-MiniLM-L-6-v2 via sentence-transformers) with the original question as query and each record's title+abstract as document. Replace the token-overlap primary rank key with cross-encoder score; keep token-overlap as tiebreaker. Batch all candidates in a single API call. Gate behind SPARKIT_ENABLE_CROSSENCODER_RERANK env var (default 0; set 1 in research_max EffortProfile). Add COHERE_API_KEY to provider registry.",
      "owner": "retrieval_service/aggregator.py + engine.py:_select_records_for_ingestion",
      "effort": "medium"
    },
    {
      "step": "Add _generate_answer_hypotheses() in engine.py: after round-1 retrieval records are ingested, call a fast cheap model (Kimi k2-turbo or Gemini Flash) with the question and top-5 evidence title+abstract snippets. Prompt instructs: generate 3-4 distinct, specific, falsifiable candidate answers; include one adversarial hypothesis that contradicts the most salient retrieved claim; format as numbered list. Parse into list[str]. If parsing fails or returns < 2 hypotheses, skip hypothesis mode for this question and fall back to existing synthesis. Gate behind SPARKIT_ENABLE_HYPOTHESIS_GEN (default 0; 1 in research_max).",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_hypothesis_gap_queries() in engine.py: for each hypothesis H_i, generate 2 retrieval queries - (a) '[question_stem] [H_i]' for confirming evidence, (b) '[question_stem] alternative to [H_i]' for discriminating evidence. Insert these as a new 'hypothesis' intent round in the RetrievalPlan after the existing rounds. Execute via the existing _execute_retrieval_round() path. Merge deduplicated results into the shared evidence pool using _dedupe_records().",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add Semantic Scholar citation-graph expansion in retrieval_service/adapters.py: for the top-3 records by cross-encoder score after round-1, extract their Semantic Scholar paper ID from the record URL or DOI. Call /graph/v1/paper/{id}/citations?fields=title,abstract,year,externalIds with a 1s rate-limiter (token bucket). Filter returned papers: abstract token overlap with question > 0.12 AND year >= 2008. Add to candidate pool before the ingestion target selection step. Gate behind SPARKIT_ENABLE_CITATION_HOP (default 0; 1 in research_max). Implement per-session rate limiter as a module-level threading.Semaphore to avoid Semantic Scholar 429s.",
      "owner": "retrieval_service/adapters.py + aggregator.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_hypothesis_evidence_ledger() in engine.py: after full ingestion, for each hypothesis H_i, score each stored passage against H_i using cross-encoder scores (or token overlap as fallback). Classify each passage as support (score > 0.65), contradict (score < 0.35), or neutral. Build dict {hypothesis_i: {support: [top-4 passages], contradict: [top-2 passages]}}. Cap total passages at 4 support + 2 contradict per hypothesis to stay within token budget. Store ledger in the run trace as 'hypothesis_evidence_ledger' stage.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Restructure _build_synthesis_prompt() to add hypothesis evaluation court mode: when hypotheses and ledger are available (and question is not already MCQ-detected), replace claim-cluster + section-summary format with a structured evaluation court. For each hypothesis: present support snippets as bullet quotes, present contradict snippets, ask model to output VERDICT: SUPPORTED/CONTRADICTED/INSUFFICIENT and CONFIDENCE: 0-10 per hypothesis. After all hypotheses are evaluated, select the hypothesis with highest (support_count - contradict_count) * confidence as the answer basis. The existing MCQ pipeline is unaffected. Add a token budget guard: if (num_hypotheses * 6 passages * avg_passage_tokens) > synthesis_max_tokens * 0.7, reduce to top-2 hypotheses.",
      "owner": "orchestrator/engine.py:_build_synthesis_prompt",
      "effort": "high"
    },
    {
      "step": "Update calibration.py to track three new features: hypothesis_count (number of generated hypotheses), hypothesis_winner_margin (top hypothesis net score minus second-place net score), and evidence_ledger_coverage (fraction of hypotheses with >= 2 support passages). These feed into the existing calibrated confidence computation and allow correlation analysis between margin width and answer correctness post-benchmark.",
      "owner": "orchestrator/calibration.py",
      "effort": "low"
    },
    {
      "step": "Extend eval_service benchmark harness: add a HLE-gold-20 ablation run that tests all four flags independently (CROSSENCODER only, HYPOTHESIS_GEN only, CITATION_HOP only, all three combined) in research_max mode. Output per-question accuracy, gold-evidence rank improvement, hypothesis_recall_rate (correct answer subsumed by a generated hypothesis), and cost_per_question. Add these metrics to the existing baseline manifest schema.",
      "owner": "eval_service",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Cross-encoder passage re-ranking to replace token-overlap: _record_relevance_score() in engine.py scores candidates by title/abstract token overlap with the query string. This fails for hard STEM questions where the relevant paper uses different terminology (e.g., 'activation energy landscape' vs 'transition state barrier'). A cross-encoder jointly encodes the question-passage pair and scores semantic entailment direction. Replacing token-overlap with cross-encoder scores as the primary ranking key ensures that the ingestion target documents are the ones most semantically relevant to the question, not just the ones that share surface keywords - directly controlling what evidence enters the synthesis prompt.",
    "Hypothesis-conditioned gap-filling retrieval round: after round-1 retrieval, SPARKIT has enough evidence to generate specific candidate answers. These hypotheses, when used as query components, produce highly targeted retrieval for evidence that discriminates between candidates. The current adversarial round ('adversarial' intent) retrieves papers that generally contradict the question topic, but hypothesis-gap queries like '[question_stem] [specific candidate A]' vs '[question_stem] [specific candidate B]' retrieve passages that directly compare the two candidates, which is far more useful for hard questions where the answer hinges on a precise experimental detail.",
    "Semantic Scholar 1-hop citation-graph expansion for top-3 retrieved papers: the Semantic Scholar adapters.py already authenticates against the Semantic Scholar Graph API. For hard research questions, the paper that resolves ambiguity is often a follow-up study that either confirms or corrects the original. By fetching citations of the top-3 retrieved papers (1 hop deep), filtered by abstract relevance to the question and recency, SPARKIT accesses this layer of literature that intent-based queries systematically miss - queries cannot retrieve papers whose titles/abstracts describe the original result, but the resolution is in papers citing them.",
    "Per-hypothesis evidence alignment via evidence ledger: current evidence organization via _build_claim_clusters() groups passages by document topic tokens and presents them as thematic clusters to the synthesis model. This places the burden of organizing evidence-to-hypothesis alignment on the LLM at synthesis time, under token pressure. Building an explicit evidence ledger that pre-aligns passages to each candidate hypothesis removes this burden: the synthesis model receives pre-organized support/contradict tables and adjudicates rather than self-organizes, reducing the hallucination-under-ambiguity failure mode that dominates on hard STEM questions."
  ],
  "evaluation_plan": [
    "Accuracy ablation on HLE-gold-20: run four conditions in research_max mode - baseline (no new flags), CROSSENCODER only, HYPOTHESIS_GEN+LEDGER only, all three combined. Measure exact-match accuracy per condition. Gate success: combined condition achieves >= +2pp over baseline, with no individual component causing > -1pp regression when run alone. Track cost_per_question for each condition.",
    "Gold-evidence rank improvement test: for each HLE-gold-20 question, manually identify the 'gold-evidence document' (the document whose snippet contains the correct answer token or fact). Measure the rank of the gold-evidence document in the candidate list before vs. after cross-encoder re-ranking. Success criterion: mean rank of gold-evidence document improves from pre-rerank to post-rerank by >= 2 positions across the 20 questions, measured as paired Wilcoxon test p < 0.10.",
    "Hypothesis recall rate measurement: for each HLE-gold-20 question where hypothesis generation succeeds, check whether the correct answer string (normalized) is subsumed by or textually entailed by at least one generated hypothesis. Track hypothesis_recall_rate = (questions where correct answer subsumed) / (total questions with successful generation). Gate: hypothesis_recall_rate >= 65%; if below threshold, hypothesis mode defaults off and a WARNING is emitted in the run trace.",
    "Citation-hop contribution rate: for each HLE-gold-20 question where citation-hop is enabled, track whether at least one citation-hop document (fetched from the Semantic Scholar graph, not from primary retrieval) appeared in the final evidence ledger with a cross-encoder score >= 0.50 for the winning hypothesis. Gate: citation-hop documents contribute to the winning hypothesis evidence in >= 15% of questions, at < 4s additional latency per question (measured as p95 across 20 questions).",
    "Regression test on full routed_frontier_plus benchmark: after all changes are deployed, run the full routed_frontier_plus benchmark with hypothesis mode enabled only in research_max and disabled in routed_frontier. Verify: (a) no accuracy regression > 1pp in routed_frontier mode, (b) cost regression in routed_frontier mode < 5% (hypothesis mode off means no added cost), (c) the calibration.py hypothesis_winner_margin feature shows positive Spearman correlation with per-question correctness (rho >= 0.15), validating it as a useful confidence signal."
  ],
  "risks": [
    "Hypothesis generation quality bottleneck for highly technical questions: the fast cheap model (Kimi/Gemini Flash) used for hypothesis generation may produce vague or incorrect candidate answers for highly specialized STEM questions (e.g., quantum chemistry, advanced genomics), resulting in an evidence ledger misaligned to the actual answer space. Mitigate: implement hypothesis_recall_rate gate (>= 65%) that disables hypothesis mode per-question and falls back to existing claim-cluster synthesis when recall is too low; use top-5 evidence snippets in the hypothesis generation prompt to anchor candidates to retrieved literature rather than LLM priors.",
    "Token budget overrun in hypothesis evaluation court synthesis: 4 hypotheses * 6 passages * ~120 tokens/passage = ~2880 additional synthesis prompt tokens. Combined with existing evidence sections, this may exceed provider context windows or the synthesis_max_tokens budget, causing truncation or provider refusal. Mitigate: add a pre-synthesis token budget check that reduces to top-2 hypotheses if the full ledger exceeds 70% of synthesis_max_tokens; compress passages to first 2 sentences before including in synthesis prompt.",
    "Cross-encoder API latency and cost: Cohere re-rank at ~20-40 candidates per question adds an external API call with ~300-600ms latency. At $0.001/1K tokens for Cohere re-rank, cost is negligible (<$0.01/question), but the latency impact on routed_frontier mode (which has tighter SLAs) may be unacceptable. Mitigate: run cross-encoder re-ranking in parallel with document ingestion/parsing (it operates on metadata only, not full text); in non-research_max modes, skip re-ranking and retain token-overlap ranking.",
    "Semantic Scholar citation-graph rate limits and DOI coverage gaps: Semantic Scholar free tier allows 1 req/s; 3 citation-hop requests per question at 10 concurrent questions = 30 req/s, well above the limit. Additionally, many SPARKIT records come from arXiv or Crossref and may not have Semantic Scholar IDs, resulting in 404s and wasted time. Mitigate: implement per-session token-bucket rate limiter in adapters.py; skip citation-hop for records missing a Semantic Scholar ID or DOI; cache citation-hop results by paper ID in Postgres corpus_documents to avoid re-fetching across runs.",
    "Evidence ledger adversarial hypothesis bias inversion: the explicit adversarial hypothesis (the one that contradicts the most salient retrieved claim) is intended to surface disconfirming evidence. However, if the correct answer IS the counterintuitive one (common in hard benchmark questions designed to test against LLM priors), the adversarial hypothesis may actually be the correct one, but the evidence ledger will score it lower because mainstream literature discusses the common-but-wrong answer more than the uncommon-but-correct answer. Mitigate: weight citation-hop documents (which tend to contain corrections and follow-up studies) at 1.5x in the evidence ledger scoring for the adversarial hypothesis specifically; monitor adversarial-hypothesis win rate post-benchmark to detect systematic suppression."
  ]
}
```
