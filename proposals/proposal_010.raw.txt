```json
{
  "proposal_id": "SPARKIT-PROP-2026-001",
  "title": "Answer-First Claim-Coverage Retrieval: Predictive Anchor-Claim Targeting with Coverage-Gated Multi-Hop Evidence Chains",
  "distinctive_angle": "Invert SPARKIT's query-first retrieval pipeline by predicting the exact factual anchor-claims a correct answer must contain before any retrieval begins, measuring evidence coverage of those anchor-claims after each retrieval round, and using uncovered claims to generate targeted gap-fill queries — replacing novelty-quantity adaptive gating with a coverage-completeness gate that stops only when the question's evidential needs are demonstrably met or budget is exhausted.",
  "summary": "SPARKIT currently plans retrieval queries from the question text alone, then adaptively gates further rounds on new-document novelty and marginal quality gain (SPARKIT_ADAPTIVE_MIN_NEW_DOCS, SPARKIT_ADAPTIVE_MIN_QUALITY_GAIN). For hard questions, novelty is a poor proxy for evidential completeness: the pipeline may fetch many novel documents that still leave critical sub-questions unanswered, or stop early because a retrieval round adds few new docs even when key claims remain uncovered. This proposal adds four coordinated changes: (1) a pre-retrieval Anchor-Claim Prediction step where a cheap LLM call decomposes the question into N=5-8 atomic factual claims the correct answer must ground; (2) a per-round Anchor Coverage Scorer that rates each anchor claim against the current evidence pool using token-overlap plus lightweight embedding cosine similarity; (3) an Anchor-Gap Query Generator that replaces fixed 'reference' and 'gap_fill' intent-query slots with dynamically generated queries targeting uncovered anchors; and (4) a Coverage-Gated Adaptive Decision that continues retrieval if any anchor has coverage below a threshold (default 0.65) even when novelty is low, and halts early if all anchors exceed the threshold even when novelty is high. For MCQ questions, anchor-claims are generated per option, enabling option-level coverage gating that can eliminate poorly-supported options before the expensive LLM scoring round.",
  "reasoning": "The engine's adaptive gate in engine.py (lines 1313-1454) checks (new_unique_docs < SPARKIT_ADAPTIVE_MIN_NEW_DOCS AND quality_gain < SPARKIT_ADAPTIVE_MIN_QUALITY_GAIN). This is an indirect signal: it measures evidence supply novelty, not demand satisfaction. Hard questions fail precisely because the retrieval plan — generated once upfront via _build_retrieval_plan — does not know what gaps remain after partial retrieval. The intent_queries dict has fixed 'primary', 'methods', 'adversarial', 'reference' slots pre-generated at planning time, so every round fires queries that may be redundant given evidence already in hand. By predicting anchor claims first, SPARKIT acquires an explicit coverage target, making retrieval demand-driven rather than supply-driven. Gap-fill queries generated from uncovered anchors are semantically closer to the missing information than generic 'reference' intent queries, increasing the probability that the next round retrieves the specific passage needed. Coverage gating prevents over-early stopping (current risk with novelty gating) and allows over-early stopping when coverage is already complete (cost saving). For MCQ questions, per-option anchor claims map directly onto the elimination → scoring → adjudication chain: an option whose anchors are uncovered can be eliminated cheaply without an LLM scoring call, reducing expensive provider usage on irrelevant options.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "mixed",
    "latency_impact": "mixed",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _predict_anchor_claims(question, answer_choices, plan, provider) in engine.py. Call after _build_retrieval_plan. Use a cheap provider (Haiku / grok-4-fast) with a structured prompt: 'List the N=5 atomic factual claims a correct answer to this question must be grounded in. For MCQ, include 1-2 discriminative claims per option.' Return List[AnchorClaim] where AnchorClaim has fields: text (str), option_tag (str|None for MCQ), priority (1-3). Cache in RunState as state.anchor_claims.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Implement _score_anchor_coverage(anchor_claims, selected_records) in engine.py. For each anchor claim, compute: (a) best token-overlap score against all record titles+abstracts using the existing _score_record_relevance helper; (b) if numpy/scikit-learn available, cosine similarity against TF-IDF vectors of record abstracts. Combine: coverage_score = 0.6*token_overlap + 0.4*cosine_sim. Threshold: covered if score >= SPARKIT_ANCHOR_COVERAGE_THRESHOLD (default 0.45). Return AnchorCoverageReport with per-anchor scores and a global coverage_ratio = covered_count/total_anchors.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Implement _generate_gap_fill_queries(uncovered_anchors, focus_terms, provider) in engine.py. Called after _score_anchor_coverage if uncovered_anchors is non-empty. Prompt: 'Given these uncovered factual claims, generate 2 targeted academic search queries per claim that would retrieve passages directly supporting or refuting them. Prefer specific entity names and technical terms.' Inject returned queries into the 'gap_fill' and 'reference' intent slots for the next retrieval round, replacing the pre-planned static queries.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify _should_stop_retrieval_early in engine.py to accept anchor_coverage_ratio as a parameter. Add condition: if anchor_coverage_ratio < SPARKIT_ANCHOR_MIN_COVERAGE (default 0.65) and stage_idx < adaptive_max_rounds, return False (do not stop) regardless of novelty/quality signals. Add inverse condition: if anchor_coverage_ratio >= SPARKIT_ANCHOR_FULL_COVERAGE (default 0.90) and stage_idx >= adaptive_min_rounds, return True with reason='anchor_coverage_complete'. Emit both metrics in the retrieval_adaptive_gate artifact.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "For MCQ: modify _run_mcq_elimination_round to skip LLM elimination call for any option whose per-option anchor_coverage_ratio < SPARKIT_MCQ_ANCHOR_ELIMINATE_THRESHOLD (default 0.15). Mark these options as ELIMINATE directly in the elimination_result dict with reason='insufficient_anchor_coverage'. This avoids spending a scoring-round provider call on options with near-zero evidence support.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Extend synthesis prompts to include an anchor coverage summary table. In _build_synthesis_context, append a section: 'Anchor Claim Coverage: [claim_text] — [covered|partial|uncovered] ([score:.2f])'. Instruct the LLM to treat uncovered anchors as high-uncertainty and flag them with explicit hedging in the answer. This propagates coverage information into generated text quality.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add anchor_coverage_ratio and per_anchor_scores to CalibrationFeatures in calibration.py. Update the confidence formula to include: +0.10 * anchor_coverage_ratio and -0.08 * (1 - anchor_coverage_ratio) replacing 0.05 * min(1.0, evidence_count/10) to make confidence more directly tied to evidential completeness rather than raw document count.",
      "owner": "orchestrator/calibration.py",
      "effort": "low"
    },
    {
      "step": "Add environment variables to configuration.md: SPARKIT_ANCHOR_PREDICTION_ENABLED (default 1), SPARKIT_ANCHOR_N_CLAIMS (default 6), SPARKIT_ANCHOR_COVERAGE_THRESHOLD (default 0.45), SPARKIT_ANCHOR_MIN_COVERAGE (default 0.65), SPARKIT_ANCHOR_FULL_COVERAGE (default 0.90), SPARKIT_MCQ_ANCHOR_ELIMINATE_THRESHOLD (default 0.15). Add anchor_coverage_ratio to observability run metadata.",
      "owner": "docs/configuration.md + engine.py",
      "effort": "low"
    },
    {
      "step": "Write unit tests in test_synthesis_quality.py: test anchor claim prediction structure, test coverage scorer with synthetic records, test gap-fill query injection replaces static intent queries, test MCQ elimination fires for low-coverage options, test adaptive gate does not stop when anchor_coverage_ratio < 0.65 even if novelty is low.",
      "owner": "services/orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Demand-driven gap-fill query generation: replace static 'reference' and 'gap_fill' intent-query slots (pre-generated once by _build_retrieval_plan before any evidence is seen) with dynamically generated queries targeting only the specific anchor claims that remain uncovered after each round. Queries are semantically precise because they are generated from the specific missing claim text rather than the original question, increasing hit rate in academic search APIs.",
    "Coverage-completeness adaptive gating: the current gate in engine.py lines 1313-1454 halts on low new-document novelty, which can prematurely terminate retrieval when the few uncovered anchor claims require niche sources not represented in earlier rounds. The new gate continues retrieval specifically when high-priority anchors remain uncovered, even if aggregate novelty is low, ensuring that hard sub-questions receive additional targeted rounds rather than being silently dropped.",
    "Per-option anchor targeting for MCQ retrieval: generate option-specific anchor claims during the planning phase so that retrieval rounds include queries targeting discriminative facts for each MCQ choice. The aggregator's per-source cap and diversity logic already distributes results across sources; option-anchored queries ensure that the result set contains material relevant to distinguishing between options rather than only material supporting the most popular option.",
    "Cross-document reference chain following for uncovered anchors: when a retrieved abstract cites a specific prior work by name (pattern: Author et al., YEAR) and the corresponding anchor claim remains uncovered, generate a targeted query using that citation as search terms against Semantic Scholar and Crossref. This implements lightweight one-hop reference following without requiring full citation graph infrastructure, exploiting the fact that hard questions often require evidence from seminal papers cited-but-not-fetched in the primary round.",
    "Anchor-weighted record selection for ingestion: modify the document selection step (_select_docs_for_ingestion) to boost records whose abstract tokens overlap with uncovered anchor claim text, scoring them above records with high generic relevance but low anchor relevance. This ensures ingestion slots are spent on documents most likely to resolve remaining evidential gaps rather than on additional high-quality but redundant documents."
  ],
  "evaluation_plan": [
    "Anchor coverage rate at synthesis time: after implementing, log anchor_coverage_ratio for every HLE Gold Bio/Chem run. Compare mean coverage ratio between runs that answered correctly vs. incorrectly. Hypothesis: correct answers will have anchor_coverage_ratio >= 0.70, wrong answers <= 0.50. If the distributions are not separated, the anchor prediction quality may need improvement (check anchor claim specificity).",
    "Accuracy delta on HLE-25 benchmark: run the current SPARKIT pipeline (baseline) and the modified pipeline (with anchor-coverage gating) on the same HLE-25 balanced subset used in the dce7d9e commit. Compare accuracy@1 and accuracy@3 across both modes. Additionally compare accuracy stratified by task type (mechanism, numerical, comparative, factual) to identify which question types benefit most from anchor-coverage-driven retrieval.",
    "Gap-fill query hit rate: instrument _generate_gap_fill_queries to log for each dynamically generated query whether the subsequent retrieval round returned at least one record with anchor_coverage improvement > 0.15 for the targeted anchor. Target: gap-fill queries should achieve > 50% hit rate on previously uncovered anchors. Below 40% suggests the query generation prompt needs to be more specific.",
    "MCQ option elimination accuracy: for MCQ questions in the benchmark, compare the set of options eliminated by anchor-coverage (coverage < threshold) against the set of incorrect options. Compute precision (fraction of anchor-eliminated options that are actually wrong) and recall (fraction of wrong options caught by anchor elimination). Target: precision > 0.80 to avoid incorrectly eliminating correct options. If precision < 0.80, raise SPARKIT_MCQ_ANCHOR_ELIMINATE_THRESHOLD.",
    "Cost and latency impact: measure total provider cost and end-to-end latency for baseline vs. modified pipeline on the HLE-25 subset. The anchor prediction call adds ~1 cheap LLM call; coverage-complete early stopping should reduce rounds for easy questions. Target: net cost change < +15% on average, with > 20% cost reduction on questions where all anchors are covered after round 2.",
    "Calibration improvement: compare ECE (Expected Calibration Error) and Brier score between baseline and modified pipeline. The updated calibration formula incorporating anchor_coverage_ratio should improve ECE by reducing overconfidence on low-coverage answers. Target: ECE reduction > 0.03 on the HLE benchmark."
  ],
  "risks": [
    "Anchor claim prediction quality: if the cheap provider (Haiku / grok-4-fast) generates vague or incorrect anchor claims, the coverage scoring will be noisy and gap-fill queries will target the wrong information. Mitigation: add a specificity check — filter anchor claims shorter than 10 tokens or containing fewer than 2 technical terms; fall back to the existing static retrieval plan if fewer than 3 valid anchors are generated.",
    "Coverage scorer precision: the token-overlap + TF-IDF cosine similarity approach may over-score abstract matches (e.g., a paper about a different enzyme being scored as covering an anchor about a specific enzyme). This causes false coverage confidence and premature gating. Mitigation: use bigram overlap rather than unigram in the coverage scorer; add a minimum-abstract-length filter (skip records with fewer than 50 abstract tokens); monitor via the gap-fill hit rate evaluation metric.",
    "Latency increase from extra LLM call: anchor prediction adds one LLM round-trip before retrieval begins, and gap-fill query generation adds one call per round where anchors remain uncovered. Under tight latency budgets (max_latency_s < 120s), this may crowd out synthesis quality. Mitigation: gate anchor prediction on budget — only activate when max_cost_usd >= 2.0 and max_latency_s >= 90s; use the fastest available provider for both calls.",
    "MCQ option over-elimination: if anchor coverage scoring is too aggressive, a correct option may be eliminated before the LLM scoring round, hard-failing the question. Mitigation: the SPARKIT_MCQ_ANCHOR_ELIMINATE_THRESHOLD default (0.15) is set conservatively low; log eliminated option IDs in the trace for post-hoc audit; add a safety check that never eliminates all options via anchor coverage alone (always preserve at least 2 for the LLM round).",
    "Anchor claims leaking question structure into retrieval queries: for questions where the correct answer is one of the anchor claims themselves, the gap-fill queries may be semantically equivalent to stating the answer, which could bias retrieval toward confirming sources. This is acceptable for factual recall but may reduce calibration validity. Mitigation: frame anchor claims as 'what must be true' rather than 'what is the answer'; the adversarial retrieval round already targets contradicting evidence.",
    "Integration complexity with research_max mode: research_max uses a five-round pipeline with dual solvers and a separate adjudication judge; inserting anchor coverage gating between rounds requires careful state threading (RunState must carry AnchorCoverageReport across the dual-solver branches). Mitigation: implement anchor coverage as a read-only annotation on RunState with no branching logic in the dual-solver paths; coverage gating applies only to the retrieval phase, which completes before dual synthesis begins."
  ]
}
```
