I now have sufficient depth to produce the proposal. Here it is:

```json
{
  "proposal_id": "SPARKIT-P-EDAG-001",
  "title": "Evidence Dependency Graph with Forward-Citation Traversal and Orphan-Node Gap Retrieval",
  "distinctive_angle": "Replace SPARKIT's flat sub-claim decomposition and static multi-round queries with a dependency graph of factual premises. After each ingestion round, map evidence onto graph nodes; nodes with no evidence coverage ('orphan nodes') trigger dedicated targeted gap-retrieval. Simultaneously exploit Semantic Scholar's /paper/{id}/citations endpoint—not yet used anywhere in adapters.py—to traverse forward from high-confidence anchor papers to their citing literature, finding corroborating or superseding work invisible to keyword search.",
  "summary": "SPARKIT currently decomposes questions into flat sub-claims (ResearchPlan.sub_claims) and runs three fixed retrieval rounds (primary, gap-fill, adversarial) whose queries are generated once upfront with no feedback loop from what was actually found. For very hard questions this creates two failure modes: (1) a generic gap-fill query like '{question} limitations' is too broad to plug specific missing evidence nodes, and (2) the most relevant corroborating papers are never retrieved because they postdate or don't lexically match the original query but do cite the anchor papers already found. This proposal adds (a) a one-shot LLM dependency-graph pass that structures sub-claims into root and derived nodes with expected evidence type per node, (b) post-ingestion orphan detection that feeds node-specific targeted queries into subsequent retrieval rounds, (c) forward-citation traversal using the Semantic Scholar citations API to expand from anchor DOIs into their citing papers, (d) a cross-option discriminator retrieval pass for MCQ questions that queries specifically for passages that mention multiple answer options simultaneously, and (e) temporal evidence stratification that separates retrieved records into year cohorts and weights the most recent cohort's evidence higher in _record_relevance_score.",
  "reasoning": "Hard benchmark questions fail primarily because the evidence found is either too general (doesn't address the specific sub-claim needed to derive the answer) or too old (a more recent paper updates the finding but doesn't share keywords with the original query). The dependency graph addresses the first problem by making evidence gaps explicit and query-targetable rather than relying on the heuristic '{question} limitations'. Forward-citation traversal addresses the second problem by navigating the citation graph rather than keyword space—papers that cite anchor papers are by definition relevant to the same topic and are often newer. The discriminator retrieval for MCQ targets the single most valuable class of evidence for option elimination: passages that simultaneously discuss multiple candidate answers, which existing _build_option_evidence_packs cannot surface because it builds per-option lexical packs independently. Temporal stratification addresses the calibration problem where old consensus evidence and new contradicting evidence are treated identically; stratifying lets the synthesizer know whether it is dealing with settled or evolving science. All four mechanisms target distinct failure modes that the current adaptive gating (SPARKIT_ADAPTIVE_MIN_NEW_DOCS, SPARKIT_ADAPTIVE_MIN_QUALITY_GAIN) cannot detect because the gating only measures novelty volume, not structural evidence completeness.",
  "expected_impact": {
    "accuracy_delta_pct_points": 9,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add DependencyGraph dataclass alongside ResearchPlan in engine.py. Fields: nodes=[{node_id, claim_text, depth, expected_evidence_type}], edges=[{from_id, to_id, relation}]. Add _decompose_to_dependency_graph() that calls the planning provider with a new prompt asking the LLM to output JSON with these fields. Invoke this instead of _decompose_question() in research_max mode.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "After each ingestion round, add _map_evidence_to_graph_nodes(graph, claim_texts) that uses token overlap (reuse _tokenize from engine.py) to assign each retrieved claim to the closest graph node by Jaccard similarity. Nodes with zero assigned claims and depth <= 1 are 'orphans'. Return orphan_nodes list.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify _build_round_queries_from_plan() to accept orphan_nodes from the previous round. For each orphan node, generate a dedicated query: '{node.claim_text} {node.expected_evidence_type} experimental evidence'. Prepend these targeted orphan queries before the generic gap-fill queries in round 2 and round 3. Cap orphan queries at 4 per round to bound API cost.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add search_semantic_scholar_citations(paper_id, max_results) to services/retrieval_service/app/adapters.py. Use endpoint https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations with fields=title,abstract,year,externalIds,url. Map response to LiteratureRecord with source='semantic_scholar_citations'. Add search_semantic_scholar_references(paper_id) similarly for backward traversal.",
      "owner": "retrieval_service/adapters.py",
      "effort": "medium"
    },
    {
      "step": "After round 1 ingestion, collect DOIs from the top-3 highest-relevance ingested records (by _record_relevance_score). Extract Semantic Scholar paper_ids from their DOIs using the /graph/v1/paper/DOI:{doi} lookup. Call search_semantic_scholar_citations for each. Inject resulting records into round 2 retrieval pool via a new 'anchor_expansion' sub-round within _execute_retrieval_round. Add SPARKIT_ENABLE_CITATION_TRAVERSAL env var (default 0).",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "For MCQ questions, add _build_discriminator_queries(stem, answer_choices) that generates queries pairing each distinct option combination: '{choice_A_text} vs {choice_B_text} {stem_keywords}'. Run these as an additional sub-round before synthesis. Feed resulting claim texts into a new _identify_discriminator_claims() that scores each claim by how many distinct option token sets it overlaps (higher = more discriminating). Surface top-5 discriminator claims as a special section in _build_mcq_option_judge_prompt.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "In _record_relevance_score (engine.py line 121) add year_cohort_weight: split records into cohorts [pre-2020, 2020-2022, 2023+]. Apply cohort multiplier 1.0 / 1.15 / 1.35 respectively to the base relevance score. Add SPARKIT_RECENCY_BOOST env var (default '1.35,1.15,1.0' for 3 cohorts newest-first) to allow tuning without code changes.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Extend _build_synthesis_prompt to include a 'Evidence Gap Warning' section when orphan nodes remain after all retrieval rounds. List the unresolved node claim_texts. This lets the synthesis LLM explicitly acknowledge missing evidence rather than confabulate, which improves calibration and reduces hallucination on hard questions.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add unit tests in test_synthesis_quality.py: test_orphan_node_detection_identifies_uncovered_claims, test_citation_traversal_records_are_injected_into_round2, test_discriminator_claims_score_higher_than_single_option_claims, test_recency_boost_weights_recent_cohort_higher.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "low"
    },
    {
      "step": "Benchmark: run full HLE-gold bio_chem 25-question set (benchmarks/hle_gold_bio_chem/questions_bio13_chem12_hle25.json) with research_max mode before and after the change under a single config (single_anthropic) with SPARKIT_ENABLE_CITATION_TRAVERSAL=1. Compare accuracy, per-run cost, and latency using the existing drift-check workflow (scripts_drift_check.py).",
      "owner": "eval_service/direct_call_runner.py + scripts_drift_check.py",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Orphan-Node Gap Retrieval: After round 1 ingestion, compute which dependency-graph nodes have zero evidence coverage and generate semantically precise queries for exactly those nodes (e.g., 'CRISPR off-target rate NGS measurement' instead of generic '{question} limitations'). This replaces the static gap-fill template in _build_round_queries() which cannot know which specific sub-claim is missing evidence.",
    "Forward-Citation Traversal via Semantic Scholar /citations endpoint: Identify the top-3 anchor papers from round 1 by _record_relevance_score, resolve their DOIs to Semantic Scholar paper_ids, then fetch papers that cite them. This surfaces newer work (post-2023) that shares the same research thread but does not share keywords with the original query—a class of evidence that keyword-based retrieval structurally cannot find.",
    "Cross-Option Discriminator Retrieval for MCQ: Generate explicit versus queries for MCQ answer choice pairs ('{option_A_text} compared to {option_B_text}') and retrieve passages that discuss both options simultaneously. These discriminator passages are the highest-information evidence for option elimination because they directly contrast the candidates. The current _build_option_evidence_packs builds per-option packs independently and cannot surface cross-option evidence.",
    "Temporal Cohort Weighting in _record_relevance_score: Apply a graduated recency multiplier (1.35x for 2023+, 1.15x for 2020-2022, 1.0x for pre-2020) to the relevance score before document selection in _select_records_for_ingestion. For rapidly evolving fields (bio, chemistry, AI), more recent evidence is structurally more likely to reflect current consensus. The existing recency_bonus in _record_relevance_score (line 130-131) caps at 0.25 and uses a 15-year linear ramp—insufficient to distinguish a 2023 paper from a 2021 paper.",
    "Dependency-Graph-Guided Retrieval Ordering: Execute retrieval queries in depth-first order from the dependency graph (root nodes first, derived nodes after). Root-node evidence, once retrieved and ingested, can be used to refine derived-node queries by extracting key entities and technical terms from root-node claims and appending them to derived-node query strings before the round executes."
  ],
  "evaluation_plan": [
    "Accuracy delta on HLE-gold-25: Run research_max / single_anthropic on benchmarks/hle_gold_bio_chem/questions_bio13_chem12_hle25.json with and without SPARKIT_ENABLE_CITATION_TRAVERSAL=1. The primary metric is fraction of questions answered correctly per the exact-match rubric. Target: >= +6 percentage points versus baseline on this subset.",
    "Orphan-node coverage rate: For each HLE-gold question in research_max mode, log orphan_node count after round 1 vs. after all rounds. Track fraction of orphan nodes that receive at least one covering claim by end of retrieval. Target: >= 70 percent of round-1 orphan nodes receive coverage by end of round 3, versus the current implicit 0 percent (orphan nodes are never explicitly tracked).",
    "Citation-traversal novelty rate: Among records returned by search_semantic_scholar_citations, measure what fraction are not duplicates of records already returned by keyword search (deduplication by DOI). Target: >= 40 percent novel records from citation traversal, confirming it is finding evidence invisible to keyword search.",
    "MCQ discriminator effectiveness: For MCQ questions where the discriminator retrieval fires, compare answer accuracy with vs. without the discriminator claims in the synthesis prompt (ablation). Target: >= +8 percentage points on MCQ subset when discriminator claims are present and non-empty.",
    "Hallucination rate via orphan-gap warning: Count how often the synthesis answer makes a specific factual claim about an entity that appears in an unresolved orphan node (indicating confabulation about un-evidenced material). Use a post-hoc LLM judge to score each answer against its orphan node list. Target: hallucination rate decreases by >= 15 percent versus baseline when the 'Evidence Gap Warning' section is present in the synthesis prompt.",
    "End-to-end cost regression check: The citation traversal adds up to 3 additional Semantic Scholar API calls (free tier) and at most 2 extra ingestion fetches per run. Monitor total_estimated_cost_usd from RunObservability.stage_metrics across 25 HLE runs. Verify cost increase is <= 15 percent versus baseline (citation traversal should not trigger expensive extra LLM synthesis passes)."
  ],
  "risks": [
    "Semantic Scholar rate limits: The /paper/{id}/citations endpoint allows 100 requests/second unauthenticated. With 3 anchor papers per run and potentially concurrent runs, rate limits could cause citation traversal to fail silently. Mitigation: wrap in _get_with_retry() (already in adapters.py) and add SPARKIT_ENABLE_CITATION_TRAVERSAL=0 default so the feature is opt-in.",
    "Dependency graph LLM call adds latency and token cost: The new _decompose_to_dependency_graph() pass is an extra LLM generation before retrieval begins. For a complex hard question, this adds roughly 1-3 seconds and 800-1200 input tokens. Mitigation: make this pass conditional on research_max mode only (which already has elevated cost/latency tolerance) and cache the graph structure in the run's trace so retries skip it.",
    "Orphan-node queries may over-specialize and fail to retrieve: If the dependency graph LLM produces overly technical node claim_text values (e.g., exact enzyme names with uncommon abbreviations), the targeted orphan queries may retrieve zero results from keyword-based adapters. Mitigation: apply _relax_query() to all orphan queries before dispatch, same as existing aggregator.py logic.",
    "Forward-citation traversal may retrieve tangentially related papers: Papers that cite a given anchor paper may be on adjacent topics (e.g., citing only the methodology, not the scientific finding). This could dilute retrieval quality. Mitigation: apply _record_relevance_score filter post-retrieval and admit only citation-traversal records scoring above the 50th percentile of round-1 records.",
    "MCQ discriminator queries may not produce coherent evidence: If the two compared options are very different entities, the versus query may retrieve papers that discuss one option entirely and only mention the other in passing. Mitigation: require discriminator claims to overlap both option token sets by at least 2 tokens each (reuse _tokenize from engine.py) before elevating them to the discriminator section of the synthesis prompt."
  ]
}
```
