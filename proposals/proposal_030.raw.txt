```json
{
  "proposal_id": "P-028-OPTEX",
  "title": "Option-Implied Claim Decomposition with Retrieval Vocabulary Self-Improvement and Multi-Passage Density Scoring",
  "distinctive_angle": "Two novel mechanisms absent from all 27 existing proposals are combined: (1) Abstract vocabulary self-improvement — after round-1 retrieval, run TF-IDF extraction over the retrieved abstract corpus to surface domain-specific terminology (IUPAC names, field-specific acronyms, benchmark dataset names) that SPARKIT's static synonym lists miss, then inject the top-N mined terms into all subsequent retrieval rounds as query enrichments; (2) Option-implied claim decomposition — parse each MCQ option into structured factual commitment triples (subject, predicate, object, unit/value) that the option asserts as true, generate 2-3 targeted retrieval queries per implied claim, and score evidence by per-claim entailment (support/neutral/refute) rather than lexical overlap with the question stem. For open-ended QA, the same mechanism operates on factual anchors extracted from question sub-clauses. A third component upgrades document ingestion from single-best-chunk to top-3 claim-density passages per document, breaking the hard 1200-char bottleneck without requiring new services.",
  "summary": "OPTEX proposes three tightly integrated improvements to SPARKIT's retrieval and evidence ingestion pipeline. First, vocabulary self-improvement mines the TF-IDF signal from round-1 retrieved abstracts to discover domain jargon the static query expander misses, feeding those terms back into rounds 2 and 3. Second, for MCQ questions, each answer option is decomposed into the specific factual claims it commits to (structured triples), and targeted retrieval queries are generated per claim rather than per option; evidence is then entailment-scored per implied claim so dossier construction is fact-grounded rather than term-overlap-grounded. Third, document ingestion is upgraded from the single best 1200-char chunk to the top-3 highest claim-density 800-char windows per document, tripling effective evidence coverage per ingested paper. All three are additive to existing stages in engine.py with no new microservices.",
  "reasoning": "SPARKIT's current accuracy ceiling on HLE-level hard questions is set by two cascading failures: (a) retrieval misses key documents because queries use generic terminology while domain literature uses precise nomenclature (IUPAC compound names, benchmark dataset abbreviations, field-specific acronyms that no static synonym list covers); and (b) even when the right document is retrieved, the single-best-chunk ingestion discards 80-90% of the document's content, leaving the synthesis LLM unable to cite the specific sentence that contains the answer. The option-implied claim decomposition directly addresses a third failure mode: SPARKIT's dossier scoring rewards options whose terminology overlaps with retrieved text, but hard MCQ options are often terminologically similar — the discriminating signal is whether retrieved evidence ENTAILS the specific measurement/value/mechanism each option commits to, not whether the words match. By making retrieval queries answer-hypothesis-specific at the claim level (not just option level), SPARKIT issues queries like 'Shockley-Queisser limit silicon 33 percent' rather than the generic 'silicon solar cell maximum efficiency', dramatically improving precision for numerical and mechanistic questions. The vocabulary mining acts as a zero-cost, zero-latency-overhead signal amplifier: it reuses already-retrieved abstracts to bootstrap domain-specific query enrichment for rounds 2-3 with only a lightweight counting pass.",
  "expected_impact": {
    "accuracy_delta_pct_points": 7,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add `_mine_abstract_vocabulary(records: list[dict], top_n: int = 20) -> list[str]` to engine.py. Uses collections.Counter over tokenized abstract text, filters tokens by length >= 6 and frequency >= 2 across documents, removes stopwords, and returns the top_n highest-IDF tokens (IDF = log(total_docs / doc_freq)). Called after round-1 retrieval completes; returned terms appended to query enrichments for rounds 2-3 inside execute_orchestration's retrieval loop.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add `_extract_option_implied_claims(stem: str, option_label: str, option_text: str, provider: str) -> list[dict]` to engine.py. Makes a targeted LLM call (max_tokens=300, cheapest available provider) with prompt: 'List the specific factual claims that option {label} asserts as true. Each claim: subject | predicate | object | value_or_unit (use None if not applicable). Max 3 claims. Be concrete.' Returns list of dicts {subject, predicate, object_, value}. Controlled by env var SPARKIT_OPTEX_ENABLED (default 0) and SPARKIT_OPTEX_MAX_CLAIMS_PER_OPTION (default 3). Skipped when len(answer_choices) < 2.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add `_build_claim_targeted_queries(claim: dict) -> list[str]` to engine.py. From a structured claim triple, constructs 2 retrieval queries: (1) a tight subject+predicate+object query for academic search, (2) a value-anchored query if claim.value is not None (e.g., '{subject} {value} {unit}'). Returns query list. Called in MCQ option hypothesis query generation loop, interleaved with existing per-option queries.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add `_score_claim_entailment(passage: str, claim: dict) -> str` to engine.py. Makes an LLM call (max_tokens=50): 'Does the following passage support, refute, or neither confirm the claim \"{subject} {predicate} {object_} {value}\"? Answer: SUPPORT, REFUTE, or NEITHER.' Returns one of the three labels. Called during dossier construction for each (option, implied_claim, passage) triple. Results stored in dossier as claim_entailments: {claim_text: {support_count, refute_count}}.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify `_build_option_dossiers()` in engine.py: after existing support_snippets/counter_snippets collection, if claim_entailments exist for this option, compute entailment_score = (support_count - refute_count) / max(1, support_count + refute_count) per claim, average across claims, and blend with existing dossier_score: final_score = 0.6 * entailment_avg + 0.4 * dossier_score. Add entailment_score and claim_entailments to returned dossier dict. No change to _select_option_from_dossiers() thresholds needed.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Modify document ingestion in execute_orchestration: change `_select_records_for_ingestion` chunk selection from top-1 to top-3 windows per document. Add `_extract_top_k_chunks(parsed_doc: dict, question: str, implied_claims: list[dict], k: int = 3) -> list[str]` which scores ALL section windows with `_chunk_relevance_score` plus a claim_density bonus (unique_tokens / len(window)), returns top-k non-overlapping windows. Concatenate top-k chunks with '[...continued...]' separator, respect SPARKIT_INGESTION_MAX_CHARS. Controlled by SPARKIT_OPTEX_MULTI_CHUNK_K (default 3).",
      "owner": "orchestrator/engine.py + ingestion_service/parser.py",
      "effort": "medium"
    },
    {
      "step": "Integrate vocabulary mining into retrieval loop in execute_orchestration: after round-1 completes, call `_mine_abstract_vocabulary(round_1_records, top_n=int(os.getenv('SPARKIT_OPTEX_VOCAB_TOP_N','15')))` and store as vocab_enrichments. In rounds 2-3, for each generated query q, append the top-5 vocab_enrichments as space-separated terms to the query string if vocab_enrichments is non-empty and the query does not already contain the term. Log enrichment count to observability metrics.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add tests to test_synthesis_quality.py: test_mine_abstract_vocabulary_returns_domain_terms, test_extract_option_implied_claims_returns_structured_triples, test_build_claim_targeted_queries_produces_subject_value_query, test_claim_entailment_blends_into_dossier_score, test_top_k_chunk_extraction_avoids_overlap. Add SPARKIT_OPTEX_ENABLED, SPARKIT_OPTEX_MAX_CLAIMS_PER_OPTION, SPARKIT_OPTEX_VOCAB_TOP_N, SPARKIT_OPTEX_MULTI_CHUNK_K to docs/configuration.md.",
      "owner": "orchestrator/tests + docs",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Abstract vocabulary self-improvement: after round-1 retrieval, compute per-token IDF across retrieved abstracts using collections.Counter and inject the top-15 highest-IDF domain-specific terms (length >= 6, freq >= 2 across docs) as query enrichment tokens into rounds 2 and 3. This is a zero-overhead mechanism that bootstraps domain-jargon discovery from the documents SPARKIT already retrieves, addressing the core failure on IUPAC compound names, benchmark dataset abbreviations, and field-specific acronyms that the static synonym expander in aggregator.py misses entirely.",
    "Option-implied claim targeted retrieval: for each MCQ answer option, extract up to 3 structured factual commitment triples (subject | predicate | object | value) via a small LLM call, then generate 2 retrieval queries per triple — one tight academic search query and one value-anchored query (e.g., 'Shockley-Queisser silicon 33.7 percent' for an option claiming 33.7% maximum efficiency). These queries are issued in the MCQ option hypothesis round, replacing or augmenting the generic '{stem} {choice}' queries. Precision on numerical and mechanistic questions improves because the queries target the specific claim being tested rather than the option's surface text.",
    "Multi-passage per-document extraction: replace the single-best 1200-char chunk selection in _select_records_for_ingestion with extraction of the top-3 non-overlapping 800-char windows per document, scored by a combined metric of `_chunk_relevance_score` plus a claim_density bonus (unique_term_count / chunk_length). This triples the effective evidence surface per ingested paper and recovers answer-containing sentences that the current single-chunk approach systematically discards when the answer is in a Results or Methods section rather than the Abstract.",
    "Claim-entailment-driven query feedback: after entailment scoring reveals that retrieved passages consistently return NEITHER for a specific option's implied claim, generate one additional targeted retrieval query for that claim (appended to the adaptive gap-fill round if it has not yet run) using the claim's subject and value as anchors. This creates a lightweight closed-loop where persistent entailment uncertainty triggers additional evidence-seeking specifically for the most ambiguous option's claim.",
    "Source-stratified vocabulary injection: when vocabulary mining surfaces terms matching known arXiv subject category tokens (cs., quant-ph., cond-mat., etc.), prepend those tokens to the arXiv-specific adapter query in subsequent rounds. When terms match Europe PMC MeSH-style compound patterns (capitalized multi-word biological terms), they are injected into the Europe PMC query. This per-adapter enrichment exploits each source's indexing characteristics rather than sending identical queries to all adapters."
  ],
  "evaluation_plan": [
    "HLE-gold MCQ accuracy ablation: run the HLE-gold benchmark in three configurations — (A) baseline current SPARKIT, (B) SPARKIT with multi-passage extraction only (SPARKIT_OPTEX_MULTI_CHUNK_K=3, SPARKIT_OPTEX_ENABLED=0), (C) full OPTEX (all components). Report per-configuration accuracy, cost, and latency. For configuration C vs. A, compute McNemar's test on the question-level accuracy matrix to establish statistical significance. Target: >= 4 pp net gain on MCQ-subset with p < 0.05.",
    "Retrieval recall with vocabulary mining: for 20 randomly sampled HLE-gold questions with known relevant papers, compare the fraction of known-relevant papers appearing in the top-20 retrieved results before and after vocabulary enrichment. Specifically, for each question compute recall@20 in round-1 vs. rounds 2-3 with vocabulary enrichment, and report the delta. A recall improvement of >= 10 pp across the 20 questions would confirm that vocabulary mining surfaces domain-specific papers the first round misses.",
    "Claim entailment direction accuracy spot-check: for 50 questions where OPTEX's entailment scoring selects a different top option than the current dossier scoring, record OPTEX's choice and compare against gold labels. Compute precision of OPTEX's entailment-driven choice where it diverges from baseline. If OPTEX precision > baseline precision on these divergence cases, entailment scoring adds signal; if lower, the scoring blend weights need recalibration.",
    "Multi-passage evidence coverage measurement: for 30 questions where SPARKIT currently produces correct answers, extract the specific sentence from the gold explanation that most directly supports the answer. Check whether that sentence appears in the current single-best-chunk vs. in the top-3-chunk extraction. Report the fraction of questions where the gold-supporting sentence is recovered by multi-passage extraction but would have been missed by single-chunk. Target: >= 25% of questions have materially better coverage.",
    "Cost regression gate: implement a benchmark guard that fails if the cost-per-question ratio (OPTEX cost / baseline cost) exceeds 1.8x on the HLE-25 balanced subset. This prevents the claim extraction LLM calls and additional retrieval queries from making OPTEX uneconomical. If the ratio exceeds 1.8x, flag which component (vocab mining, claim extraction, entailment scoring, multi-chunk ingestion) contributes the most cost and tune its knob (e.g., reduce SPARKIT_OPTEX_MAX_CLAIMS_PER_OPTION from 3 to 2 or route claim extraction to the cheapest provider).",
    "Calibration ECE delta: run the existing calibration evaluation on OPTEX outputs and compare ECE and Brier score against baseline. The entailment-grounded evidence should produce better-calibrated confidence scores since claim_entailments directly contribute to dossier_score, which flows into the synthesis prompt. Target: ECE reduction of >= 0.02 on the HLE-gold calibration split."
  ],
  "risks": [
    "LLM claim extraction quality: the structured triple extraction from MCQ options via a small LLM call may produce noisy, incorrect, or hallucinated predicates/values, particularly for multi-clause or ambiguous options. Mitigation: constrain the extraction prompt to produce at most 3 claims, require the value field to be quoted directly from the option text if present, and add a validation pass that rejects claims where subject or object is empty. If claim quality is low, fall back gracefully to existing dossier scoring (entailment_score = None, weight = 0).",
    "Vocabulary mining noise injection: TF-IDF extraction over a small corpus of 10-18 retrieved abstracts may surface proper nouns, author names, or venue names as high-IDF tokens, injecting irrelevant query terms into rounds 2-3. Mitigation: add a blocklist of known noise patterns (title-cased single tokens that match author-name patterns, year tokens like '2024', journal abbreviations) and cap vocabulary injection to 5 terms per query to limit blast radius.",
    "Multi-passage ingestion prompt size explosion: extracting 3 passages per document instead of 1 triples the evidence text fed into synthesis prompts, potentially exceeding synthesis_max_tokens limits or increasing synthesis LLM cost by 2-3x. Mitigation: shorten each passage window from 1200 chars to 800 chars (net increase ~2x), enforce synthesis_max_tokens via existing SPARKIT_INGESTION_MAX_CHARS, and add a chunk count guard: if total evidence chars > 12000, fall back to top-1 chunk per document.",
    "Entailment scoring API call overhead: for a 4-option MCQ question with 3 claims per option and 5 evidence passages each, claim entailment scoring requires up to 60 LLM calls per question at ~100ms each, adding 6+ seconds of latency. Mitigation: batch the entailment calls using the cheapest/fastest provider (e.g., deepseek-reasoner or haiku), cap at top-2 passages per claim rather than all passages, and disable entailment scoring when SPARKIT_PROVIDER_TIMEOUT_S constraints would be violated.",
    "Open-ended QA claim extraction degrades gracefully: the sub-clause factual anchor extraction for open-ended questions is less well-defined than MCQ option decomposition and may produce low-quality anchors. Mitigation: initially gate OPTEX claim decomposition to MCQ-only (len(answer_choices) >= 2) with a separate SPARKIT_OPTEX_OPEN_ENDED flag defaulting to 0 until MCQ performance is validated.",
    "Regression on questions where baseline already performs well: adding vocabulary enrichment to rounds 2-3 may shift query focus away from high-recall broad queries toward narrow domain-jargon queries, reducing recall on questions where round-1 already found all relevant documents. Mitigation: make vocabulary injection additive (append terms to existing queries rather than replacing them), and respect the adaptive retrieval gating so that if round-1 already achieves high quality gain, rounds 2-3 with vocabulary enrichment are skipped anyway."
  ]
}
```
