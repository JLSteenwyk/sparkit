```json
{
  "proposal_id": "SPARKIT-HGIEC-001",
  "title": "Hypothesis-Grounded Iterative Evidence Chains (HGIEC)",
  "distinctive_angle": "Replace SPARKIT's static one-shot query decomposition with a hypothesis-compete-and-prune loop: generate competing mechanistic hypotheses from the question, assign each a targeted source-routing strategy, update hypothesis posteriors after each retrieval round via LLM scoring, prune losers before they consume later rounds, and feed synthesis a structured hypothesis-vs-evidence dossier instead of flat claim clusters — turning the multi-round retrieval architecture into a directed Bayesian search over hypothesis space rather than a novelty-driven crawl.",
  "summary": "Hard questions fail not because SPARKIT retrieves too little, but because it retrieves the wrong things. Current query decomposition (_decompose_retrieval, engine.py ~674-750) runs once and produces intent_queries via a static keyword-expansion dictionary. HGIEC inserts a hypothesis layer between planning and retrieval: (1) a planning LLM generates 3-5 competing mechanistic hypotheses for the question; (2) each hypothesis drives its own focused retrieval sub-strategy selecting the 2 domain-appropriate sources from the 6 available adapters; (3) after each retrieval round, a lightweight evaluator LLM scores evidence coverage per hypothesis and updates hypothesis posteriors; (4) hypotheses with posterior < 0.05 are pruned before the next round, redirecting query budget toward survivors; (5) the surviving hypothesis with highest posterior drives adversarial round queries; (6) synthesis receives a structured hypothesis-vs-evidence dossier enabling explicit conflict resolution rather than flat cluster label enumeration.",
  "reasoning": "SPARKIT's adaptive gating (engine.py lines 1398-1454) stops early when new_unique_docs < 2 or quality_gain < 0.03. For hard questions this fires prematurely because the underlying queries are insufficiently targeted — additional rounds repeat the same broad search space rather than exploring orthogonal angles. The static keyword-expansion dictionary in _rewrite_queries() (aggregator.py lines 59-75) maps surface synonyms but cannot generate hypothesis-specific entity queries like 'cyclopentyl-ring strain + Diels-Alder selectivity' or 'RLHF reward hacking + policy collapse'. Hypothesis-driven retrieval breaks this plateau by partitioning the evidence search space across competing explanations, ensuring each round explores a structurally distinct region. Evidence grounding further improves because each ingested claim is tagged to the hypothesis it supports, making synthesis prompts structurally richer: the LLM must resolve explicit competition between mechanistic alternatives rather than enumerate a flat bullet list. This mirrors how expert researchers incrementally narrow hypotheses — the strategy that produces correct answers on multi-hop, counter-intuitive, and contested scientific questions.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _generate_hypotheses() to engine.py in the planning stage (after line ~1272, inside execute_orchestration). Call the planning provider LLM with a structured prompt that extracts 3-5 competing mechanistic hypotheses as JSON array. Each hypothesis must be a short (<40 words) causal/mechanistic statement (not a restatement of the question) and must contain at least one noun phrase from the question. Store output in research_plan.hypotheses: List[str] and surface it in the planning TraceStage artifacts.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_hypothesis_retrieval_strategies() to engine.py. For each hypothesis, use domain-keyword heuristics to select 2 target sources from {arxiv, semantic_scholar, openalex, europepmc, crossref, brave}: biomedical terms -> europepmc + crossref; ML/AI terms -> arxiv + semantic_scholar; clinical/trial terms -> europepmc + brave; general science -> openalex + brave. Then call the planning LLM to extract 2-3 core technical entities from each hypothesis and construct targeted queries as entity AND-combinations. Output: List[HypothesisStrategy(hypothesis_text, sources, queries, priority)] stored in retrieval_plan.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend retrieval_service/app/aggregator.py to accept a source_filter: Optional[List[str]] parameter in aggregate(). Refactor the adapter dispatch loop (around lines 137-175) to skip adapters not in source_filter when the parameter is provided. This enables hypothesis-targeted source routing without spawning separate service calls and with zero change to the adapter implementations.",
      "owner": "retrieval_service/app/aggregator.py",
      "effort": "low"
    },
    {
      "step": "Implement _score_hypothesis_posteriors() in engine.py. After each retrieval round, call the synthesis provider with a compact structured prompt: given hypothesis list and this round's retrieved abstract snippets, score each hypothesis 0.0-1.0 for evidence support. Parse output into hypothesis_posteriors: Dict[int, float]. Normalize posteriors to sum to 1.0. Prune hypotheses with posterior < 0.05 after round 2+ by removing them from the active strategy list. Add pruning decisions as artifacts to the retrieval_adaptive_gate TraceStage.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify the adaptive retrieval gate decision (engine.py lines ~1430-1454) to incorporate hypothesis posteriors as an additional early-stop signal: if exactly 1 hypothesis survives pruning AND its posterior >= 0.70, set winning_hypothesis and stop retrieval immediately. If no hypothesis reaches 0.05 after round 3, fall back to question-level queries for remaining rounds. Retain existing novelty/gain stopping as fallback when HGIEC is disabled.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Tag every ingested claim with its nearest hypothesis index via token-overlap scoring between claim text and each surviving hypothesis statement (reuse the _select_best_section_chunk scoring pattern from engine.py lines 539-554). Store as hypothesis_id on the claim object before evidence_store.insert_claim(). Modify _build_claim_clusters() (engine.py lines 918-939) to produce hypothesis-annotated clusters: {hypothesis_text, posterior, support_claims, counter_claims, coverage_score} rather than title-token-pair buckets.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Update the synthesis prompt templates (engine.py lines 993-1032 for free-form, 1035-1077 for MCQ judge) to include a HYPOTHESIS ANALYSIS section above the flat evidence bullets. Format: 'HYPOTHESIS ANALYSIS:\\n  H1 (posterior=0.72): [text]\\n    Supporting: [top-2 claim texts]\\n    Contradicting: [top-1 claim text if any]\\n  H2 (posterior=0.18, pruned at round 2): [text]'. Add instruction: resolve hypothesis competition explicitly before stating the final answer. Compress to top-2 hypotheses when input token estimate exceeds 75% of provider context window.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Extend calibration.py CalibrationFeatures (lines 6-14) with four new fields: hypothesis_count (initial), hypotheses_pruned (int), winning_posterior (float, 0 if none), and hypothesis_convergence_round (int, -1 if no winner). Update the confidence formula (lines 20-36) to apply +0.10 bonus when winning_posterior >= 0.6 and -0.05 penalty when max_posterior < 0.3 across all surviving hypotheses (signals genuine ambiguity). Update CalibrationStore upsert to persist new fields.",
      "owner": "orchestrator/calibration.py",
      "effort": "low"
    },
    {
      "step": "Add test_hypothesis_pipeline() to test_synthesis_quality.py covering: hypothesis generation returns >=2 distinct mechanistic strings not equal to the original question; strategy assignment routes a biomedical hypothesis to europepmc; posterior updates are monotonically consistent under repeated identical evidence; pruning fires when posterior < 0.05; synthesis prompt string contains 'HYPOTHESIS ANALYSIS'; and convergence early-stop fires when winning_posterior >= 0.70. Add SPARKIT_ENABLE_HGIEC env var guard (default 1) to allow disabling the full loop for latency-constrained deployments.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Hypothesis-targeted source routing: instead of querying all 6 adapters with every intent query, route each hypothesis to its 2 domain-appropriate sources. A computational chemistry hypothesis hits arxiv + semantic_scholar; a clinical-outcome hypothesis hits europepmc + crossref; a tech-benchmark hypothesis hits brave + semantic_scholar. This halves per-round API fan-out while doubling topical precision relative to the current uniform broadcast in aggregator.py lines 137-175.",
    "Posterior-weighted adversarial queries: the existing retrieval_round_3_adversarial (engine.py ~1380-1395) currently fires generic contradiction-seeking queries. With HGIEC, it instead targets the top surviving hypothesis with queries designed to find falsifying evidence by prepending 'failure of [hypothesis_entity]', 'limitations [hypothesis_mechanism]', and 'negative results [hypothesis_topic]' — making adversarial retrieval hypothesis-aware rather than question-generic.",
    "Entity-grounded query construction per hypothesis: for each hypothesis, extract 2-3 core technical entities (specific compound names, model architectures, biological mechanisms) via LLM and construct queries using those entities as AND-combinations. This replaces the generic 18-term truncation in _relax_query() (aggregator.py lines 48-56) which sacrifices specificity on technical STEM questions where precision vocabulary is essential.",
    "Gap-driven secondary queries from weak-evidence hypotheses: after _score_hypothesis_posteriors(), any surviving hypothesis with posterior in (0.05, 0.35) triggers a targeted secondary query constructed as '[hypothesis_entity] [aspect missing from evidence]', where missing aspect is inferred by comparing hypothesis_text noun phrases against the union of retrieved abstract token sets. This drives round N+1 retrieval directly toward the evidential gap rather than repeating broad queries.",
    "Per-hypothesis source diversity enforcement: the current source diversity cap (max(1, max_results//2) per source globally, aggregator.py lines 86-105) can starve niche hypotheses when a dominant source swamps the global result set. HGIEC applies the cap independently per hypothesis retrieval call before global deduplication, ensuring each hypothesis receives at least 1 record from each of its designated sources before the global merge and deduplication step."
  ],
  "evaluation_plan": [
    "A/B benchmark on HLE-gold subset (>=25 questions, matched difficulty tiers): run baseline SPARKIT ROUTED mode vs HGIEC-enabled engine with identical provider configuration and random seed. Primary metric: exact-match or rubric-graded accuracy. Secondary metrics: evidence unique-source count per answer, abstain rate, and mean rounds_used. Flag regressions if HGIEC accuracy drops below baseline minus 2pp on any difficulty tier.",
    "Hypothesis convergence vs correctness correlation: for each HGIEC run, log hypothesis_count_initial, hypotheses_pruned_per_round, winning_posterior, and answer_correct. Compute Pearson correlation between winning_posterior and correctness. Acceptance criterion: runs with winning_posterior >= 0.60 must achieve >= 10pp higher accuracy than runs with max_posterior < 0.30, confirming posterior is a reliable signal rather than noise.",
    "Evidence precision audit: for 10 randomly sampled hard QA runs, manually verify that each of the top-3 cited sources contains information directly relevant to the final answer. Compare claim-to-source precision between baseline flat-cluster synthesis and HGIEC hypothesis-tagged synthesis. Target: >= 20% improvement in citation precision (fraction of citations where the cited source materially supports the stated claim).",
    "Retrieval round efficiency: compare distribution of rounds_used between baseline adaptive gating and HGIEC early-stopping (winning_posterior >= 0.70). Verify that on well-defined factual questions where a clear hypothesis winner emerges, HGIEC uses fewer rounds while achieving equal or higher accuracy — demonstrating that early stopping is quality-preserving not quality-degrading.",
    "Abstain rate calibration and ECE improvement: compute Expected Calibration Error (ECE) on a held-out 50-question set using both baseline calibration features (support_coverage, unsupported_claims) and HGIEC-augmented features (winning_posterior, hypotheses_pruned). Plot reliability diagrams for both. Acceptance criterion: HGIEC ECE must be <= 0.85 x baseline ECE, confirming hypothesis-aware confidence scoring is better calibrated.",
    "Synthesis coherence scoring by LLM judge: for 20 hard questions, run both baseline and HGIEC synthesis, then query a separate judge LLM (different provider from synthesis) to score each answer 1-5 on three dimensions: (a) explicitly addresses competing explanations, (b) cites specific mechanisms not just conclusions, (c) correctly identifies the crux of the question. Expect HGIEC to score >= 0.5 points higher on dimension (a) as a direct result of the hypothesis dossier structure in the prompt."
  ],
  "risks": [
    "Hypothesis hallucination: the planning LLM may generate hypotheses that sound mechanistically plausible but do not correspond to real scientific possibilities, biasing retrieval toward non-existent evidence. Mitigation: require each hypothesis_text to contain at least one noun phrase token-matched to the original question; abort hypothesis-specific routing after 2 zero-result retrieval attempts and fall back to question-level queries for that hypothesis slot.",
    "Posterior scoring unreliability: LLM-based hypothesis scoring adds 1 extra LLM call per retrieval round, increasing cost ~15-25% per run depending on provider. If the scoring LLM disagrees with human judgment, pruning may eliminate the correct hypothesis. Mitigation: use DeepSeek-reasoner (lowest cost provider, policy.py lines 58-59) for posterior scoring, keep pruning threshold conservative at 0.05 (not 0.20), and log all pruning decisions in TraceStage artifacts to enable post-hoc audit.",
    "Latency amplification beyond max_latency_s budget: hypothesis generation (1 LLM call), posterior scoring per round (1 LLM call x N rounds), and strategy assignment add 3-5 extra sequential LLM calls. For latency-sensitive deployments with max_latency_s < 30s, this will breach the budget. Mitigation: skip hypothesis stage entirely when max_latency_s < 30s or SPARKIT_ENABLE_HGIEC=0; expose SPARKIT_HGIEC_MAX_HYPOTHESES (default 3) to cap generation and bound overhead.",
    "Source routing domain misclassification: the keyword-heuristic hypothesis-to-source routing may mismatch interdisciplinary hypotheses (e.g., 'neural network pruning for drug discovery' spans ML and biomedical domains). Mitigation: always include brave web search as a universal fallback source for every hypothesis; allow overlap between domain buckets; if a hypothesis matches zero domain keywords, default to arxiv + semantic_scholar as the broadest STEM coverage pair.",
    "Synthesis prompt token inflation: adding the HYPOTHESIS ANALYSIS section to synthesis prompts increases input tokens by ~400-800 tokens per run, raising costs for premium providers (GPT-5.2-pro at $21/MTok, claude-opus-4-6 at $5/MTok per policy.py lines 43-50). Mitigation: compress hypothesis dossier to top-2 hypotheses only when input token estimate exceeds 75% of the provider context limit; track token_inflation_pct in ObservabilityStore metrics and alert when average exceeds 20% above baseline."
  ]
}
```
