```json
{
  "proposal_id": "SPARKIT-P-007",
  "title": "Discriminative Hypothesis-Driven Retrieval with Cross-Evidence Triangulation",
  "distinctive_angle": "Instead of retrieving evidence toward a pre-formed answer direction, generate competing answer hypotheses first, retrieve evidence that specifically differentiates between them, score each retrieved piece by its discriminative power (how much it favors one hypothesis over others), and synthesize exclusively from the highest-discriminativeness evidence pool—turning retrieval from a support-gathering exercise into a hypothesis elimination tournament.",
  "summary": "SPARKIT's current retrieval is support-seeking: queries are built from the question stem and attempt to retrieve generally relevant literature. On hard questions, multiple plausible answers exist, and current retrieval often gathers evidence that supports all of them equally, producing ambiguous synthesis and low-confidence calibration. This proposal introduces Hypothesis-Discriminative Retrieval: (1) after initial planning, the system generates 2-3 competing candidate hypotheses, (2) retrieval in rounds 2+ is guided by cross-hypothesis discriminative queries, (3) each retrieved document receives a discriminativeness score reflecting how uniquely it supports one hypothesis over others, (4) the synthesis prompt is restructured to lead with highest-discriminativeness evidence, and (5) confidence calibration gains a new hypothesis_margin feature—the discriminative evidence ratio between the leading and second hypotheses.",
  "reasoning": "Hard questions (HLE-style) are hard precisely because the distractors are also well-supported by literature. Standard retrieval collects papers supporting multiple incompatible positions; when fed to a synthesis LLM, these cancel out into hedge language and wrong or abstained answers. By explicitly modeling competing hypotheses and retrieving to discriminate between them, SPARKIT shifts from symmetric evidence collection to asymmetric hypothesis elimination. The discriminativeness score allows synthesis to be seeded with evidence that has maximal information content for the question, reducing hallucination risk and calibration errors. The MCQ path already has partial support via option evidence packs and dossier scoring, but the general QA path and the retrieval query generation step both lack any hypothesis-aware logic. Adding hypothesis generation costs one LLM call but unlocks targeted discriminative queries across multiple retrieval rounds, improving evidence quality more than increasing retrieval volume. The current adaptive retrieval gate stops rounds based on novelty and quality gain thresholds, but these are computed over the full document pool—not per-hypothesis. A question can have high overall evidence novelty yet all new documents support the wrong hypothesis. Discriminativeness scoring fixes this blind spot.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "mixed",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _generate_competing_hypotheses(question, research_plan, provider) -> list[str] in engine.py: single LLM call after _decompose_question(), returns 2-3 distinct candidate answers with rationale. For MCQ, map directly to non-eliminated options. For free-form, use structured prompt 'List 2-3 mutually exclusive candidate answers that a careful expert might consider, each in one sentence.' Parse output as newline-separated hypotheses with regex fallback to heuristic splits on 'OR', 'alternatively', 'however'.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_discriminative_queries(hypotheses, focus_terms) -> list[str] in engine.py: takes the top-2 competing hypotheses and generates cross-hypothesis queries targeting differentiating evidence. Template examples: 'evidence {h1_key_term} mechanism excludes {h2_key_term}', '{h2_key_term} failure conditions experimental', '{h1_key_term} vs {h2_key_term} comparison'. Extract key terms from hypotheses via _extract_lexical_anchors() already in engine.py. Cap at 4 queries to contain cost.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _score_evidence_discriminativeness(record, hypotheses, focus_terms) -> float in engine.py: for each ingested document, compute token overlap with each hypothesis text; discriminativeness = max_overlap - mean(other_overlaps). Store as discriminativeness_score on the record dict alongside existing relevance_score. Reuse the tokenization logic already in _record_relevance_score().",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Update _select_records_for_ingestion() in engine.py: in the second pass (slot-fill by relevance), add 0.5 * discriminativeness_score to the sort key. This biases ingestion toward records uniquely supporting one hypothesis without fully overriding source-diversity logic from the first pass.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Update _build_round_queries_from_plan() in engine.py: inject _build_discriminative_queries() output into the round_2_gap_fill query list (replacing up to 2 existing gap-fill queries if the plan already has > 6 queries, otherwise appending). This fires discriminative retrieval in round 2 after hypotheses are established from round 1 evidence.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Update _build_synthesis_prompt() in engine.py: sort evidence bullets by discriminativeness_score descending, prepend the top 5 highest-discriminativeness snippets tagged with [KEY EVIDENCE] before the existing claim_clusters block. Update _build_mcq_option_scoring_prompt() similarly to place discriminative option evidence first in each option's dossier block.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add hypothesis_margin to CalibrationFeatures dataclass in shared/schemas/domain.py and calibration.py: computed as (discriminative_evidence_leader - discriminative_evidence_runner_up) / max(1, total_discriminative_evidence), clamped to [0, 1]. Add coefficient +0.12 * hypothesis_margin in calibrate_answer() formula. Cap total contribution at +0.15 to avoid overconfidence.",
      "owner": "orchestrator/calibration.py + shared/schemas/domain.py",
      "effort": "low"
    },
    {
      "step": "Update verifier.py run_verifier() to accept optional leader_hypothesis and challenger_hypothesis strings. When provided, score adversarial records against leader_hypothesis first using existing heuristic markers, then for records scoring > 0.4, run a single LLM stance classification prompt ('Does this abstract support or contradict: {leader_hypothesis}?') using the cheapest available provider. Track the extra cost in the existing cost accounting.",
      "owner": "orchestrator/verifier.py + policy.py",
      "effort": "medium"
    },
    {
      "step": "Add discriminativeness telemetry to observability_store.py: per run, log top_hypothesis, hypothesis_margin, discriminative_evidence_count, cross_hypothesis_query_count, and discriminative_query_new_doc_count (documents retrieved only by discriminative queries). This enables offline analysis of which question types benefit most.",
      "owner": "orchestrator/observability_store.py",
      "effort": "low"
    },
    {
      "step": "Add unit tests in test_synthesis_quality.py: test _generate_competing_hypotheses output parsing for 2-3 outputs, _score_evidence_discriminativeness on synthetic records with controlled token overlap, synthesis prompt KEY EVIDENCE ordering (discriminative records appear before non-discriminative), hypothesis_margin calibration computation on known feature inputs, and round-2 query injection count guard (no more than +4 queries added).",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Discriminative Cross-Hypothesis Query Generation in round_2_gap_fill: Replace generic gap-fill queries with queries targeting evidence that favors one competing hypothesis over others (e.g., 'experimental evidence {H1_term} mechanism excludes {H2_term}', '{H2_term} failure conditions', '{H1_term} vs {H2_term} comparative study'). This shifts the retrieval objective from 'find relevant literature' to 'find differentiating literature', which is more informative for hard questions where many papers are tangentially relevant to all options. Implemented via _build_discriminative_queries() injected into _build_round_queries_from_plan() at round 2.",
    "Discriminativeness-Weighted Slot Filling in _select_records_for_ingestion(): After computing discriminativeness_score per record (max_hypothesis_overlap - mean_other_hypothesis_overlap), add 0.5 * discriminativeness_score to the relevance_score used for the second-pass slot fill. This biases ingestion toward records that uniquely support one hypothesis, maximizing information-per-document without breaking the source-diversity logic of the first pass.",
    "Contrastive Pair Queries for MCQ Option Hypothesis Retrieval: Augment _build_option_hypothesis_queries() with contrastive pair queries for the top-2 non-eliminated options. Currently the function generates '{stem} {choice_text}' and '{stem} evidence for {choice_text}'. Add '{stem} why {choice_A_text} not {choice_B_text}' and 'evidence against {choice_B_text} mechanism' where A is current top-dossier-scored option and B is second. These contrastive queries retrieve papers that explicitly compare or rule out competing mechanisms, which are the most informative for hard MCQ.",
    "Round 3 Adversarial Query Targeting Against Leading Hypothesis: Feed the current leading hypothesis text into _build_round_queries_from_plan() for round_3_adversarial. Instead of generating adversarial queries only from focus_terms (current behavior), generate queries that specifically attempt to falsify the leading hypothesis: 'limitations of {leading_hypothesis_key_term}', '{leading_hypothesis_term} fails when', 'negative results {leading_hypothesis_term}'. This produces hypothesis-specific falsification attempts rather than generic adversarial queries, surfacing high-value contradiction evidence.",
    "Source-Novelty Bonus for Discriminative Records: In _select_records_for_ingestion(), when a record has discriminativeness_score > 0.3 AND comes from a source not yet represented in the current ingestion pool, apply an additional 0.3 source_novelty_bonus on top of the discriminativeness bonus. This prevents high-discriminativeness records from being filtered out by the per-source dominance cap (max_results/2), ensuring hypothesis-differentiating evidence from rare sources (e.g., Europe PMC for a question dominated by arXiv records) reaches ingestion."
  ],
  "evaluation_plan": [
    "HLE-Gold Accuracy Stratified by Hypothesis Margin: After one full benchmark run, split questions by median hypothesis_margin (high = top 50%, low = bottom 50%). Compute accuracy within each group. Hypothesis: high-margin questions should have meaningfully higher accuracy than low-margin questions (target delta >= 8 pp), validating hypothesis_margin as a quality signal and justifying its inclusion in calibration.",
    "Discriminative Query New-Document Recall: For 20 manually reviewed HLE-Gold questions, verify that cross-hypothesis discriminative queries (injected into round 2) retrieve at least 1 document with discriminativeness_score > 0.25 that was NOT retrieved by round_1 or original round_2_gap_fill queries. Log using the cross_hypothesis_query_new_doc_count telemetry field. Target: >= 75% of questions gain at least one new discriminative document, confirming the queries are meaningfully expanding the evidence set rather than duplicating existing retrieval.",
    "MCQ Option Selection Cascade Rate Improvement: Track the fraction of MCQ questions resolved at each pipeline stage (dossier_select vs. blended_select vs. judge_prompt vs. generic_synthesis) before and after this proposal. Discriminative retrieval should improve evidence distinction per option, widening dossier margins and increasing the dossier_select rate. Target: dossier_select rate increases by >= 10 percentage points and judge_prompt fallback rate decreases by >= 5 percentage points relative to pre-proposal baseline.",
    "Hypothesis Margin vs. Correctness Spearman Correlation: Compute Spearman correlation between hypothesis_margin (continuous, 0-1) and is_correct (binary, 0/1) across >= 100 HLE questions in the benchmark run. If hypothesis_margin is a valid quality proxy, correlation should be positive and statistically significant (r > 0.15, p < 0.05). If correlation is weak or negative, the calibration coefficient should be zeroed out to avoid introducing miscalibration.",
    "Cost and Latency Regression on 50-Question Subset: Run a matched 50-question sample before and after the proposal. Measure p50 and p95 wall-clock latency per question and average cost per question. Accept trade-off if: p50 latency increase < 20%, average cost increase < 30%, and accuracy improvement >= 4 percentage points. If cost exceeds threshold, gate discriminative retrieval behind budget check (remaining_budget > $1.50) and re-evaluate.",
    "Abstention Rate Stability Check: Count hard-abstain and soft-abstain rates on the full benchmark before and after. Discriminative retrieval should reduce abstentions on answerable hard questions by improving quality gate passage (better evidence → higher support_coverage → fewer abstain_reasons). Flag as regression if abstention rate changes > 3 percentage points without a corresponding accuracy change, which would indicate the quality gate is being mis-triggered by the new evidence structure."
  ],
  "risks": [
    "Hypothesis Generation Quality Failure: If the LLM generates hypotheses that are too similar, implausible, or all wrong, discriminative queries will retrieve irrelevant evidence and degrade rather than improve accuracy. Mitigation: validate hypothesis pairwise diversity via token-set Jaccard (require < 0.5 between all pairs); if diversity check fails, log a warning and skip discriminative query injection, falling back to existing gap-fill queries unchanged.",
    "Cross-Hypothesis Query Budget Overrun: Adding up to 4 discriminative queries per question in round 2 increases retrieval volume and Brave Search API costs ($0.005/request * 4 = $0.02 extra per question). For tight single-mode budgets this may trigger the adaptive early-stopping gate before discriminative queries fire. Mitigation: inject discriminative queries only when should_stop_early() check shows remaining_budget > $1.50 at the start of round 2; this preserves existing behavior for low-budget runs.",
    "Discriminativeness Score Instability for Vocabulary-Dense Domains: For chemistry or biology questions where hypotheses share heavy technical vocabulary (e.g., both involve 'phosphorylation at Ser473'), all retrieved documents will have similar overlap with both hypotheses, yielding near-zero discriminativeness for all records. Mitigation: if max(discriminativeness_scores) < 0.10 across all ingested documents, disable discriminativeness-weighted slot filling and KEY EVIDENCE tagging for that run and fall back to existing relevance-only ranking.",
    "Synthesis Prompt Structure Disruption: Injecting [KEY EVIDENCE] blocks before claim_clusters may break the established prompt template structure that synthesis models have been optimized against. The model may ignore or be confused by the new block. Mitigation: A/B test KEY_EVIDENCE_FIRST vs. KEY_EVIDENCE_AFTER_CLUSTERS on a 30-question eval before committing; if per-question accuracy is not statistically improved by KEY_EVIDENCE_FIRST, use AFTER_CLUSTERS placement which minimally changes existing prompt structure.",
    "LLM Stance Verification Cost Underestimation in policy.py: The upgraded verifier.py that calls an LLM for high-scoring adversarial records adds a generation cost not currently modeled in estimate_stage_cost('verification'). At claude-sonnet-4-6 pricing ($3/M input tokens), a 2,000-token stance prompt costs ~$0.006 per call. Mitigation: add a verification_llm_calls counter to the budget state and include verification LLM cost in the policy.py cost estimation to prevent budget_remaining underestimation and premature should_stop_early() triggers.",
    "Hypothesis Margin Calibration Coefficient Overfitting: Deriving the +0.12 hypothesis_margin coefficient from a small eval set risks overfitting—high-margin but wrong answers would be systematically overconfident. Mitigation: derive the coefficient from a held-out calibration set of >= 50 questions separate from the test set, cap the total hypothesis_margin contribution at +0.15 regardless of the fitted coefficient, and re-evaluate ECE (Expected Calibration Error) before and after to confirm calibration improves."
  ]
}
```
