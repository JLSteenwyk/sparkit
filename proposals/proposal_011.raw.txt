```json
{
  "proposal_id": "SPARKIT-PROP-001",
  "title": "Evidence-Feedback Iterative Query Refinement with Structured Micro-Claim Triangulation",
  "distinctive_angle": "Turn retrieved evidence into a real-time feedback signal for subsequent queries: extract structured micro-claims (entity, relation, value, condition) from every retrieved passage, map them against question sub-aspects to detect coverage gaps, then generate surgically targeted gap-fill queries—replacing SPARKIT's static round-intent vocabulary with evidence-informed lexical anchors. Pair this with cross-document numerical consistency scoring at the verification stage to surface exact conflicts to the synthesizer rather than burying them in 10 000-character context windows.",
  "summary": "SPARKIT's multi-round retrieval (primary, gap_fill, adversarial, reference) issues queries using static LLM-generated intent expansions that are not informed by what was already found. For HLE-level hard questions, failure typically occurs not because relevant papers are absent but because the precise sub-fact lives in a subsection that does not keyword-match the original query, or because it requires combining specific numerical claims across three separate papers. This proposal adds two interlocking mechanisms: (1) after each retrieval round, an LLM extracts structured micro-claims from retrieved passage text and maps them to pre-identified question sub-aspects, identifying which aspects still lack sufficient evidence; gap-fill queries for the next round are generated from those uncovered aspects plus the titles/abstracts of already-retrieved papers to inject domain-specific terminology. (2) A cross-document numerical consistency layer at verification explicitly extracts and compares numbers, percentages, and quantities across retrieved passages, tagging conflicts with specific values for the synthesizer rather than relying on keyword-scan heuristics. Together these changes improve retrieval recall on paraphrase-heavy technical queries and raise factual precision on quantitative questions by giving the synthesizer a pre-reconciled evidence map instead of a flat concatenated context.",
  "reasoning": "SPARKIT's current gap_fill round generates queries like 'limitations of [topic]' and 'benchmark results [topic]'—useful but generic. These queries may not surface a paper whose abstract says 'we revisit the capacity bounds of [entity] and find [specific result]' if the original question used different terminology. Evidence-feedback loop solves this: if round-1 retrieval finds a paper reporting 'FLOP count: 3.8e23' for a specific model, the gap-fill query can include the model name and year directly, dramatically narrowing vocabulary mismatch. Micro-claim extraction also enables sub-aspect coverage tracking—the system knows it has evidence for 'training compute' but not 'inference latency', so it generates a targeted query for the missing dimension. On the verification side, SPARKIT's current contradiction detector scans for marker words ('contradict', 'negative result'), missing the common case where Paper A reports 73.2% and Paper B reports 68.4% on the same benchmark with no explicit contradiction language. Structured numerical extraction at verification catches this and passes the conflict explicitly to synthesis, enabling the synthesizer to reason about which value is more credible (newer, larger sample, peer-reviewed) rather than hallucinating a reconciliation.",
  "expected_impact": {
    "accuracy_delta_pct_points": 4,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add `extract_micro_claims(passages: list[str], question: str, provider) -> list[dict]` to engine.py. Calls the planning provider with a structured extraction prompt requesting JSON array of {entity, relation, value, condition, source_idx}. Limit to top-10 retrieved passages, max 300 tokens output. Cache result per round.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add `identify_question_aspects(question: str, provider) -> list[str]` to retrieval planning. Calls the planning provider once at orchestration start to decompose the question into 3-6 atomic sub-aspects (e.g., 'training compute', 'benchmark dataset', 'inference latency'). Store in retrieval plan. Reuse decomposition from existing heuristic decomposition path if already present.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add `compute_aspect_coverage(aspects: list[str], micro_claims: list[dict]) -> dict[str, float]` to engine.py. Uses token overlap between each aspect string and micro-claim entity+relation fields to compute per-aspect coverage score in [0,1]. Returns map of aspect→coverage.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add `generate_gap_queries(uncovered_aspects: list[str], retrieved_titles: list[str], retrieved_abstracts: list[str], provider) -> list[str]` to engine.py. Prompts the planning provider with uncovered aspects plus a condensed summary of retrieved paper titles to generate 1-3 targeted queries that incorporate retrieved domain vocabulary. Output replaces or augments the static gap_fill intent queries.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Wire evidence-feedback loop into the multi-round retrieval block in `execute_orchestration`. After round 1 completes and passages are selected, call extract_micro_claims → compute_aspect_coverage → generate_gap_queries. Inject returned queries into the round-2 query list ahead of static intent queries. Guard behind env var SPARKIT_EVIDENCE_FEEDBACK=1 (default 0 initially for safe rollout).",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add Semantic Scholar citation-neighbor expansion to retrieval_service/adapters.py: `fetch_citation_neighbors(paper_id: str, direction: 'references'|'citations', top_k=5) -> list[SearchRecord]`. Calls Semantic Scholar /graph/v1/paper/{id}/{direction}?fields=title,abstract,year,externalIds,authors. For the top-3 most relevant retrieved papers (by SPARKIT relevance score), fetch their top-5 cited-by neighbors and inject as additional candidate records into the selection pool before ingestion deduplication.",
      "owner": "retrieval_service/adapters.py",
      "effort": "medium"
    },
    {
      "step": "Add section-type intent to ingestion: extend `_select_best_chunk` in engine.py to accept a `preferred_section` hint derived from the question's sub-aspects (e.g., 'Results' for quantitative questions, 'Methods' for procedural questions). Score section chunks by both focus-term overlap and section-type match, with preferred_section getting a +2 bonus over the current scoring.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add numerical consistency layer to verifier.py: `extract_quantities(text: str) -> list[dict]` using regex for patterns like `\\d+\\.?\\d*\\s*%`, `\\d+\\.?\\d*[eE][+-]?\\d+`, and named quantities. After existing contradiction scoring, group quantities by extracted entity (from micro-claims or lexical anchors), flag pairs where the same entity has values differing by >10%, and append explicit conflict strings to the `verification_notes` artifact for the synthesizer to reference.",
      "owner": "orchestrator/verifier.py",
      "effort": "medium"
    },
    {
      "step": "Update synthesis prompt templates in engine.py to include a `EVIDENCE_CONFLICTS` section populated from verification_notes when non-empty. Instruct the synthesizer to explicitly reason about each conflict and state which value it relies on and why. This surfaces the conflict-resolution step in the answer trace.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add cost guard: evidence-feedback loop budget (micro-claim extraction + gap-query generation) is capped at 15% of total max_cost_usd via a new reserve slot in should_stop_early. If budget is insufficient for the loop, skip it and fall back to static gap_fill queries.",
      "owner": "orchestrator/policy.py",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Evidence-feedback gap-fill queries: replace static round-2 intent strings with LLM-generated queries derived from micro-claims already retrieved and uncovered question sub-aspects. Queries carry exact domain terminology (model names, dataset names, metric names) from the retrieved evidence pool, eliminating vocabulary mismatch between the question and the sub-papers that contain the answer.",
    "Semantic Scholar citation-neighbor expansion: for the top-3 papers by SPARKIT relevance score, fetch their first-degree reference and citation neighbors via the Semantic Scholar Graph API and inject as additional candidate records into the selection pool. This surfaces papers that are definitively relevant (cited by or citing a known-good paper) but do not keyword-match the original query due to paraphrase, jargon shift, or language differences.",
    "Section-type-aware ingestion: identify whether each question sub-aspect requires quantitative results (→ prefer Results/Tables sections), methodology (→ prefer Methods), or interpretation (→ prefer Discussion/Abstract). Apply a section-type bonus in _select_best_chunk so ingestion extracts the most informationally dense region of the paper for that specific aspect rather than the best purely lexical match, reducing the chance that useful numbers in a Table are skipped in favor of the keyword-dense Introduction.",
    "Entity-anchored mandatory filter injection: extract named entities (chemical names with IUPAC patterns, model family names, dataset names, author+year pairs) from question and from already-retrieved paper titles using existing lexical_anchors logic, then append these as required terms to all subsequent round queries using AND-style syntax where each API supports it (ArXiv all: field, Semantic Scholar with multiple keywords). Reduces false-positive retrievals that match general topic but not the specific entity the question asks about."
  ],
  "evaluation_plan": [
    "HLE-25 balanced subset A/B: run the full HLE-25 balanced benchmark twice under identical budget constraints (max_cost_usd=3.0, mode=research_max), once with SPARKIT_EVIDENCE_FEEDBACK=0 (baseline) and once with SPARKIT_EVIDENCE_FEEDBACK=1 (treatment). Compare exact-match and judge-graded accuracy. Target: ≥3pp improvement; abort rollout if accuracy regresses or cost exceeds 1.4x baseline.",
    "Sub-aspect coverage rate audit: for 25 manually selected hard questions spanning chemistry, biology, ML, and physics, annotate 3-5 required atomic sub-aspects per question. After each run, check whether at least one retrieved passage micro-claim covers each sub-aspect (token overlap ≥0.5). Report per-category coverage rate before and after the feedback loop. Target: coverage rate ≥75% for all categories.",
    "Retrieval precision@10 vs gold reference papers: for questions where ground-truth answer explanations cite specific papers (HLE gold or manually annotated), compute the fraction of gold papers appearing in SPARKIT's top-10 retrieved and top-10 ingested sets. Measure separately for baseline (no feedback) and treatment (with feedback + citation expansion). Target: precision@10 for ingested documents increases by ≥10pp.",
    "Numerical conflict detection recall: construct a test set of 15 questions with known quantitative conflicts in the literature (e.g., two papers reporting different accuracy on the same benchmark). Run verification stage and check whether the structured numerical consistency layer flags the conflict in verification_notes. Target: recall ≥80% of known conflicts identified vs. ≤20% for keyword-scan baseline.",
    "Cost and latency regression check: confirm that median per-question cost does not exceed 1.35x baseline and median latency does not exceed 1.5x baseline across HLE-25, ensuring the evidence-feedback overhead (micro-claim extraction + gap-query generation calls) stays within acceptable budget headroom."
  ],
  "risks": [
    "Micro-claim extraction LLM calls add one extra provider round-trip per retrieval round (typically 1-2 calls), increasing median cost by an estimated 8-15% and latency by 4-10s. Mitigation: budget cap at 15% of max_cost_usd; skip loop if budget insufficient.",
    "Citation-neighbor expansion via Semantic Scholar Graph API may hit rate limits (100 req/5 min unauthenticated, 1 req/s authenticated). If top-3 anchor papers all require neighbor fetches simultaneously, burst risk exists. Mitigation: serialize neighbor fetches with 1.1s inter-request delay and cache by paper_id within a session.",
    "Micro-claim extraction quality degrades for non-English papers, highly mathematical papers (equations without verbal claims), and very short abstracts (<100 words). The aspect-coverage computation will undercount covered aspects in these cases, generating spurious gap queries. Mitigation: fall back to static gap_fill queries when micro-claim count < 3 after round 1.",
    "Gap-query generation using retrieved paper vocabulary may cause the system to anchor on a wrong paper's terminology if round-1 retrieval retrieved an off-topic paper with high lexical overlap. This would steer round-2 retrieval away from correct sources. Mitigation: only inject terminology from papers with SPARKIT relevance score ≥ 1.5 (above median) into the gap-query generation prompt.",
    "Numerical consistency layer using regex may produce false-positive conflicts for quantities with different units (e.g., 3.8 TFLOP vs 3.8e12 FLOP are the same) or different reference points (top-1 vs top-5 accuracy). Mitigation: include unit normalization for common STEM units (FLOP/GFLOP/TFLOP, %, pp) and tag each conflict with its extracted context string so the synthesizer can resolve unit ambiguity."
  ]
}
```
