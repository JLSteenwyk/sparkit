Now I have a comprehensive understanding of the entire SPARKIT pipeline. Let me produce the proposal JSON.

```json
{
  "proposal_id": "SPARKIT-PROP-2026-001",
  "title": "Atomic Sub-Claim Mining with Multi-Passage Semantic Scoring and Coverage-Gated Gap-Fill Retrieval",
  "distinctive_angle": "SPARKIT's critical bottleneck is not retrieval breadth but evidence granularity: each selected document contributes exactly ONE claim text (title + first sentence of best chunk), discarding >98% of full-text content, while synthesis receives 'Title (year) reports: first sentence' instead of the specific passages that directly address atomic factual sub-questions. This proposal replaces document-level claim extraction with atomic-fact-level mining: generate explicit factual sub-claims → score ALL extracted passages from each document against each sub-claim → feed synthesis only passage-subclaim pairs with demonstrated grounding → trigger targeted gap-fill for sub-claims with zero coverage, creating a closed feedback loop between what the question needs and what evidence is sent to synthesis.",
  "summary": "Extend the SPARKIT orchestration pipeline with four tightly integrated components: (1) a sub-claim generator that decomposes each hard STEM question into 4-6 atomic factual assertions that must be verified to answer it; (2) a multi-passage extractor that replaces the current single best-chunk selection with a ranked pool of all above-threshold passages from each ingested document; (3) an LLM passage scorer that maps extracted passages to sub-claims and filters to only grounded passage-subclaim pairs; and (4) a coverage-gated gap-fill round that detects uncovered sub-claims and generates targeted queries until coverage meets a configurable threshold or budget exhausts. The synthesis prompt is restructured from a flat claim-text list into a sub-claim-anchored evidence block, giving the synthesis model the precise textual evidence it needs rather than first-sentence proxies.",
  "reasoning": "The HLE benchmark questions are hard precisely because they require very specific mechanistic, numerical, or nomenclature facts (e.g., 'the half-life of reaction X under condition Y', 'the stereochemistry of intermediate Z') that are buried in specific sections of specific papers, not in titles or abstracts. The current pipeline's _select_best_section_chunk picks one 1,200-char chunk per doc, _first_sentence truncates that to 220 chars, and the resulting claim_text is almost always too abstract to discriminate between MCQ options. The synthesis model—even gpt-5.2 or claude-opus-4-6—cannot produce correct answers from evidence that does not contain the relevant facts. By decomposing the question into atomic sub-claims, extracting all candidate passages from each document, scoring passage-subclaim relevance with a targeted LLM call, and only forwarding grounded passages to synthesis, SPARKIT provides evidence that actually resolves the sub-questions. The coverage-gated gap-fill closes the loop: when sub-claim coverage is below threshold after the first retrieval rounds, new targeted queries are auto-generated and retrieval restarts for those specific gaps. This is grounded in the exact SPARKIT failure mode: retrieval_round_1 often returns relevant paper titles but irrelevant section text because the best section is buried in the Methods or Results section that the 1,200-char chunk misses, and the adaptive gate terminates early when new_unique_docs drops below adaptive_min_new_docs even though existing docs have unexploited content.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _generate_sub_claims(question, provider) in engine.py: prompt the planning provider to decompose the question into 4-6 atomic factual assertions in structured pipe-delimited format (e.g., 'The leaving group in the reaction is X | The configuration at the chiral center inverts under SN2 | The half-life at 25C is in range Y'). Parse output into list[str]. Fall back to heuristic _heuristic_decomposition sub_claims on failure. Store in ResearchPlan.sub_claims (already exists as a field but is currently populated with sentence-split question fragments, not LLM-derived atomic facts). Called once per run before retrieval planning.",
      "owner": "orchestrator/engine",
      "effort": "medium"
    },
    {
      "step": "Extend _decompose_retrieval to emit one targeted query per sub-claim as a new intent key 'sub_claim_targets': for each sub-claim, call _dedupe_queries([f'{sub_claim}', f'{stem} {sub_claim}'], max_items=2). Merge into merged_intents and include in retrieval rounds. This ensures retrieval is anchored to atomic facts rather than only the question stem and option text.",
      "owner": "orchestrator/engine",
      "effort": "low"
    },
    {
      "step": "Replace single-chunk selection in the ingestion loop (current: _select_best_section_chunk returns one (heading, text) tuple) with multi-passage extraction: iterate all parsed_sections, apply _chunk_text to each section, score all chunks with _chunk_relevance_score, collect all chunks above a configurable threshold SPARKIT_PASSAGE_RELEVANCE_THRESHOLD (default 2.0), cap at SPARKIT_MAX_PASSAGES_PER_DOC (default 4), deduplicate by first 80 chars. Store as list[tuple[str, str, float]] = (heading, chunk, score). If no chunk meets threshold, fall back to current best-chunk behavior.",
      "owner": "orchestrator/engine + ingestion",
      "effort": "medium"
    },
    {
      "step": "Add _score_passages_against_subclaims(passages_by_doc, sub_claims, provider, budget_tokens=800) in engine.py: build a compact batch prompt listing up to 16 candidate passages and up to 6 sub-claims, ask provider to return a binary relevance matrix in structured text (e.g., 'P1->SC2: yes, P3->SC1: yes, P3->SC4: yes'). Parse to dict[passage_key, list[sub_claim_idx]]. This is a single targeted LLM call (not per-passage), capped by SPARKIT_PASSAGE_SCORING_MAX_TOKENS env var. Passages with at least one sub-claim match are flagged as grounded. Fall back to score-threshold selection if LLM call fails.",
      "owner": "orchestrator/engine",
      "effort": "medium"
    },
    {
      "step": "Add sub-claim coverage tracking and gap-fill round in execute_orchestration: after ingestion loop, for each sub-claim compute coverage = (count of grounded passages mapping to it) / 1. Sub-claims with coverage == 0 are 'uncovered'. If uncovered_count >= SPARKIT_SUBCLAIM_GAP_THRESHOLD (default 2) and budget permits (should_stop_early check), generate gap-fill queries: for each uncovered sub-claim call search_literature(sub_claim, max_results=effort.retrieval_min_results), ingest top results, re-run passage scoring. This is a new retrieval stage 'retrieval_subclaim_gap_fill' inserted after normal retrieval rounds but before verification.",
      "owner": "orchestrator/engine",
      "effort": "medium"
    },
    {
      "step": "Restructure synthesis prompt: replace the flat evidence_lines block (current: '- Title (year) reports: first sentence' x N) with a sub-claim anchored evidence block: for each sub-claim, list up to 3 grounded passages with their source doc title and section. Ungrouped passages go into a 'general evidence' block. Modify _build_synthesis_prompt and _build_mcq_option_judge_prompt to accept sub_claim_evidence: dict[str, list[str]] and render it as 'Sub-claim 1 [assertion text]:\\n  - passage a\\n  - passage b\\n'. This is the highest-value change: synthesis now sees textual evidence directly addressing each sub-question.",
      "owner": "orchestrator/engine",
      "effort": "low"
    },
    {
      "step": "Wire new sub-claim coverage into CalibrationFeatures: add sub_claim_coverage_rate: float (average per-sub-claim coverage, 0-1) as a new feature. Adjust calibrate_answer to add 0.10 * sub_claim_coverage_rate term (offset 0.10 reduction in the constant to keep scale). Add TraceStage 'subclaim_coverage' with artifacts showing per-sub-claim grounded passage count.",
      "owner": "orchestrator/calibration",
      "effort": "low"
    },
    {
      "step": "Add SPARKIT_SUBCLAIM_MINING env flag (default False, enable explicitly) to gate the feature behind a flag for staged rollout. When disabled, the pipeline runs exactly as before. When enabled, the 4 new components activate. This allows A/B benchmarking on the same question set.",
      "owner": "orchestrator/engine",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Sub-claim targeted query generation: for each of 4-6 LLM-derived atomic factual assertions, emit a dedicated retrieval query rather than relying solely on the question stem, option text, or heuristic segments. For chemistry MCQ questions, atomic assertions like 'retention of configuration at carbon center in SN1 reaction' produce far more relevant results from CrossRef/Semantic Scholar/EuropePMC than the full question stem, which often contains MCQ scaffolding that degrades search quality.",
    "Multi-passage extraction per document replacing single best-chunk: the current _select_best_section_chunk discards all but one 1,200-char chunk per document. A 10-page chemistry paper may have the mechanism in Section 3 (Methods) and the numerical result in Section 5 (Results), both relevant to different sub-claims. Extracting and scoring all above-threshold chunks captures this cross-section signal without increasing the number of documents fetched.",
    "Sub-claim coverage gap-fill retrieval round: after the standard multi-round retrieval and ingestion, uncovered sub-claims (those with zero grounded passages) trigger a dedicated retrieval stage using the sub-claim text as the query directly. This is distinct from the existing retrieval_round_2_gap_fill which uses heuristic 'limitations' and 'benchmark comparison' suffixes. Sub-claim-direct queries are far more specific: e.g., 'trans-2-butene dehydrohalogenation stereochemistry anti-periplanar' instead of 'trans-2-butene limitations'.",
    "Entity-anchored query normalization for chemistry/biology questions: detect chemical entity strings (IUPAC names, molecular formulas, reaction class names, gene identifiers) using the existing _extract_lexical_anchors function's hyphen/slash regex patterns, then generate normalized search aliases. For example, 'E2 elimination' → also query 'bimolecular elimination' and 'beta-elimination'; 'SN2' → also 'backside attack substitution'. These aliases are added to sub-claim-targeted queries, dramatically increasing recall for questions where the paper uses different terminology than the question."
  ],
  "evaluation_plan": [
    "Sub-claim coverage rate check: for each HLE benchmark question run with SPARKIT_SUBCLAIM_MINING=true, compute sub_claim_coverage_rate = (number of sub-claims with ≥1 grounded passage) / (total sub-claims generated). Assert p50 coverage >= 0.67 and p10 coverage >= 0.33 across the test set. A run where coverage < 0.33 and the final answer is correct is likely getting lucky on prior knowledge rather than evidence grounding.",
    "Evidence faithfulness audit: on a 25-question stratified sample from the HLE gold set, run a post-hoc LLM judge (separate from the synthesis provider) with prompt: 'Given these evidence passages and the final answer, identify any claim in the answer that cannot be traced to at least one provided passage. Return: grounded_claims_count, hallucinated_claims_count, faithfulness_rate.' Assert faithfulness_rate >= 0.80 for SPARKIT runs and compare to direct-call baselines which will have faithfulness_rate near 0 by construction (no evidence provided).",
    "Answer stability under evidence subsampling: for each benchmark question, run synthesis 5 times with a random 70% subsample of grounded passages (keeping the sub-claim structure). Compute answer_stability = (fraction of 5 runs agreeing on the same answer). For MCQ, assert stability >= 0.60 for correctly answered questions (unstable correct answers indicate evidence is borderline). For open-ended, compute semantic similarity between runs using LLM-as-judge. Track stability delta between SPARKIT-with-subclaim-mining vs SPARKIT-baseline to confirm the new pipeline produces more robust evidence grounding.",
    "Passage relevance quality gate: before synthesis, compute avg_passage_relevance = mean(chunk_relevance_score for all grounded passages). Assert avg >= SPARKIT_MIN_PASSAGE_RELEVANCE (configurable, suggested default 3.0 from the _chunk_relevance_score scale). If avg falls below threshold, log a retrieval_quality_warning in the TraceStage and increment a counter in ObservabilityStore. This creates a per-run signal that the retrieval phase failed to find on-topic content, distinguishing 'question is genuinely hard' from 'retrieval returned wrong papers'.",
    "Regression gate against direct-call baselines: in the benchmark runner (direct_call_runner.py), add a sub_claim_coverage field to the prediction record. For each HLE-gold run with SPARKIT_SUBCLAIM_MINING=true, compare accuracy on questions where sub_claim_coverage_rate >= 0.67 vs < 0.67. Assert that high-coverage questions have statistically higher accuracy (expected: +10-15 pct points). This validates the core hypothesis that evidence grounding drives accuracy improvements."
  ],
  "risks": [
    "Sub-claim generation quality degrades for questions that require domain expertise SPARKIT's planning provider lacks: if the LLM generates generic sub-claims ('the reaction proceeds by a standard mechanism') instead of specific testable assertions, the downstream passage scoring and gap-fill provide no signal. Mitigation: include few-shot examples of good sub-claims in the _generate_sub_claims prompt, and fall back to heuristic decomposition if all generated sub-claims are shorter than 15 chars or contain only stopwords.",
    "Passage scoring LLM call adds $0.01-0.05 per run (depending on provider and passage count), increasing cost by 20-50% for research_max mode. For the HLE benchmark at 25 questions, this is manageable, but at scale or with expensive providers (gpt-5.2-pro) it is significant. Mitigation: make SPARKIT_SUBCLAIM_MINING=false the default, expose as opt-in, and use the cheapest configured provider for passage scoring (add provider_plan.scoring role defaulting to planning provider).",
    "Multi-passage extraction increases synthesis prompt length: if 4 docs each contribute 4 passages of 1,200 chars, the evidence block grows from ~800 chars (current 8 first-sentences) to ~19,200 chars. For context-limited providers or high per-token costs, this may hit limits or cause significant cost increases. Mitigation: cap total evidence chars at SPARKIT_MAX_EVIDENCE_CHARS (default 8,000), prioritize highest-scored passages, and truncate to first 400 chars per passage when total exceeds cap.",
    "Sub-claim gap-fill retrieval adds a latency-unbounded extra round: if 4 sub-claims are uncovered and each triggers search_literature across 5 adapters, worst-case latency addition is 4 * 5 * 15s = 300s. Mitigation: run gap-fill retrieval for all uncovered sub-claims in a single parallel batch (not per sub-claim sequential), cap at SPARKIT_SUBCLAIM_GAP_MAX_DOCS (default 6) total additional records, and apply the existing should_stop_early budget check before triggering the gap-fill stage.",
    "The entity-anchored query normalization for chemistry may generate incorrect synonyms for ambiguous abbreviations: 'E1' could mean 'unimolecular elimination' or an energy level in spectroscopy. Mitigation: the normalization is additive (aliases are appended to the primary query, not replacing it), so false expansions add noise at worst rather than replacing correct signal; apply only when _extract_lexical_anchors already finds a high-signal anchor (length >= 6, contains digit or hyphen)."
  ]
}
```
