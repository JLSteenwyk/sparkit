Now I have everything needed for a deeply-informed, genuinely novel proposal. Let me compose it:

```json
{
  "proposal_id": "SCAL-RRL-048",
  "title": "Synthesis-Claim Attribution Loop with Hallucination-Triggered Repair Retrieval",
  "distinctive_angle": "Every prior SPARKIT proposal (001-047) intervenes before or during retrieval. This proposal adds the first post-synthesis feedback loop: after generating the initial answer draft, it extracts specific factual claims from that draft, scores each against retrieved passages, flags ungrounded claims as hallucination candidates, and fires a dedicated repair retrieval round with queries targeting only the unsupported claims. The synthesis LLM's own output drives additional retrieval — converting the forward-only pipeline into a closed-loop grounding system.",
  "summary": "SPARKIT's pipeline is strictly forward: retrieve → ingest → verify → synthesize. No mechanism checks whether the synthesis draft's specific factual claims are actually traceable to retrieved passages. A synthesis LLM operating on incomplete evidence often generates plausible-but-wrong facts from training memory rather than from retrieved evidence, and the existing calibration formula (calibration.py:21-29) cannot distinguish evidence-grounded correctness from training-data confabulation because support_coverage measures retrieved-document claims, not synthesis-draft claims. This proposal adds a four-step post-synthesis attribution pass: (1) _extract_draft_claims() prompts the planning-role LLM to extract 5-8 specific factual assertions from the synthesis draft as structured output; (2) _score_claim_attribution() scores each extracted claim against the combined claim_texts pool using token overlap, flagging claims below SPARKIT_REPAIR_MIN_SCORE (default 0.12) as ungrounded; (3) _build_repair_queries() converts each flagged claim into 2 targeted retrieval queries; (4) a retrieval_round_repair stage executes queries via the existing search_literature() path, ingests new documents via fetch_and_parse(), and triggers a second synthesis pass with the repair evidence injected and ungrounded claims explicitly labeled in the synthesis prompt. Three additional retrieval improvements are bundled: a query specificity entropy gate in _build_round_queries_from_plan() that auto-augments underspecified queries with focus_terms before API submission; abstract-trigram semantic deduplication in aggregator.py that merges near-duplicate cross-source records missed by DOI-only deduplication; and a claim-source provenance check that triggers a targeted diversification retrieval round when more than 50% of claim_texts trace to a single document title, preventing synthesis from relying on a single potentially-biased or incorrect paper.",
  "reasoning": "Hard HLE questions fail SPARKIT via two distinct and currently conflated pathways. Retrieval pathway failures (right papers never found) are targeted by proposals 001-047. Synthesis pathway failures (right papers retrieved but LLM uses training memory instead) are entirely unaddressed. The synthesis LLM is capable of generating technically plausible answers to expert-level STEM questions from its training data alone, bypassing the retrieved evidence entirely. The existing verifier.py detects contradictions between retrieved documents, not between the synthesis draft and retrieved documents. The support_coverage calibration feature counts claims extracted from retrieved documents — it has no visibility into what the synthesis LLM actually wrote. On HLE-style chemistry and biology questions, the synthesis model frequently hallucinate specific enzyme names, reaction rates, gene functions, or molecular interactions that sound correct but differ from the ground truth by one critical detail. These hallucinations are indistinguishable from correct answers in the calibration signal unless the synthesis output is explicitly attributed to retrieved evidence. By extracting claims from the draft and checking them against the evidence corpus, SCAL-RRL identifies exactly which facts the LLM is asserting without evidence — then retrieves evidence for those specific gaps. This is fundamentally more precise than generic gap-fill retrieval (which queries for broad topic coverage) because the repair queries are derived directly from the unsupported claim text, maximally targeting the specific paper that would confirm or deny what the LLM asserted. The secondary improvements address orthogonal failure modes: query specificity gating prevents wasted API calls on underspecified queries that return high-count low-precision results; semantic deduplication prevents the same paper's findings from appearing as multiple independent evidence claims after being indexed by three different academic APIs; claim-source diversity enforcement prevents the synthesis from treating a single paper's perspective as consensus, which is a known failure mode on contested or evolving scientific questions.",
  "expected_impact": {
    "accuracy_delta_pct_points": 5,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _extract_draft_claims(synthesis_text: str, question: str, planning_provider: str) -> list[str] in engine.py. Prompt the planning-role LLM with: 'Extract 5-8 specific factual assertions from this answer draft. Each assertion must be a complete claim containing at least one named entity or numerical value. Output as a JSON array of strings.' Parse the JSON array response; fall back to empty list on parse failure. Guard with SPARKIT_ENABLE_HALLUCINATION_REPAIR env var (default 1). Cap extraction to claims containing at least 3 non-stopword tokens. Call after the initial synthesis draft is generated, before the final answer is assembled.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _score_claim_attribution(claim: str, claim_texts: list[str]) -> float in engine.py. Tokenize claim using the existing _tokenize() function (line 44 in aggregator.py, or a local copy). Compute token overlap fraction between the claim and the concatenation of all claim_texts: score = len(claim_tokens & evidence_token_universe) / max(len(claim_tokens), 1). Return max over per-claim-text scores. Claims below SPARKIT_REPAIR_MIN_SCORE (default 0.12) are added to ungrounded_claims list. Cap ungrounded_claims at SPARKIT_REPAIR_MAX_CLAIMS (default 3) to bound the repair round cost, selecting the lowest-scoring claims.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "low"
    },
    {
      "step": "Add _build_repair_queries(ungrounded_claims: list[str], focus_terms: list[str]) -> list[str] in engine.py. For each ungrounded claim: strip LLM hedge phrases ('according to', 'studies show', 'research indicates'), extract entity tokens (non-stopwords > 5 chars), combine with top-3 focus_terms from RetrievalPlan.focus_terms, and form 2 queries: '{entity_tokens} {focus_terms}' and '{entity_tokens} experimental evidence mechanism'. Deduplicate via _dedupe_queries(). Cap total repair queries at SPARKIT_REPAIR_MAX_QUERIES (default 6). Optionally call planning LLM with SPARKIT_REPAIR_USE_LLM_QUERIES=1 for higher-quality query generation at additional cost.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "low"
    },
    {
      "step": "Insert a retrieval_round_repair stage in the orchestration flow immediately after the initial synthesis draft and before final answer assembly. Check should_stop_early() from policy.py before executing; skip repair if budget is exhausted. Execute repair queries via the existing search_literature() path with max_results=8. Call fetch_and_parse() for up to 3 new documents (excluding DOIs already ingested in earlier rounds, using the existing seen DOI set). Extract best sections via _select_best_section_chunk(). Add new claim_texts entries tagged with source='repair_round'. Record TraceStage with name='retrieval_round_repair' and artifacts containing ungrounded_claim_count, repair_queries, repair_docs_found, claims_resolved.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Trigger re-synthesis if repair round found at least 1 new document. Rebuild the synthesis prompt via _build_synthesis_prompt() with the expanded claim_texts, adding an explicit grounding instruction: 'The following claims from the initial draft lacked evidence support and have been targeted for additional retrieval: {ungrounded_claim_texts}. Prioritize using the additional evidence below to answer these sub-questions.' Gate re-synthesis on repair_docs_found > 0 to avoid a redundant LLM call when repair retrieval returned nothing. Use the existing synthesis provider; do not invoke ensemble mode for repair to avoid cost explosion.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add query specificity gating in _build_round_queries_from_plan() in engine.py. Before each call to search_literature(), compute a specificity score: count tokens > 7 chars not in _STOPWORDS (engine.py line 51) divided by total query token count. If score < SPARKIT_QUERY_SPECIFICITY_GATE (default 0.25), augment the query by appending the top-2 unused focus_terms from RetrievalPlan.focus_terms (terms not already present in the query string). Log augmented queries in the round stage artifact under 'specificity_augmented_queries'. Skip augmentation for adversarial and falsify-intent queries where general language is intentional.",
      "owner": "services/orchestrator/app/engine.py:_build_round_queries_from_plan",
      "effort": "low"
    },
    {
      "step": "Add abstract-trigram semantic deduplication in services/retrieval_service/app/aggregator.py inside search_literature(), after the existing DOI/URL deduplication pass (line 182-191). For each pair of records from different sources where both have abstracts of length >= 100 chars, compute Jaccard similarity on character trigrams of the first 400 characters of each abstract. If similarity > SPARKIT_SEMANTIC_DEDUP_THRESHOLD (default 0.75), merge the pair: keep the record with the longer abstract, higher year, or non-None DOI. This catches arXiv preprint + Semantic Scholar indexed version of the same paper that have different DOIs. Cap pairwise comparison to O(N^2) pairs only when len(deduped) <= 30 to bound CPU time.",
      "owner": "services/retrieval_service/app/aggregator.py",
      "effort": "medium"
    },
    {
      "step": "Add claim-source provenance diversity check in engine.py after claim_texts are assembled from ingested documents. Build a per-document title to claim count map. If any single title accounts for > SPARKIT_CLAIM_SOURCE_DOMINANCE (default 0.50) of all claim_texts, generate 2 diversity queries: '{focus_terms} independent study alternative approach' and '{question_core_tokens} meta-analysis review'. Execute via search_literature() with max_results=6. Ingest up to 2 new documents. Tag new claims as source='diversity_round'. Record TraceStage with name='claim_source_diversity_repair'. Gate behind SPARKIT_ENABLE_SOURCE_DIVERSITY_REPAIR=1 (default 1).",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend CalibrationFeatures in calibration.py with two new fields: hallucination_repair_triggered: bool (default False) and repair_coverage_rate: float (default 0.0, fraction of ungrounded claims that found supporting repair evidence). In calibrate_answer(), add: raw += 0.05 * repair_coverage_rate if hallucination_repair_triggered else 0.0. This rewards runs where repair retrieval successfully grounded previously-unsupported claims. Update features_to_dict() to include the new fields. Update the CalibrationStore schema (or use a JSON extras column) to persist these new features.",
      "owner": "services/orchestrator/app/calibration.py",
      "effort": "low"
    },
    {
      "step": "Update policy.py estimate_stage_cost() to account for repair round: add repair_retrieval cost estimate (SPARKIT_REPAIR_MAX_QUERIES * max(adapter_costs) + 3 ingestion calls). Add estimate_repair_round_cost() helper. Check this estimate in the should_stop_early() guard before triggering repair round in engine.py so the repair loop respects max_cost_usd budget. Update docs/configuration.md with all new env vars: SPARKIT_ENABLE_HALLUCINATION_REPAIR, SPARKIT_REPAIR_MIN_SCORE, SPARKIT_REPAIR_MAX_CLAIMS, SPARKIT_REPAIR_MAX_QUERIES, SPARKIT_REPAIR_USE_LLM_QUERIES, SPARKIT_QUERY_SPECIFICITY_GATE, SPARKIT_SEMANTIC_DEDUP_THRESHOLD, SPARKIT_ENABLE_SOURCE_DIVERSITY_REPAIR, SPARKIT_CLAIM_SOURCE_DOMINANCE.",
      "owner": "services/orchestrator/app/policy.py + docs/configuration.md",
      "effort": "low"
    },
    {
      "step": "Add tests in services/orchestrator/tests/test_synthesis_quality.py: (a) _extract_draft_claims parsing with fixture synthesis text asserts 5-8 claim strings; (b) _score_claim_attribution returns < 0.12 for a claim not present in evidence and > 0.12 for a claim paraphrasing evidence; (c) _build_repair_queries produces <= SPARKIT_REPAIR_MAX_QUERIES queries from 3 ungrounded claims; (d) repair stage TraceStage artifact keys are present in result; (e) abstract trigram deduplication merges a fixture pair of near-duplicate cross-source records and retains the richer record; (f) query specificity gating augments a short underspecified query and leaves a long specific query unchanged; (g) calibration raw score increases when repair_coverage_rate=1.0 vs 0.0.",
      "owner": "services/orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Hallucination-triggered repair retrieval: converts specific factual claims in the synthesis draft that lack evidence attribution into targeted retrieval queries, then executes a repair round via the existing search_literature() infrastructure. Unlike all prior retrieval rounds (primary, gap-fill, adversarial, option-hypotheses) which are planned from the question text before any synthesis occurs, repair queries are derived from what the synthesis LLM actually asserted — making them maximally precise for the specific gaps. A claim like 'AKT1 phosphorylates FOXO3 at Ser253' generates a repair query 'AKT1 FOXO3 Ser253 phosphorylation mechanism' that is vastly more targeted than any question-level intent query, directly finding the paper that either confirms or denies the specific fact.",
    "Query specificity entropy gate in _build_round_queries_from_plan(): before each search_literature() call, computes a specificity score (entity-density proxy: long non-stopword token fraction). Queries below threshold are auto-augmented with the most domain-specific focus_terms from RetrievalPlan. This prevents low-precision queries like 'protein binding mechanism review' from flooding the aggregator with off-topic high-count results that dilute the evidence pool and reduce support_coverage, while leaving already-specific queries like 'ubiquitin E3 ligase RING domain substrate recognition' unchanged.",
    "Abstract-trigram semantic deduplication across sources in aggregator.py: the current DOI-based deduplication misses the same paper indexed under different DOIs or without a DOI (arXiv preprint vs. published version, OpenAlex vs. Crossref, Europe PMC vs. Semantic Scholar). Adding character-trigram Jaccard similarity check (threshold 0.75) on abstract prefixes detects these cross-source duplicates and merges them, preserving the richest record. This prevents the same paper's findings from contributing 3-4 independent-looking claim_texts to synthesis, which inflates support_coverage and evidence_count calibration features without adding real information.",
    "Claim-source provenance diversity enforcement with targeted diversification retrieval: after claim_texts are assembled, checks whether a single document title dominates the evidence pool above a configurable threshold. If so, fires 2 diversification queries seeking independent studies, alternative approaches, or meta-analyses. This addresses a systematic failure mode where the primary retrieval round happens to surface one highly-relevant paper that scores above all others on _record_relevance_score(), causing SPARKIT to ingest it deeply (multiple passages, multiple claims) while neglecting equally relevant papers with different perspectives. Synthesis that relies on one paper as its primary evidence source is highly vulnerable to that paper's errors, biases, or scope limitations.",
    "Synthesis-grounding feedback prompt injection: when repair evidence is found for a previously ungrounded claim, the second synthesis prompt explicitly labels both the ungrounded claim and the repair evidence snippet, instructing the synthesis LLM to use the retrieved passage rather than its training-data response for that sub-question. This is more effective than simply appending new evidence to the prompt because it creates a direct correspondence between the claim the LLM asserted incorrectly and the specific passage it should use instead, reducing the probability that the LLM re-generates the same hallucinated response from training memory in the second synthesis pass."
  ],
  "evaluation_plan": [
    "Hallucination rate measurement pre/post repair: for each run where SPARKIT_ENABLE_HALLUCINATION_REPAIR=1, log ungrounded_claim_count (claims with attribution score < SPARKIT_REPAIR_MIN_SCORE) before and after repair. Track repair_coverage_rate = ungrounded_claims_resolved / ungrounded_claim_count per run. A repair_coverage_rate > 0.40 means the repair round is finding evidence for more than 40% of initially unsupported claims, indicating the retrieval mechanism is working. If repair_coverage_rate < 0.20 across 20 consecutive runs, the SPARKIT_REPAIR_MIN_SCORE threshold is likely set too low (flagging borderline-grounded claims that retrieval cannot improve).",
    "Accuracy improvement on repair-triggered question subset: split HLE-gold-149 results into repair-triggered (ungrounded_claim_count >= 1 in first pass) and non-triggered groups. Measure exact-match accuracy separately for each group before and after the proposal. The proposal is specifically designed to improve the repair-triggered group. Require +4pp minimum on the repair-triggered group to declare signal positive. If the repair-triggered group shows no improvement but non-triggered group regresses (possible if repair round adds noisy evidence), disable re-synthesis and use repair evidence only for the final calibration signal.",
    "False-positive claim extraction audit: manually inspect _extract_draft_claims() output for 30 randomly sampled HLE questions to verify that flagged ungrounded claims are specific factual assertions (containing named entities, numerical values, or mechanistic relationships) rather than generic statements that are paraphrases of the question itself. Flag runs where > 30% of extracted claims are generic. Adjust the extraction prompt to require entity tokens if generic-claim rate is high. This audit must be performed before enabling the repair loop in full HLE-149 runs to prevent wasted repair retrieval budget on uninformative claim extraction.",
    "Semantic deduplication noise reduction measurement: on a 30-question benchmark subset, log the count of records before and after the semantic deduplication pass, along with the fraction of merged pairs from the same vs. different academic adapters. Expected outcome: 10-20% reduction in apparent unique records, with > 80% of merges being cross-source duplicates (arXiv+S2, Crossref+OA) rather than genuinely different papers. If > 20% of merges are genuinely different papers (verified by manual inspection of 10 merged pairs), raise SPARKIT_SEMANTIC_DEDUP_THRESHOLD to 0.85 to reduce false merge rate.",
    "Query specificity augmentation ROI on hard questions: for queries that were augmented by the specificity gate (specificity_score < 0.25 before augmentation), compare the relevance scores of documents retrieved by the augmented vs. original query on a 20-question held-out set. Measure fraction of augmented queries where the top result from the augmented version has higher _record_relevance_score than the top result from the original version. Require > 60% improvement rate to justify the specificity gate. Also verify that no augmented query returns zero results (focus_terms may sometimes over-constrain the query).",
    "End-to-end cost and latency regression gate: run SCAL-RRL on 10 representative HLE questions and assert: (a) mean per-question cost increase vs baseline routed_frontier_plus is < 50% (the repair round adds at most 6 retrieval queries + 3 ingestion calls + 1 synthesis call); (b) mean latency increase is < 60 seconds; (c) hard abstain rate does not increase by more than 3 percentage points (repair evidence should not push contradiction_flags above the abstain threshold by adding noisy repair documents). These regression checks must pass before enabling SPARKIT_ENABLE_HALLUCINATION_REPAIR=1 in production benchmark runs."
  ],
  "risks": [
    "Claim extraction quality bottleneck: if the planning LLM extracts generic or question-paraphrasing claims rather than specific factual assertions, the repair queries will be too broad and the repair round will surface the same high-level papers already retrieved in round 1. Mitigation: use a structured extraction prompt with 3 in-context examples requiring entity names and numerical values; add a post-extraction filter that rejects claims with fewer than 3 non-stopword tokens > 5 chars; log extraction quality as a named observability metric.",
    "Re-synthesis regression when repair evidence is off-topic: if repair retrieval returns documents that are tangentially related (e.g., a repair query for an ungrounded claim about 'BRCA2 fidelity in homologous recombination' returns papers about BRCA2 in breast cancer epidemiology), the second synthesis pass may be distorted by irrelevant new evidence. Mitigation: gate re-synthesis on a minimum _record_relevance_score threshold (> 0.15) for repair documents before ingesting them; if repair documents score below threshold, skip re-synthesis and use the repair evidence only as a calibration signal (updating repair_coverage_rate).",
    "Budget overrun from repair round on constrained configurations: in research_max mode, 5 retrieval rounds + 1 repair round + 1 diversity round could breach max_cost_usd. Mitigation: call should_stop_early() before executing each secondary round (repair, diversity), passing the BudgetState with remaining_budget_usd checked against estimate_repair_round_cost(); skip secondary rounds if less than 1.3x the estimated cost remains to prevent hard budget violations.",
    "Semantic deduplication merging genuinely distinct papers with similar abstracts: two papers on the same topic (e.g., two independent replications of the same experiment) may have high abstract trigram similarity despite reporting different results. Merging them would discard one paper's findings, potentially removing a contradiction signal needed by the verifier. Mitigation: only merge records where title similarity is also above 0.60 (requires two conditions not one); preserve the discarded record in a shadow pool accessible to verifier.py for contradiction detection even if excluded from synthesis evidence.",
    "Attribution score threshold miscalibration for technical terminology: a synthesis claim using domain-specific abbreviations (e.g., 'PCNA K164 monoubiquitination') may have low token overlap with retrieved passages that use spelled-out forms ('proliferating cell nuclear antigen lysine 164 mono-ubiquitylation') despite being semantically identical. This would falsely trigger repair retrieval for a well-grounded claim. Mitigation: add a set of domain-specific abbreviation expansions (reuse/extend the existing alias dict in engine.py used for MCQ lexical scoring) to normalize claim tokens before attribution scoring; also raise SPARKIT_REPAIR_MIN_SCORE to 0.15 initially and tune down only if false-positive claim flagging rate is below 10% on a manual audit."
  ]
}
```
