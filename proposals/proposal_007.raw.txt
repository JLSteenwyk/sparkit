Now I have all the detail I need. Let me produce the proposal JSON.

```json
{
  "proposal_id": "SPARKIT-2026-EGR-001",
  "title": "Evidence-Graded Retrieval: LLM-Batch Abstract Scoring, Sub-Claim Coverage Loops, and Mechanism-Aware Claim Extraction",
  "distinctive_angle": "Three mutually-reinforcing changes that replace every lexical-overlap proxy in SPARKIT's pipeline with a semantically-grounded signal: (1) a batched LLM relevance pass on raw retrieved abstracts before ingestion selection, so `_select_records_for_ingestion` is seeded by actual question understanding, not token co-occurrence; (2) a sub-claim coverage ledger that makes round 2 and 3 queries surgically target whichever sub-claims of the ResearchPlan still lack evidence rather than firing generic '{question} limitations' queries; (3) an LLM-mediated claim-text extraction step that replaces the `\"{title} ({year}) reports: {first_sentence}\"` construction with an evidence-in-context extraction, so the synthesis prompt receives the mechanistically relevant finding from each paper rather than its opening remark.",
  "summary": "SPARKIT's retrieval-to-synthesis pipeline degrades on hard HLE biology/chemistry questions because every evidence quality signal is lexical: `_relevance_score` in aggregator.py uses token overlap + year bonus + a static 0.03 source-name bonus; `_record_relevance_score` in engine.py uses 2x title overlap + 1x abstract overlap + 0.25 recency; claim texts are built from first sentences; round 2/3 queries are templated strings like '{lead} benchmark comparison' and '{lead} contradictory findings' with no feedback from what was actually found in round 1. The result is that (a) semantically relevant papers with different vocabulary are systematically downranked, (b) retrieval rounds 2 and 3 repeat the same evidence space rather than filling genuine gaps, and (c) the synthesis prompt contains claim texts that cite opening abstracts rather than the specific mechanism or quantitative finding the question is testing. This proposal adds three targeted changes—LLM-batch abstract scoring, sub-claim coverage tracking with directed follow-on queries, and LLM mechanism-extraction for claim texts—that compose within the existing `orchestrate()` loop in engine.py without touching the database schema, provider interfaces, or calibration formula.",
  "reasoning": "Hard HLE questions (biology mechanism, chemistry reaction product, quantitative biology) fail SPARKIT for a characteristic reason: the correct answer depends on a finding buried in the methods or results section of a niche paper whose abstract uses domain jargon that does not lexically overlap with the question text. Three failure modes follow. First, that paper is ranked below papers whose titles superficially match the question but whose content is about general background, because `_relevance_score` awards 2x for title token overlap. Second, even if the paper is ingested, the `_select_best_section_chunk` function finds the best chunk by token overlap between question and section text, which may be the introduction (high question-term density) rather than the results section (low general terms, high domain-specific quantitative content). Third, the claim text is generated as the first sentence of that best chunk, which for results sections is typically a restatement of experimental conditions, not the finding. LLM-batch abstract scoring fixes the first failure by using semantic understanding to score relevance, replacing token overlap. Sub-claim coverage loops fix the second failure by ensuring that when one sub-claim (e.g., 'rate constant for step 2') has no evidence after round 1, round 2 explicitly queries for it, rather than querying for generic 'limitations'. Mechanism-aware claim extraction fixes the third failure by extracting the specific quantitative or mechanistic finding from the section in the context of the question. Together these produce a synthesis prompt that contains evidence that actually supports or refutes each answer option, which is the prerequisite for correct MCQ scoring and confident free-text synthesis.",
  "expected_impact": {
    "accuracy_delta_pct_points": 9,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add `score_abstracts_batch()` in `services/orchestrator/app/engine.py`: takes question, a list of (title, abstract) tuples capped at 20, and the planning_provider; issues one LLM call with a structured prompt asking for a relevance score 0-10 per record; parses the response into a dict[index, float]; falls back to 5.0 (neutral) on parse failure. Keep prompt under 1500 tokens by truncating abstracts to 300 chars. Cache results by (question_hash, abstract_hash) in a module-level dict for the life of the process to avoid re-scoring the same abstract across rounds.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Modify `_select_records_for_ingestion()` in `services/orchestrator/app/engine.py` (currently lines 135-168): accept an optional `llm_scores: dict[int, float]` parameter. When present, multiply the existing `_record_relevance_score` value by `(llm_scores.get(i, 5.0) / 5.0)` so a perfectly relevant abstract (10/10) doubles the lexical score and an irrelevant one (0/10) zeroes it. The two-pass diversity-first selection logic is unchanged. Call `score_abstracts_batch()` from `orchestrate()` immediately after each `search_literature()` call and before each `_select_records_for_ingestion()` call.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Add `_compute_sub_claim_coverage()` in `services/orchestrator/app/engine.py`: takes `sub_claims: list[str]` from the ResearchPlan and `claim_texts: list[str]` built so far; for each sub-claim, computes a token-overlap coverage score against the union of claim_texts; returns a dict[sub_claim_index, float] where 0.0 means no evidence. Sub-claims with coverage < 0.25 are considered uncovered. This is a fast lexical check (no LLM call needed), run after each round's ingestion finishes.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Modify `_heuristic_retrieval_plan()` (engine.py line 596) to accept an optional `uncovered_sub_claims: list[str]` parameter. When non-empty, add a new `'sub_claim_fill'` intent key to `intent_queries` whose values are `[f\"{sub_claim} mechanism\", f\"{sub_claim} experimental evidence\", f\"{sub_claim} quantitative result\"]` for each uncovered sub-claim (up to 3, deduplicated via `_dedupe_queries(max_items=8)`). In `orchestrate()`, call `_compute_sub_claim_coverage()` after round 1 ingestion and pass uncovered sub-claims into the retrieval plan for rounds 2 and 3, replacing the current generic `'{lead} limitations'` and `'{lead} benchmark comparison'` templates for those slots.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add `_extract_mechanism_claim()` in `services/orchestrator/app/engine.py`: takes question, section_name, section_text, title, year, and the synthesis_provider; issues a short LLM call (<300 token prompt) asking 'In one sentence, what is the specific finding in this {section_name} section from \"{title}\" ({year}) that directly addresses: {question}? Reply with exactly the finding, no preamble.'; uses the result as claim_text instead of the current `\"{title} ({year}) reports: {first_sentence_of_section}\"` construction. Gate on `SPARKIT_ENABLE_CLAIM_EXTRACTION=1` (default off initially) so it can be A/B tested without breaking existing runs. Fall back to the existing first-sentence format on LLM error or empty response.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add `citation_count: int | None = None` field to `LiteratureRecord` in `services/retrieval_service/app/models.py`. Update the Semantic Scholar adapter to populate it from the `citationCount` field already returned by the Semantic Scholar Graph API. Update `_relevance_score()` in `aggregator.py` to add `math.log1p(record.citation_count or 0) / math.log1p(5000)` (clamped [0, 0.5]) as an additive term alongside the existing year_bonus and source_bonus. This deprioritises uncited preprints and elevates highly-cited primary literature for the same query. Other adapters leave `citation_count=None`.",
      "owner": "retrieval",
      "effort": "low"
    },
    {
      "step": "Add `sub_claim_coverage_by_round: dict[str, float]` to the `retrieval_adaptive_gate` trace artifact in `orchestrate()`. After each round's `_compute_sub_claim_coverage()` call, record the per-sub-claim scores keyed by round name. This enables offline analysis of how quickly sub-claim coverage saturates across rounds and validates whether targeted fill queries are working.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Run A/B benchmark on HLE-25 balanced subset: execute `single_anthropic` config with `SPARKIT_ENABLE_CLAIM_EXTRACTION=0` (baseline) and `SPARKIT_ENABLE_CLAIM_EXTRACTION=1` (new), 3 seeds each, compare accuracy and cost. If accuracy improves >= 4 ppts at acceptable cost (<30% increase), enable by default and promote to HLE-149 full run.",
      "owner": "eval",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Replace the lexical `_record_relevance_score` ranking in `_select_records_for_ingestion()` (engine.py:135) with an LLM-batch abstract scoring step: one batched LLM call scores up to 20 retrieved abstracts on a 0-10 relevance scale against the question; the resulting per-record multiplier (score/5.0) is applied to the existing token-overlap score before sorting, so semantically relevant papers with low lexical overlap are no longer penalised relative to topically adjacent but uninformative background reviews.",
    "Add citation_count to LiteratureRecord and populate it from the Semantic Scholar Graph API's citationCount field; integrate as a log-scaled additive credibility term (log1p(count)/log1p(5000), capped at 0.5) in `_relevance_score()` in aggregator.py alongside the existing year_bonus and source_bonus, so highly-cited primary literature is systematically preferred over uncited or low-citation preprints when relevance scores are otherwise close.",
    "Implement sub-claim coverage tracking: after each retrieval round's ingestion, compute per-sub-claim evidence coverage scores against the accumulated claim_texts; sub-claims with coverage < 0.25 are labelled uncovered and drive a new 'sub_claim_fill' intent in the RetrievalPlan for subsequent rounds, replacing the current generic '{lead} limitations' and '{lead} benchmark comparison' gap-fill queries with targeted '{sub_claim} experimental evidence' and '{sub_claim} quantitative result' queries that address the specific factual gap.",
    "Gate Brave web search activation on sub-claim coverage: if any sub-claim remains uncovered after two academic-source rounds AND SPARKIT_ENABLE_WEB_SEARCH is set, automatically invoke search_brave_web with the uncovered sub-claim as query text; this reserves the $0.005/request web search cost for genuinely hard-to-evidence sub-claims rather than firing it for all questions.",
    "Replace the `_relax_query()` fallback (which removes quotes and truncates to 18 terms) with a two-stage fallback: first relaxation removes quotes/parentheses (existing), second relaxation applies domain synonym expansion from `_QUERY_REWRITES` in aggregator.py extended with 30 additional chemistry/biology synonym pairs (e.g., 'radical' -> 'free radical, reactive oxygen species', 'rate constant' -> 'kcat, turnover number, kinetic parameter') before retrying failed adapters, giving niche chemistry and biology queries a better chance of retrieving relevant papers from arXiv and Europe PMC."
  ],
  "evaluation_plan": [
    "Accuracy delta on HLE-149 benchmark: run full `single_anthropic` config with and without the three changes (LLM-batch scoring, sub-claim coverage loops, mechanism claim extraction), 2 seeds each; report per-domain accuracy (biology, chemistry separately), overall correct-answer rate, and per-question delta to identify which question types benefit most; require >= 4 ppt improvement overall at p < 0.10 (permutation test) to declare the change beneficial.",
    "Evidence relevance precision audit: for 25 randomly-sampled hard HLE questions (10 biology, 10 chemistry, 5 numerical), manually label whether each of the top-8 ingested claims in the run trace is actually relevant to the question on a binary scale; compute precision@8 for baseline (current) vs. new runs; the LLM-batch scoring change should raise precision@8 by at least 10 ppts for this to be considered a retrieval quality improvement.",
    "Sub-claim coverage rate measurement: using the new `sub_claim_coverage_by_round` trace artifacts, compute the fraction of ResearchPlan sub-claims with coverage >= 0.25 after round 1 vs. after all rounds for 50 hard HLE questions; targeted sub-claim fill should produce >= 15 ppt more sub-claims covered by end-of-round-2 compared to the current generic gap-fill queries.",
    "Calibration check (ECE/Brier): compute Expected Calibration Error and Brier score on HLE-149 predictions before and after the changes; mechanism-aware claim extraction should improve synthesis accuracy, which should tighten the confidence-accuracy alignment and reduce ECE; a regression in ECE > 0.03 absolute would indicate the LLM claim extraction is introducing overconfident but wrong answers.",
    "Cost and latency regression gate: measure median per-question cost and p90 latency on HLE-25 for the new config vs. baseline; LLM-batch abstract scoring adds approximately 1 LLM call per retrieval round (amortised across 20 records); mechanism claim extraction adds 1 LLM call per ingested document; total cost increase must remain below 40% and p90 latency increase below 25 seconds compared to baseline `single_anthropic` to be acceptable for default-on deployment."
  ],
  "risks": [
    "LLM-batch abstract scoring quality depends on the planning provider model: weaker models (e.g., smaller Kimi or Mistral variants) may produce inconsistent 0-10 scores or fail to parse correctly, triggering fallback to neutral (5.0) for all records and silently degrading to baseline behaviour; the parse-failure fallback must be tested with each supported provider.",
    "Sub-claim coverage tracking uses token overlap between sub-claim text and accumulated claim_texts: for chemistry questions where sub-claims use IUPAC systematic names but retrieved papers use common names or abbreviations, coverage will be falsely reported as low, triggering unnecessary additional retrieval rounds and increasing cost without benefit; adding a synonym-expansion step to sub-claim matching (using `_QUERY_REWRITES` or a small lookup table) would mitigate this.",
    "Mechanism-aware claim extraction (the `_extract_mechanism_claim()` LLM call) risks hallucination: if the section text does not contain a directly answerable finding, the LLM may fabricate a plausible-sounding but incorrect claim text; this is potentially more harmful than the existing first-sentence fallback because it enters the synthesis prompt as an authoritative citation; the gate flag `SPARKIT_ENABLE_CLAIM_EXTRACTION=0` (default) and the explicit fallback on empty/error response partially mitigate this, but human spot-checks on 50 claims are needed before enabling by default.",
    "Citation count availability is limited to Semantic Scholar results: approximately 30-50% of records retrieved by arXiv, Crossref, OpenAlex, and Europe PMC adapters will have citation_count=None, leaving the credibility signal inaccessible for those sources; the log-scaled term gracefully degrades to 0 for None, so no records are penalised, but the signal is only applied to the Semantic Scholar fraction of the corpus, reducing its overall discriminative power.",
    "Batched LLM calls for abstract scoring add a synchronous delay before each ingestion selection step: if the planning_provider has high latency (e.g., DeepSeek with 45s timeout), batching 20 abstracts may add 5-15 seconds per retrieval round; with 3 rounds, this could add 15-45 seconds of wall-clock latency per run; to mitigate, cap batch size at 12 records when provider latency is expected to be high and use a non-reasoning model for the scoring call (Haiku 4.5 or Gemini Flash) rather than the same model used for planning."
  ]
}
```
