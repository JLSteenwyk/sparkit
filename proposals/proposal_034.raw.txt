```json
{
  "proposal_id": "SPARKIT-AEGC-001",
  "title": "Adaptive Epistemic Gap Closure with Citation Genealogy and Quantitative Evidence Anchoring",
  "distinctive_angle": "Replace SPARKIT's fixed retrieval intent buckets with a dynamic epistemic-state model: after round 1, an LLM identifies specific knowledge gaps and generates precision-targeted queries for rounds 2+, while a parallel citation genealogy pass traverses Semantic Scholar reference graphs to fetch the primary experimental papers that review articles cite but that direct API search never surfaces. Numerical entities in the question become mandatory retrieval anchors and post-ingestion verification constraints—not just synthesis assumptions.",
  "summary": "Hard HLE questions fail not because the synthesis LLM is incapable but because retrieved evidence is (a) review-level rather than primary-experimental, (b) missing the precise numerical anchors needed for quantitative discrimination, and (c) targeted at generic intent buckets rather than the specific sub-questions left unresolved after round 1. This proposal introduces three orthogonal retrieval upgrades: (1) Epistemic Gap Querying—a lightweight LLM call after round 1 produces a structured knowledge-gap manifest (unknown facts, conflicting signals, missing quantitative data) and generates precision-targeted academic search queries to drive rounds 2+, replacing the hardcoded primary/methods/adversarial/reference intent queries; (2) Citation Genealogy Retrieval—for the top-5 highest-scoring round-1 records, query Semantic Scholar's /paper/{id}/references endpoint to fetch their cited primary sources and add those as ingestion candidates; (3) Quantitative Evidence Anchoring—extract all numerical entities (concentrations, wavelengths, temperatures, p-values, ratios) from the question using a unit-aware regex, inject them as mandatory focus_terms in retrieval queries, and add a post-ingestion check that flags claims where no retrieved document contains a matching quantitative value within a configurable tolerance, penalizing calibration confidence proportionally.",
  "reasoning": "The technical audit reveals three root causes for hard-question failures in the current codebase. First, _record_relevance_score (engine.py) and aggregator._relevance_score (retrieval_service/app/aggregator.py) use pure token overlap—IUPAC names, chemical formulas, and discipline-specific synonyms score 0 even when semantically present, causing systematic miss of the most precise primary papers. Second, citation graph traversal is completely absent: retrieval stops at the first-hop API surface, meaning the primary experimental paper that a retrieved review article cites in its results section is never fetched, even though it is the only document containing the actual measured value needed to answer the question. Third, the fixed round intent buckets (primary/methods/adversarial/reference defined in _decompose_retrieval) are query-agnostic—they do not adapt to what was actually found in round 1, so if the critical sub-question is not matched by any fixed template, it remains unanswered through all rounds. Epistemic gap modeling directly addresses cause 3 by making round 2+ queries semantically specific to what is still unknown. Citation genealogy directly addresses cause 2 by making Semantic Scholar's reference graph a first-class retrieval mechanism rather than an untapped signal. Quantitative anchoring addresses the precision gap (cause 1) by constraining both retrieval filters and post-ingestion evidence grounding to the specific numerical regime of the question. These three improvements are orthogonal—each activates in a different sub-system (query planning, retrieval adapters, evidence grounding)—meaning they can be ablated independently and their accuracy contributions measured separately on HLE-gold-35.",
  "expected_impact": {
    "accuracy_delta_pct_points": 7,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add unit-aware numerical entity extractor: implement _extract_quantitative_entities(question_text) -> list[tuple[float, str, str]] in engine.py using a regex covering SI prefixes (μ, m, n, k, M, G), chemical concentrations (mM, μM, nM, mol/L), wavelengths (nm, cm⁻¹, eV), temperatures (°C, K, °F), statistical values (p<, r=, R²=, n=, HR=), and plain numeric ranges with units. Return (value, unit, surrounding_context_token) tuples. Inject matched tokens into RetrievalPlan.focus_terms alongside existing lexical anchor extraction in _extract_lexical_anchors. Gate behind SPARKIT_ENABLE_QUANTITATIVE_ANCHORING (default True).",
      "owner": "engine.py:_extract_lexical_anchors → new _extract_quantitative_entities",
      "effort": "low"
    },
    {
      "step": "Build epistemic gap manifest: add _build_epistemic_gap_manifest(question, records_round1, focus_terms) in engine.py. After retrieval round 1 completes, collect unique title+abstract snippets (up to 2500 chars total) and send a single planning-provider LLM call with a structured prompt: 'Given this question and the evidence retrieved so far, output a JSON object with keys: unknown_facts (list of strings describing specific facts needed but not yet found), conflicts (list of strings describing contradictory signals), missing_quantitative (list of numeric values or ranges mentioned in the question that no retrieved document covers). Be concise and factual.' Parse with json.loads + fallback to empty manifest on invalid output. Store result as EpistemicGapManifest dataclass. Gate behind SPARKIT_ENABLE_EPISTEMIC_GAP (default True for research_max, False for single/routed).",
      "owner": "engine.py:execute_orchestration (after round 1 block)",
      "effort": "medium"
    },
    {
      "step": "Replace hardcoded round 2+ intent queries with gap-targeted queries: modify _decompose_retrieval to accept an optional EpistemicGapManifest. When present and non-empty, call the planning provider with: 'Write a precise academic search query (max 12 words) to find evidence for: {unknown_fact}' for each unknown_fact (up to 4). Append these gap queries to round 2 and round 3 alongside existing intent queries. Remove or deprioritize the generic 'methods' and 'reference' intent rounds when a gap manifest is available—only keep 'adversarial' as a mandatory round. This keeps total rounds constant while making their content semantically specific to the question.",
      "owner": "engine.py:_decompose_retrieval",
      "effort": "medium"
    },
    {
      "step": "Add Semantic Scholar citation genealogy adapter: in retrieval_service/app/adapters.py, implement _fetch_s2_references(paper_id: str, top_k: int = 20) -> list[dict] using GET https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=title,abstract,year,externalIds,openAccessPdf,authors&limit=50. Filter returned references to those with abstract length > 100 chars. Return as standard record dicts with source='citation_genealogy'. Reuse the existing _get_with_retry retry logic (max 2 retries, 0.6s backoff). Add fetch_citation_genealogy(records: list[dict], top_k: int = 5) to aggregator.py: select top-5 records by existing relevance score that contain a Semantic Scholar paperId (from externalIds or extracted from semanticscholar.org URL), call _fetch_s2_references for each, apply a secondary title-token-overlap filter (require >= 2 shared tokens with original question to avoid off-topic references), dedup against existing record set, and return new candidate records.",
      "owner": "retrieval_service/app/adapters.py, retrieval_service/app/aggregator.py",
      "effort": "medium"
    },
    {
      "step": "Integrate citation genealogy into orchestration: after round 1 retrieval and adaptive gating check in execute_orchestration (engine.py), call retrieval_service fetch_citation_genealogy on the current record pool. Append returned genealogy records to all_records with source tag 'citation_genealogy'. In _select_records_for_ingestion, exclude citation_genealogy records from the source-diversity cap (they should not count against any adapter's slot limit). Add citation_genealogy_records_added to the per-round metrics dict for observability. Gate behind SPARKIT_ENABLE_CITATION_GENEALOGY (default True).",
      "owner": "engine.py:execute_orchestration",
      "effort": "low"
    },
    {
      "step": "Primary literature scoring boost: add is_primary_literature(record: dict) -> bool in retrieval_service/app/aggregator.py. A record is flagged as primary if: source in {arxiv, crossref, europepmc, semantic_scholar} (not brave/web), AND abstract contains any of {'n=', 'measured', 'observed', 'experiment', 'assay', 'specimen', 'conducted', 'participants', 'subjects', 'samples'} OR title does NOT contain any of {'review', 'meta-analysis', 'overview', 'survey', 'commentary', 'perspective', 'editorial'}. Apply +0.30 additive bonus in engine.py _record_relevance_score for is_primary_literature hits. This counteracts the current bias toward high-citation review papers that dominate top-K slots but contain secondary evidence.",
      "owner": "retrieval_service/app/aggregator.py, engine.py:_record_relevance_score",
      "effort": "low"
    },
    {
      "step": "Implement quantitative evidence verification gate: add _verify_quantitative_coverage(quantitative_entities, ingested_records) -> dict in engine.py. For each extracted numerical entity, use a float-extraction regex against each ingested record's section_text and abstract. A record 'covers' an entity if it contains a float within ±20% of the entity's value (configurable via SPARKIT_QUANTITATIVE_TOLERANCE, default 0.20). Return {entity: covered: bool, covering_record_ids: list}. Compute unverified_quantitative_count = number of entities with covered=False. Add unverified_quantitative_count as a new calibration feature in calibrate_answer (calibration.py): penalize answer_confidence by min(0.12, 0.04 * unverified_quantitative_count). Also surface this count in the synthesis prompt header as a caveat notice when unverified_quantitative_count > 0.",
      "owner": "engine.py:new _verify_quantitative_coverage, calibration.py:calibrate_answer",
      "effort": "medium"
    },
    {
      "step": "Extend observability with AEGC metrics: add fields gap_manifest_unknown_facts_count, gap_manifest_conflicts_count, citation_genealogy_candidates_fetched, citation_genealogy_records_ingested, unverified_quantitative_count to StageMetrics in observability.py and persist via ObservabilityStore. Surface these fields in eval_service/app/metrics.py per-run summary so benchmark reports can track genealogy hit rate and gap manifest utility across the HLE-gold set.",
      "owner": "observability.py, observability_store.py, eval_service/app/metrics.py",
      "effort": "low"
    },
    {
      "step": "Write unit tests: add 12 new tests to orchestrator/tests/test_synthesis_quality.py covering: (1) _extract_quantitative_entities correctly extracts mM/nm/°C/p-value entities from 5 STEM question fixtures; (2) _build_epistemic_gap_manifest gracefully returns empty manifest on malformed LLM JSON output; (3) _verify_quantitative_coverage correctly identifies covered and uncovered entities within and outside ±20% tolerance; (4) is_primary_literature correctly classifies 6 abstract fixtures (3 primary, 3 review); (5) fetch_citation_genealogy deduplication logic drops records already present in the record pool by DOI or URL key; (6) _decompose_retrieval injects gap queries when EpistemicGapManifest.unknown_facts is non-empty.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    },
    {
      "step": "Ablation benchmark on HLE-gold-35: run four configurations—(A) current SPARKIT baseline, (B) AEGC with citation genealogy only, (C) AEGC with epistemic gap querying only, (D) full AEGC with all features—each in triplicate using make baseline-capture-hle-biochem-35. Report per-configuration accuracy, abstention rate, mean answer_confidence, calibration ECE, p50/p95 latency, and per-question cost. Use the per-question failure indexing (failures_<config>.json) to identify which question types benefit most from each feature and set final default env flags accordingly.",
      "owner": "eval_service, scripts_run_eval.py, Makefile",
      "effort": "high"
    }
  ],
  "retrieval_improvements": [
    "Citation genealogy retrieval via Semantic Scholar /references API: after round 1, fetch the reference lists of the top-5 scored records and add those cited primary sources as new ingestion candidates. This reaches the experimental papers that review articles cite in their results sections—papers that never appear in direct keyword searches because their titles use different terminology than the question—addressing the single most common failure mode on hard HLE questions.",
    "Epistemic gap query generation to replace fixed intent buckets: after round 1, use a planning-provider LLM call to identify specific facts still unknown or conflicting given what was retrieved, then generate precision-targeted academic queries (e.g., 'CRISPR-Cas9 off-target frequency at 37°C in HEK293 cells') for rounds 2+ rather than the current generic templates ('primary evidence', 'methods', 'reference') that are query-agnostic and frequently produce redundant results.",
    "Quantitative entity anchoring in retrieval filters: extract numerical values with units from the question text using a unit-aware regex and inject them as mandatory focus_terms. This ensures that, for questions like 'What is the Km of enzyme X for substrate Y?', retrieval queries include the expected numerical range and unit, ranking documents that discuss the specific quantitative regime above documents that discuss the topic generally at different scales.",
    "Primary literature scoring bonus in _record_relevance_score: apply a +0.30 additive boost for records identified as primary experimental literature based on abstract vocabulary (presence of assay/measurement/n= language) and source origin (arXiv, Crossref DOI, EuropePMC), counteracting the current bias toward high-citation review papers that accumulate top-K slots but contain only secondary evidence and pointers to the actual experimental data.",
    "Adaptive query specificity escalation for gap queries: configure gap-targeted queries to use the narrowest possible search scope first (exact phrase + quantitative anchor), falling back to progressively broader formulations only on zero-result retries—replacing the current uniform query relaxation that strips all specificity at once and may return the same broad documents retrieved in round 1."
  ],
  "evaluation_plan": [
    "Four-way ablation benchmark on HLE-gold-35: run configurations (A) current baseline, (B) citation genealogy only, (C) epistemic gap queries only, (D) full AEGC each in triplicate. Accept the proposal if full AEGC (D) achieves >= +4pp accuracy over baseline (A) on the 35-question set with 95% CI non-overlapping at the lower bound. Report per-question delta to identify which question subtypes (MCQ bio, MCQ chem, open-ended) benefit most.",
    "Citation genealogy coverage and precision check: for each run, record citation_genealogy_records_ingested from observability. Compute the fraction of HLE-gold-35 questions where at least one citation-genealogy record was ingested and its claim appeared in the final synthesis prompt. Target >= 40% of questions using a genealogy record. Separately, manually inspect 10 randomly sampled genealogy-sourced claims to verify they are primary experimental sources (not further review papers), requiring >= 7/10 to be genuine primary literature.",
    "Quantitative entity coverage verification: for the subset of HLE-gold-35 questions containing a numerical entity (expected ~60%), compute the fraction where at least one retrieved document contains a matching numeric value within ±20% tolerance—both before and after AEGC. Target >= +15 percentage points improvement in quantitative coverage versus baseline. Log all entities and their coverage status to a per-run JSON file for manual inspection.",
    "Epistemic gap manifest quality spot-check: for 10 manually-selected HLE-gold questions with known failure modes in the current system, inspect the EpistemicGapManifest JSON and verify that identified unknown_facts are substantively relevant to answering the question (not hallucinated, not trivial, not already answered by round-1 evidence). Accept if >= 7/10 manifests contain at least one actionable gap description that corresponds to a query generated for round 2+.",
    "Latency and cost regression gate: measure p50 and p95 end-to-end latency and per-question USD cost for full AEGC vs. baseline. Flag for mandatory review if p95 latency exceeds 2.5x baseline or per-question cost exceeds 2x baseline. Verify that the existing adaptive retrieval gating (SPARKIT_ADAPTIVE_RETRIEVAL) correctly prevents additional gap-query rounds on questions where round-1 already achieves quality_gain >= SPARKIT_ADAPTIVE_MIN_QUALITY_GAIN, confirming AEGC does not bypass budget safeguards."
  ],
  "risks": [
    "Semantic Scholar API rate limiting: the unauthenticated S2 API allows approximately 100 requests per minute. Citation genealogy adds up to 5 S2 calls per question. At benchmark throughput with parallel eval workers, this will trigger 429 errors. Mitigation: reuse the existing _get_with_retry adapter with exponential backoff (0.6s base), add a module-level semaphore in aggregator.py capping concurrent S2 genealogy requests at SPARKIT_CITATION_GENEALOGY_CONCURRENCY (default 2), and implement a shared in-process TTL cache (10-minute expiry) for paper reference results to avoid redundant fetches across concurrent runs.",
    "Epistemic gap LLM call cost at scale: a planning-provider call with 2500-token input after round 1 costs approximately $0.013/question at claude-opus-4-6 pricing ($5.00/M input) or $0.0015/question at Kimi K2 ($0.60/M). For research_max mode where the planning provider is claude-opus-4-6, this is acceptable ($0.45 incremental over a 35-question benchmark) but non-trivial at scale. Mitigation: gate SPARKIT_ENABLE_EPISTEMIC_GAP=True only for research_max mode by default; for routed mode, use the routing provider (typically a cheaper model) for gap manifest generation.",
    "Malformed JSON from gap manifest LLM call: structured JSON generation from LLM providers fails at a non-negligible rate (estimated 5-10% depending on model and prompt). A malformed manifest must be caught with try/except json.JSONDecodeError and gracefully fall back to the existing fixed intent queries rather than crashing orchestration. Add structured error logging (gap_manifest_parse_error=True in observability) so failure rates can be monitored across benchmark runs.",
    "Citation genealogy precision degradation: Semantic Scholar's /references endpoint returns all cited papers including introductory citations (textbooks, foundational papers) that are off-topic relative to the specific question. The secondary title-token-overlap filter (>= 2 shared tokens with original question) may be insufficient for questions using specialized terminology absent from reference titles. Mitigation: also apply the existing _record_relevance_score to genealogy candidates and only admit records scoring >= 60% of the mean relevance score of round-1 ingested records, preventing low-relevance genealogy records from consuming ingestion slots.",
    "Quantitative tolerance mismatch by domain: the ±20% numeric tolerance works well for population sizes and approximate measurements but is too coarse for precise chemical constants (pKa, equilibrium constants, crystallographic parameters require < 5% tolerance) and too strict for epidemiological estimates. Mitigation: add a unit-to-tolerance mapping table (SPARKIT_QUANTITATIVE_TOLERANCE_BY_UNIT) with sensible defaults per unit class, and document it in the configuration guide. Default to ±20% for unrecognized units rather than failing.",
    "Over-retrieval pool degradation: adding citation genealogy records and gap-targeted queries significantly increases the candidate pool passed to _select_records_for_ingestion. If genealogy records score lower than direct hits in _record_relevance_score (because their titles/abstracts use terminology different from the original question), they will not be ingested despite containing the key evidence—the exact failure mode this proposal aims to fix. Mitigation: add a genealogy_provenance_bonus (+0.20) in _record_relevance_score for records sourced from citation_genealogy, reflecting the signal that they were explicitly cited by a high-scoring retrieved paper as supporting evidence."
  ]
}
```
