```json
{
  "proposal_id": "SPARKIT-PROP-001-CLAIM-GROUNDED-ITERATIVE-EVIDENCE",
  "title": "Claim-Grounded Iterative Evidence Loop with Entity-Anchored Targeted Re-Retrieval",
  "distinctive_angle": "Invert the standard retrieve-then-synthesize paradigm by injecting a lightweight draft-synthesis pass after the initial evidence rounds, extracting structured factual claims with explicit entity anchors (chemical formulas, model names, experimental parameters, taxa names), and triggering a targeted per-claim re-retrieval pass using entity-mandatory queries. Evidence is then re-ranked by semantic proximity to each individual claim rather than global question relevance before final synthesis. This is architecturally distinct from SPARKIT's current fixed 3-round retrieval (primary/gap-fill/adversarial) which operates at question level, not claim level, and from the existing adaptive gating which measures novelty volume rather than factual grounding coverage.",
  "summary": "SPARKIT currently retrieves evidence against the raw question text, performs adaptive gating on document novelty, and synthesizes once. For very hard STEM questions (HLE-style), this leaves a critical gap: the evidence pool is question-diverse but often claim-sparse — it captures many tangentially relevant papers but may miss the specific paper containing the precise quantitative fact or experimental result the answer requires. This proposal adds a Claim-Grounded Iterative Evidence Loop (CGIEL): (1) after the standard retrieval rounds, call a cheap fast provider to generate a structured draft synthesis with explicit claim assertions and entity anchors; (2) extract testable claims using LLM structured output; (3) generate entity-anchored targeted queries for each low-confidence claim; (4) execute a targeted re-retrieval pass via both academic adapters and Brave; (5) re-score all evidence by per-claim semantic proximity; (6) feed claim-specific evidence blocks into the final synthesis prompt alongside the original evidence. The result is a synthesis grounded in claim-level evidence rather than question-level evidence, dramatically improving factual accuracy for hard STEM questions that require specific quantitative knowledge.",
  "reasoning": "HLE benchmark questions are designed to require precise, verifiable factual knowledge — exact numbers, specific experimental outcomes, named entity attributes. SPARKIT's current retrieval pipeline uses question-level query decomposition with three fixed intents (primary, gap-fill, adversarial) and scores retrieved documents by title/abstract overlap with the full question plus a recency bonus. This is effective for broad coverage but systematically underperforms on claims that require a specific measurement, threshold, or named entity value — because a paper containing the key fact may rank low on global question relevance if its title does not repeat the question's phrasing. The claim-grounded loop directly addresses this by making entity specificity a first-class retrieval criterion. Additionally, the current synthesis prompt receives all evidence indiscriminately weighted; by organizing evidence blocks per-claim in the synthesis prompt (engine.py:993-1035), the LLM synthesizer has cleaner signal about which papers speak to which specific assertion, reducing synthesis hallucination under long-context noise. The draft synthesis is cheap (one fast-provider call at ~$0.002) relative to the gain in evidence quality. SPARKIT already has all the plumbing needed: structured claim text generation, Brave search, multi-round retrieval, and trace stage observability — this proposal extends them with a new orchestration phase rather than replacing any existing component.",
  "expected_impact": {
    "accuracy_delta_pct_points": 7,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add `_extract_testable_claims(question, evidence_texts, provider_client)` in engine.py (~line 970, before `_build_synthesis_prompt`). Call a fast provider (DeepSeek or Gemini-flash) with a structured-output prompt requesting a JSON list of {claim_text, entities: list[str], confidence: float, query_hint: str}. Cap to top-8 claims by ascending confidence (lowest confidence = most in need of targeted retrieval). Gate the entire call behind a budget check using `should_stop_early` from policy.py.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add `_build_claim_targeted_queries(claims: list[dict]) -> list[str]` in engine.py. For each claim where confidence < 0.65 and entities is non-empty, construct a search query as `'{primary_entity} {query_hint} measurement|value|result'`. Deduplicate queries. Return at most 5 queries. These will be passed as an extra retrieval round to aggregator.py's `search_literature()` alongside a new `mandatory_entity` parameter.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Extend `search_literature()` in retrieval_service/aggregator.py to accept an optional `entity_filter: str | None` parameter. When set, post-filter retrieved records to require the entity string appear in title, abstract, or DOI metadata (case-insensitive substring match). Apply to all adapters (arXiv, Semantic Scholar, Crossref, OpenAlex, Brave). Log entity_filter hit rate as a new field in the stage metric artifact for observability.",
      "owner": "retrieval_service/aggregator.py",
      "effort": "medium"
    },
    {
      "step": "Add `_claim_evidence_reranker(claims, records) -> list[ScoredRecord]` in engine.py. For each retrieved record, compute a claim-proximity score: max over all claims of (entity_overlap(record, claim) * 2.0 + token_overlap(record.abstract, claim.claim_text) * 1.0). Blend with the existing `_record_relevance_score` (engine.py:121) at ratio 0.4 global + 0.6 claim-proximity. Re-sort the full evidence pool using this blended score before ingestion target selection.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Insert the CGIEL phase into `execute_orchestration()` (engine.py:~1454) immediately after the adaptive retrieval gate exits and before record selection begins (current line ~1456). Sequence: (a) call `_extract_testable_claims()`, (b) call `_build_claim_targeted_queries()`, (c) call `search_literature()` with entity_filter per query, (d) merge new records into the existing records pool with deduplication, (e) call `_claim_evidence_reranker()` on full merged pool. Emit a new `TraceStage(name='claim_targeted_retrieval')` with artifacts: claims_extracted, targeted_queries, new_records_added, entity_filter_hit_rate.",
      "owner": "orchestrator/engine.py",
      "effort": "high"
    },
    {
      "step": "Modify `_build_synthesis_prompt()` (engine.py:993) to add a new 'Claim-Specific Evidence' section after the general evidence block. For each claim extracted in step 1, include a sub-block listing only the records whose claim-proximity score for that claim exceeds 0.5. This gives the synthesis LLM explicit per-claim attribution rather than relying on it to infer which paper supports which assertion across a large flat evidence list.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add configuration knobs: `SPARKIT_CGIEL_ENABLED` (default: 1), `SPARKIT_CGIEL_MAX_CLAIMS` (default: 8), `SPARKIT_CGIEL_CLAIM_CONFIDENCE_THRESHOLD` (default: 0.65 — only re-retrieve for claims below this), `SPARKIT_CGIEL_FAST_PROVIDER` (default: 'deepseek' for cost efficiency, fallback to cheapest configured provider). Document in docs/configuration.md.",
      "owner": "docs/configuration.md + engine.py",
      "effort": "low"
    },
    {
      "step": "Add pricing entry for draft-synthesis cost: DeepSeek-chat at $0.00027/1K input + $0.0011/1K output (typical draft is ~2K in + 0.5K out = ~$0.0011 per run). Add to policy.py `DEFAULT_MODEL_PRICING` and ensure it is tracked in provider_usage list alongside synthesis costs. Update cost accounting tests in test_synthesis_quality.py.",
      "owner": "orchestrator/app/policy.py + tests",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Entity-anchored mandatory inclusion filtering: Extract named entities from draft synthesis claims (chemical formulas, protein/gene names, model architecture names, taxon names, experimental parameter labels) and pass them as mandatory substring filters to aggregator.py's `search_literature()`. A paper that does not mention the specific entity — e.g. 'GSK-3β' or 'ResNet-152' — cannot directly support a claim about it, so filtering before ranking eliminates the false-positive relevance scores that currently arise from papers discussing related but distinct entities. This directly addresses SPARKIT's current title/abstract overlap scoring (aggregator.py:78-84) which has no concept of entity identity.",
    "Per-claim semantic proximity reranking: Replace the single global `_record_relevance_score()` (engine.py:121) with a two-pass scoring system. Pass 1 computes the existing global score (question-level title/abstract overlap + recency + source bias). Pass 2 computes per-claim proximity for each record as max(entity_overlap × 2.0 + abstract_token_overlap × 1.0) over all extracted claims. The blended score (0.4 × global + 0.6 × claim-proximity) promotes papers that speak directly to at least one specific factual assertion in the draft, rather than papers that loosely mention the question topic. This is particularly impactful for hard STEM questions where the answer-bearing paper often uses precise technical terminology that doesn't appear in the question text.",
    "Brave web search activation for high-entity-specificity targeted queries: SPARKIT currently gates Brave search behind `SPARKIT_ENABLE_WEB_SEARCH=1` (default off), and when enabled, applies it uniformly to all queries. The CGIEL loop should always activate Brave for claim-targeted queries where entity specificity is high (at least one entity with length > 6 chars, not a common word). For hard STEM questions, recent preprint servers, lab websites, and conference proceedings are often the only sources with experimental measurements not yet indexed by arXiv or Semantic Scholar. Brave uniquely covers these. The per-query cost is $0.005 (already tracked in engine.py), making 5 targeted Brave queries add only $0.025 to run cost — negligible relative to the accuracy gain.",
    "Claim-targeted query construction with measurement-intent suffixes: Current `_build_round_queries()` (engine.py:194-199) constructs three fixed round types (primary, gap-fill, adversarial). The CGIEL adds a fourth query type — claim-targeted — constructed as '{entity} {query_hint} {measurement_intent}' where measurement_intent is drawn from a fixed vocabulary: 'value OR measurement OR experimental result OR reported OR quantified OR threshold'. This suffix pattern exploits the fact that academic paper abstracts use these terms when reporting specific numerical findings, dramatically improving recall of result-bearing papers vs. review papers on hard quantitative questions.",
    "Claim-level citation tracking for coverage gap detection: Currently `citation_coverage` in QualityGates tracks overall support_coverage = claim_evidence_links / total_claims (engine.py calibration). The CGIEL enables per-claim coverage tracking: for each extracted claim, record whether any record in the targeted retrieval pass returned an entity-filtered hit. Claims with zero targeted hits are flagged as 'evidence-dark' and appended to the synthesis prompt with an explicit 'No direct evidence found for this specific claim — treat with low confidence' annotation. This prevents the synthesis LLM from generating confident-sounding text about facts that have no retrieval support, directly improving calibration on hard questions."
  ],
  "evaluation_plan": [
    "A/B accuracy comparison on HLE-gold-149 (biology/chemistry subset): Run all 149 questions in two configurations — (A) current SPARKIT 3-round retrieval baseline and (B) SPARKIT with CGIEL enabled — using identical provider settings (e.g., single_anthropic with claude-opus-4-6). Score answers using the existing LLM judge with majority vote. Measure: (1) exact accuracy rate (correct/total), (2) mean answer_confidence for correct vs. incorrect answers, (3) mean citation_coverage QualityGate score. Expect configuration B to show ≥5 percentage point accuracy improvement with tighter confidence calibration (lower ECE).",
    "Entity recall measurement on HLE-gold-149: For each question, extract named entities from the ground-truth answer (using the same `_extract_testable_claims()` LLM call run against the gold answer text). Measure what fraction of ground-truth entities appear in the retrieved evidence pool for both configurations A and B. Entity recall = |retrieved_entities ∩ ground_truth_entities| / |ground_truth_entities|. CGIEL should increase entity recall by at least 15 percentage points relative to baseline — this validates that the targeted re-retrieval is actually finding claim-bearing documents, not just adding noise.",
    "Targeted retrieval hit rate monitoring: For each CGIEL claim-targeted query, log whether entity_filter returned at least one document (hit) or zero documents (miss). Report aggregate hit rate across HLE-gold-149 runs. If hit rate < 40%, the entity-anchored query construction is too restrictive and the confidence threshold for triggering re-retrieval (SPARKIT_CGIEL_CLAIM_CONFIDENCE_THRESHOLD) should be relaxed, or the entity_filter should fall back to partial match. This check prevents silent failure where targeted retrieval fires but consistently returns empty.",
    "Cost and latency regression gate: Measure per-run cost delta (CGIEL total cost - baseline cost) and per-run wall-clock latency delta across 20 HLE-gold-149 questions. Assert: (1) mean cost increase < $0.50 per run (to stay within typical max_cost_usd=3.0 budget for single-provider runs), (2) mean latency increase < 45 seconds (draft synthesis + targeted retrieval overhead), (3) zero budget-guard aborts triggered by CGIEL overhead alone (should_stop_early must not fire during CGIEL phase for any question where baseline would complete). Log provider_usage entries for the draft synthesis provider separately to track CGIEL cost attribution.",
    "Claim extraction quality spot-check: On a 20-question HLE-gold-149 sample, manually review the extracted claim lists from the draft synthesis phase. Verify: (1) each claim is a specific, testable assertion (not vague like 'this topic is complex'), (2) extracted entities are accurate (correct chemical formula, gene name, model name — not hallucinated), (3) the confidence scores assigned by the LLM correlate with actual factual correctness of the claim (claims that are wrong should have lower confidence). Adjust the draft synthesis prompt in `_extract_testable_claims()` if fewer than 70% of claims meet these quality criteria.",
    "Answerability gate regression check: Verify that the CGIEL loop does not increase false-abstain rate on HLE-gold-149. Since CGIEL adds more evidence, it should reduce (not increase) abstain triggers — especially `retrieved_evidence_too_sparse` and `citation_coverage_below_threshold` (engine.py:171-191). Measure abstain rate in both configurations A and B. If CGIEL increases abstain rate, investigate whether the entity_filter is over-filtering the evidence pool and reducing total ingested document count below the answerability thresholds."
  ],
  "risks": [
    "Draft synthesis claim hallucination amplification: If the fast provider used for draft synthesis generates confidently wrong claims (common on hard questions where it has limited knowledge), the targeted retrieval will chase fictional entities, wasting budget and potentially introducing false supporting evidence if a document coincidentally mentions the hallucinated entity. Mitigation: apply confidence thresholding (default SPARKIT_CGIEL_CLAIM_CONFIDENCE_THRESHOLD=0.65 — only re-retrieve for low-confidence claims, not high-confidence ones that are more likely to be hallucinated with false confidence); additionally, never elevate a claim-targeted result above a global-question result in final synthesis ranking unless entity overlap exceeds 0.8.",
    "Entity filter over-restriction causing evidence starvation: The mandatory entity inclusion filter may be too strict for entity variants — e.g., a paper discussing 'β-secretase' will not match an entity filter for 'BACE1' even though they are synonyms. This would cause the targeted retrieval to return zero results for valid claims, triggering false 'evidence-dark' annotations in the synthesis prompt. Mitigation: maintain a small entity synonym expansion step (using the same LLM call that extracts claims, request 1-2 synonyms per entity) and apply the filter as an OR across entity + synonyms. Fallback: if entity_filter returns < 2 results, retry without the filter and use claim-proximity reranking only.",
    "Cost overrun for ensemble or multi-provider configurations: Ensemble mode already calls 3+ providers for synthesis. Adding a CGIEL draft synthesis call + targeted retrieval round could push per-run cost above max_cost_usd=3.0 for high-effort ensemble configurations. Mitigation: (1) always use the cheapest configured provider for draft synthesis regardless of mode; (2) the CGIEL phase is gated by the budget guard `should_stop_early()` before executing — if remaining budget < $0.30 reserve, skip CGIEL entirely and proceed with standard synthesis; (3) add SPARKIT_CGIEL_ENABLED=0 as a per-run override for cost-sensitive configurations.",
    "Latency regression on time-sensitive configurations: CGIEL adds at minimum 2 sequential LLM calls (draft synthesis + claim extraction via structured output) plus an additional retrieval round. On slow providers or high-latency connections, this could add 20-60 seconds to wall-clock time. For configurations with max_latency_s set, this may cause budget guard early-stopping before reaching synthesis. Mitigation: run claim extraction in parallel with any remaining adaptive retrieval rounds (if SPARKIT_ADAPTIVE_MIN_ROUNDS not yet reached); cap draft synthesis tokens to 500 output tokens maximum (claim list only, no prose); use async execution for targeted retrieval queries in parallel via existing asyncio patterns in aggregator.py.",
    "Section-type bias in draft claims producing abstract-level rather than result-level claims: If the initial evidence pool contains mostly abstract text (common in early retrieval rounds before full document fetch), the draft synthesis may generate only high-level conceptual claims ('this compound inhibits X pathway') rather than quantitative claims ('IC50 = 3.2 nM at 37°C'). The targeted retrieval would then optimize for conceptual coverage rather than quantitative fact retrieval — no improvement over baseline for HLE questions that require specific numbers. Mitigation: modify the draft synthesis prompt to explicitly request extraction of 'quantitative assertions, specific measurements, named thresholds, and experimental numerical results only — exclude qualitative statements'. Add a validation step that rejects claims lacking at least one numeric token or named entity, and triggers a prompt retry with stronger instruction if fewer than 3 quantitative claims are produced."
  ]
}
```
