```json
{
  "proposal_id": "SPARKIT-P-2026-HyDES-001",
  "title": "Hypothesis-Driven Evidence Spiraling: Claim-Conditioned Verification Retrieval with Cross-Encoder Semantic Re-ranking",
  "distinctive_angle": "SPARKIT's adversarial retrieval round issues contradiction queries against the original question, never against the specific factual claims in the draft answer. HyDES closes this loop by extracting structured falsifiable claims from the synthesized draft and issuing claim-targeted retrieval queries, creating a verification spiral rather than a one-shot linear pipeline. Combined with a cross-encoder semantic re-ranker replacing the lexical token-overlap scorer in engine.py, this attacks SPARKIT's two highest-frequency hard-question failure modes simultaneously.",
  "summary": "HyDES extends the SPARKIT pipeline with three interlocking mechanisms: (1) a post-synthesis claim extraction step using a structured LLM prompt to extract N ≤ 6 falsifiable factual claims from the draft answer with key_terms and claim_type metadata; (2) a claim-conditioned verification retrieval round that issues 2 Brave plus 2 Semantic Scholar queries per claim using both literal claim phrasing and negation-prefix adversarial variants, selects passages via a cross-encoder semantic similarity scorer instead of the current _score_record_relevance lexical token-overlap function, and assembles a per-claim support/refutation dossier requiring evidence from at least 2 independent institutions; and (3) a verification-aware re-synthesis step that re-scores MCQ options or annotates open-answer claims with VERIFIED, UNVERIFIED, or CONTESTED flags and triggers automatic RESEARCH_MAX escalation when more than one critical claim cannot be grounded. The cross-encoder re-ranker is deployed system-wide, providing accuracy gains even in non-HyDES code paths.",
  "reasoning": "Hard QA failures in SPARKIT follow a recurrent pattern: initial retrieval surfaces loosely related documents, synthesis produces a plausible but subtly wrong claim, and the adversarial round misses it because adversarial queries target the original question rather than the wrong claim. Claim-conditioned verification directly targets this root cause. The semantic re-ranker addresses SPARKIT's second major weakness: the lexical token-overlap scorer (engine.py ~line 670) systematically under-ranks passages using synonymous STEM terminology. A cross-encoder jointly encodes question and passage, capturing semantic entailment rather than surface-form overlap, which is critical for chemistry and biology questions where the correct passage may share zero tokens with the query but be semantically entailed. The source independence check prevents single-lab consensus from being mistaken for broad scientific agreement, a known failure mode in specialized STEM benchmarks. Together, these three mechanisms form a closed feedback loop between synthesis and retrieval that no current SPARKIT stage implements.",
  "expected_impact": {
    "accuracy_delta_pct_points": 11,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add claim extraction prompt and structured output parser as a new TraceStage after synthesis in engine.py. Use Anthropic tool_use or OpenAI JSON mode to enforce schema: {claims: [{claim_text: str, key_terms: str[], claim_type: 'factual'|'mechanism'|'numerical'|'comparative'}]}. Validate that key_terms.length >= 2 and claim_text.length >= 20 before proceeding; fall back to current adversarial round if extraction yields 0 valid claims.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Build claim-to-query converter: for each extracted claim, generate 2 Brave queries (literal claim phrasing; negation-prefix variant 'evidence against [claim_text]') and 2 Semantic Scholar queries (key_terms joined with AND; claim_type-specific fieldsOfStudy filter: Biology+Chemistry for mechanism, Mathematics+Physics for numerical, general for factual). Register as intent_query group 'claim_verification' in RetrievalPlan.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Integrate cross-encoder passage re-ranker to replace _score_record_relevance in engine.py. Use ms-marco-MiniLM-L-6-v2 via sentence-transformers (22 ms per pair on CPU) or Cohere Rerank API as drop-in. Score (question_text + claim_text) concatenated with each candidate passage. Batch encode all pairs before any top-k selection. Gate cross-encoder use on SPARKIT_CROSS_ENCODER_ENABLED env var defaulting to 1.",
      "owner": "orchestrator",
      "effort": "high"
    },
    {
      "step": "Add source independence check in retrieval_service: parse first-author institution from Semantic Scholar affiliation metadata. Deduplicate passages by institution before building claim dossiers. Require at least 2 distinct institutions for a claim to be marked VERIFIED. Use doi-hash as fallback institution token when affiliation is missing.",
      "owner": "retrieval_service",
      "effort": "medium"
    },
    {
      "step": "Build per-claim support/refutation dossier from verification retrieval: top-2 supporting passages (from literal claim query) plus top-2 refuting passages (from negation-prefix query) per claim. Store as claim_dossier dict keyed by claim index. Attach to synthesis context as a new evidence block alongside existing claim_clusters and section_summaries.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add verification-aware re-synthesis prompt variant. For MCQ: re-score each option using claim_dossier as additional evidence, surface options whose claims are all VERIFIED as highest-confidence selections. For open-ended: annotate each claim sentence with [VERIFIED|UNVERIFIED|CONTESTED] inline flag. Return structured JSON with per_claim_confidence array.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add RESEARCH_MAX escalation trigger: if contested_claims > 1 and budget_remaining > reserve_next_stage_usd * 3 and current mode is not already RESEARCH_MAX, set escalation flag and re-invoke execute_orchestration with mode=RESEARCH_MAX, passing existing claim_dossier as warm-start context. Cap escalations via HYDDES_MAX_ESCALATIONS env var (default 1 per run).",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Update CalibrationFeatures dataclass and calibration formula in engine.py to include claim_verification_rate (verified_claims / total_claims, default 0.5 when extraction not run). Add +0.20 * claim_verification_rate coefficient. Update run_calibration_features DB table via new Alembic migration.",
      "owner": "orchestrator",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Claim-conditioned verification retrieval: after draft synthesis, extract N ≤ 6 falsifiable claims and issue 2 Brave plus 2 Semantic Scholar queries per claim. Literal claim phrasing targets confirming evidence; negation-prefix variants ('evidence against [claim]', '[claim] shown to be false') surface refuting papers. This directly patches the root cause of hard-question failures where the current adversarial round queries the original question rather than the specific propositions in the draft answer, meaning wrong intermediate claims never get challenged.",
    "Cross-encoder semantic passage re-ranking: replace the lexical token-overlap scorer in _score_record_relevance (engine.py ~line 670) with a cross-encoder that jointly encodes (question_text + claim_text) paired with each candidate passage. This fixes systematic under-ranking of passages using synonymous STEM terminology. For example, a passage about 'half-power bandwidth' is semantically entailed by a query about 'FWHM' but scores near-zero under token overlap. Cross-encoders capture this entailment because they attend across both sequences jointly rather than comparing independent embeddings.",
    "Source independence deduplication before claim dossier assembly: parse first-author institution from Semantic Scholar affiliation metadata and require at least 2 distinct institutions before marking a claim VERIFIED. This prevents a single prolific research group's output from appearing as broad scientific consensus, a failure mode particularly common in specialized chemistry and biology benchmarks where one or two labs dominate a subfield's literature.",
    "Claim-type-specific field filters for Semantic Scholar queries: use the extracted claim_type field to apply API fieldsOfStudy filters at retrieval time. Mechanism claims route to Biology+Chemistry with publicationTypes=JournalArticle to prefer peer-reviewed sources over preprints. Numerical claims route to Mathematics+Physics+Engineering. Factual claims use the existing broad filter. This significantly improves retrieval precision for the claim type rather than relying on the re-ranker to do all the filtering post-hoc.",
    "Negation-prefix Brave queries as adversarial claim verifiers: current adversarial retrieval round in engine.py queries for general contradiction keywords against the original question. HyDES replaces this with targeted negation-prefix queries per extracted claim, which surface papers that directly challenge the specific proposition rather than papers that happen to mention contradiction-related words near the question topic. This makes adversarial retrieval semantically grounded to the draft answer rather than syntactically grounded to the question."
  ],
  "evaluation_plan": [
    "HLE-gold-bio-chem triplicate re-run with HyDES enabled (SPARKIT_HYDDES=1, SPARKIT_CROSS_ENCODER_ENABLED=1) versus current ROUTED mode baseline. Primary metric: exact-match accuracy on the HLE-gold-bio-chem subset. Threshold for adoption: >= 8 percentage-point improvement on questions where claim extraction succeeds (claim_count >= 2). Log claim_count, verified_claims, contested_claims, and escalation_triggered per question to the run_calibration_features table for post-hoc analysis.",
    "Claim verification rate correlation analysis: after 50+ HyDES runs on STEM-Exam-200, compute Pearson correlation between claim_verification_rate (verified_claims / total_claims) and binary answer correctness. Threshold for adoption: r >= 0.35 with p < 0.05. If correlation is weak, the claim extraction quality or verification retrieval precision is insufficient and claim extraction prompts need refinement.",
    "Semantic re-ranker passage precision ablation: run STEM-Exam-200 with cross-encoder enabled versus disabled (reverting to lexical scorer). Measure passage precision@5 by checking whether the top-5 re-ranked passages contain the gold answer span as defined in the benchmark manifest. Threshold: >= 15 percentage-point improvement in passage precision@5. Also measure recall@10 to verify the re-ranker is not dropping relevant passages.",
    "Calibration improvement via claim_verification_rate: compare ECE (Expected Calibration Error) and Brier scores on HLE-gold before and after adding the +0.20 * claim_verification_rate coefficient to the calibration formula. Threshold: Brier score improvement >= 0.02 and ECE reduction >= 0.03. Use reliability diagrams binned at 0.1 confidence intervals to verify calibration curve shape improves rather than just overall score.",
    "RESEARCH_MAX escalation rate audit: measure escalation_triggered rate across HLE-gold questions. Target range: 15-30% escalation rate (too low means CONTESTED threshold is too permissive, catching no errors; too high means cost is unbounded). If outside range, tune HYDDES_CONTESTED_THRESHOLD env var. Also verify that escalated questions show higher accuracy than non-escalated questions to confirm escalation is identifying genuinely hard cases.",
    "End-to-end latency and cost profiling: instrument claim extraction, cross-encoder re-ranking, and verification retrieval stages individually using the existing TraceStage timing infrastructure. Target: total HyDES overhead < 40% latency increase over ROUTED baseline for 90th percentile of questions. Verify that per-question cost increase stays below 2x ROUTED baseline for 90% of non-escalated questions, using exact Brave per-request accounting already in place."
  ],
  "risks": [
    "Claim extraction quality dependency: if the LLM produces vague claims such as 'the answer involves proteins', the resulting claim-conditioned queries retrieve irrelevant documents and add noise. Mitigation: enforce a minimum specificity filter (key_terms.length >= 2, claim_text.length >= 20 characters, no question-restating claims) with structured output via Anthropic tool_use or OpenAI JSON mode. Fall back to existing adversarial round when extraction yields 0 valid claims.",
    "Cross-encoder latency overhead: scoring N claims x M candidate passages requires O(N x M) inference calls. For 6 claims and 20 passages each, that is 120 cross-encoder calls adding roughly 2-3 seconds on CPU. Mitigation: batch all pairs in a single sentence-transformers call, use the lightweight MiniLM-L-6-v2 model (22 ms per pair), and gate cross-encoder use on questions above a difficulty heuristic (estimated by the planning stage question_type field), skipping it for simple factual lookups.",
    "RESEARCH_MAX escalation cost spiral: if the CONTESTED threshold fires on most hard questions, costs scale nonlinearly because RESEARCH_MAX runs cost 3-5x ROUTED. Mitigation: add HYDDES_MAX_ESCALATIONS env var (default 1 per session) and require budget_remaining > reserve_next_stage_usd * 3 before escalating. Log escalation decisions to TraceStage for post-hoc tuning.",
    "Source independence check brittleness: Semantic Scholar institution metadata is incomplete for preprints and pre-2010 literature, causing the independence check to incorrectly block claim verification for well-supported but poorly-annotated papers. Mitigation: use doi-hash as a fallback institution token (each missing-affiliation paper is treated as a distinct institution) rather than blocking verification entirely. Also accept verification from a single institution if that institution has 3 or more independent papers supporting the claim.",
    "Negation-prefix query false positives: Brave queries prefixed with 'evidence against' may surface opinion pieces, blog posts, and social media content rather than peer-reviewed contradictions. Mitigation: apply the existing disallowed_domains filter and add a recency cap within claim verification queries (exclude documents older than 15 years for mechanism claims where established consensus is unlikely to be overturned). Prioritize results with doi fields.",
    "Prompt format brittleness for claim extraction: structured JSON output may silently fail when the LLM adds prose preamble or uses different field names. Mitigation: use Anthropic tool_use input_schema or OpenAI response_format JSON schema enforcement rather than relying on freeform JSON generation. Add a regex fallback parser that extracts bracketed claim arrays as a last resort before the stage is skipped."
  ]
}
```
