```json
{
  "proposal_id": "SPARKIT-P010",
  "title": "Claim-Frontier Reactive Retrieval with Adversarial Option Debate",
  "distinctive_angle": "SPARKIT currently generates all retrieval queries from the original question text at plan time (_decompose_retrieval / _heuristic_retrieval_plan). For very hard questions the bottleneck is unknown unknowns: crucial facts are buried behind reference chains that only become visible after reading the first batch of documents. This proposal inverts the retrieval loop — retrieved content itself generates the next round's queries by extracting 'frontier claims' (unresolved references, cited-but-not-retrieved papers, stated dependencies that lack supporting evidence) and treating them as first-class retrieval targets. Separately, for MCQ, it replaces score-blending (_select_confident_blended_option) with a structured adversarial debate where each option gets a dedicated LLM advocate and a shared challenger, producing a debate transcript that a judge synthesizes — a qualitatively different signal from lexical overlap scores.",
  "summary": "Two coordinated changes to engine.py: (1) After every ingestion pass inside execute_orchestration(), call a new _extract_frontier_claims() step that mines retrieved passage text for unverified dependencies, unresolved citations like 'Smith et al. 2019', and low-support factual assertions. These frontier claims are injected as query seeds into the next retrieval round via an extended _build_round_queries_from_plan() that accepts a new 'frontier' intent type, overlaying the existing plan structure without replacing it. The adaptive gating already controls round count; frontier claims make each additional round more targeted. (2) A new _adversarial_option_debate() function builds per-option advocate prompts using the existing _build_option_dossiers() evidence packs, runs each advocate through generate_text(), feeds all advocate outputs plus shared counter-evidence into a challenger prompt, then passes the full transcript to a judge. Gated by SPARKIT_ENABLE_OPTION_DEBATE env flag so it activates only when discrimination is low (existing _option_score_discrimination_guard threshold).",
  "reasoning": "HLE-style hard questions fail for two compounding reasons. First, the answer often requires a fact that is never the main topic of any retrieved document — it appears as a supporting citation or assumed background in a paper about something adjacent. Pre-planned queries cannot anticipate this because the gap is revealed only after reading. Evidence-reactive retrieval closes this gap by treating every unresolved claim as a first-class retrieval signal. Second, for hard MCQ the options are often close enough that score-blending over support/contradiction counts produces near-tie scores (_select_confident_blended_option falls back to the judge anyway). An adversarial debate forces the LLM to commit to a chain of reasoning for each option and expose weaknesses under challenge, which is a stronger discrimination signal than numeric blending. The two changes are complementary: better retrieval surfaces the evidence, and the debate turns nuanced evidence into a confident selection.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _extract_frontier_claims(passages: list[str], anchor_tokens: set[str]) -> list[str] to engine.py. Uses regex patterns to extract: (a) author-year citations like 'Smith et al. 2019', (b) explicit dependency phrases like 'as shown by X', 'following the method of Y', 'based on Z', (c) factual assertions containing anchor_tokens that appear in fewer than 2 already-ingested documents (low support density). Returns deduplicated list of natural-language retrieval seeds capped at 6 per round.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Extend RetrievalPlan dataclass to include frontier_seeds: list[str] field (default []). Extend _build_round_queries_from_plan() to emit one additional query per frontier seed with intent='frontier', injected after the plan's standard queries so they do not displace primary intent queries.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Modify the main retrieval loop inside execute_orchestration() to call _extract_frontier_claims() after each _ingest_documents() call and store results in a running frontier_seeds accumulator. Pass the accumulator into _build_round_queries_from_plan() for the next round. Gate behind SPARKIT_ENABLE_FRONTIER_RETRIEVAL env flag (default 1).",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add _adversarial_option_debate(option_dossiers: dict, provider: str, anchor_tokens: set) -> str to engine.py. For each option letter: build an advocate prompt from _build_option_dossiers() support evidence asking the LLM to make the strongest case for that option. Run all advocate outputs through a single challenger prompt that is given counter-evidence and asked to identify the weakest argument. Pass advocate transcripts + challenge responses to a judge prompt that returns a single answer letter with XML tag.",
      "owner": "orchestrator",
      "effort": "high"
    },
    {
      "step": "Modify _select_option_from_dossiers() to call _adversarial_option_debate() when: (a) SPARKIT_ENABLE_OPTION_DEBATE=1 (default 0 initially for cost control), and (b) the existing discrimination guard in _option_score_discrimination_guard() returns False (scores too close). This ensures debate only fires when cheaper methods fail, minimising added cost.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Instrument both features in the existing TraceStage artifact system: emit frontier_seeds_round_N artifact listing seeds and their retrieval results; emit debate_transcript artifact with full advocate/challenger/judge exchange. These surface in the run observability store for post-hoc analysis.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Add unit tests: test_frontier_claim_extraction_finds_citations(), test_frontier_claim_extraction_finds_low_support_assertions(), test_frontier_seeds_injected_into_retrieval_plan(), test_adversarial_debate_prompt_structure(), test_debate_fires_only_below_discrimination_threshold(). Extend test_synthesis_quality.py.",
      "owner": "orchestrator/tests",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Frontier-claim query injection: After each ingestion pass, mine retrieved passage text for author-year citations, dependency phrases ('as shown by', 'following the method of', 'based on the work of'), and low-support factual assertions (appearing in fewer than 2 ingested documents). Convert these into natural-language retrieval queries with intent='frontier' and inject them into the next round via _build_round_queries_from_plan(). This closes the reference-chain gap that pre-planned queries cannot anticipate — the evidence itself tells us what is missing.",
    "Citation-chasing via structured reference extraction: Extend _extract_frontier_claims() to parse structured citation patterns (author et al. YEAR, DOI patterns, arXiv IDs) directly from retrieved text. Use these as targeted Brave Search queries with the format '<first-author> <year> <journal-hint>' rather than free-text queries. This exploits Brave's document-level indexing to retrieve the exact cited paper rather than a document that happens to mention the same topic.",
    "Semantic anchor drift detection: Compute a reference token-set from the original question's anchor_tokens (already extracted in _tokenize()). After each retrieval round, compute what fraction of newly retrieved document titles/abstracts contain at least one anchor token. If this fraction drops below 0.35 for a round, inject a re-anchoring query that concatenates the top-3 anchor tokens with the original question's first sentence as a Brave query. This prevents the retrieval plan's later rounds (adversarial, gap-fill) from drifting into tangential literature.",
    "Source-diversity-aware frontier prioritization: When multiple frontier claims compete for the same retrieval slot, prefer claims that reference source types not yet represented in the ingested document set (e.g., if all ingested docs are journal articles, prioritize claims referencing conference papers, preprints, or datasets). The existing _select_records_for_ingestion() source-diversity logic already tracks this; expose the source-type distribution to _extract_frontier_claims() so it can score seeds by type novelty.",
    "Temporal recency weighting for time-sensitive frontier claims: When a frontier claim contains a year token >= 2022 or phrases like 'recently shown', 'new results', 'updated guidelines', apply a +0.2 relevance bonus in _record_relevance_score() to records published within 3 years of the claim's year token. This surface-level heuristic is already within the scoring function's architecture and requires only a small conditional."
  ],
  "evaluation_plan": [
    "Frontier resolution rate: After each benchmark run on HLE-gold or stem_exam_200, compute (frontier_claims_that_yielded_at_least_1_relevant_new_doc) / (total_frontier_claims_generated). Target >= 0.55 to confirm frontier queries are finding genuinely new evidence rather than duplicating primary round results. This can be computed from the frontier_seeds_round_N trace artifacts without extra LLM calls.",
    "Option reversal rate on low-discrimination MCQ: From runs where _option_score_discrimination_guard() fired (score gap below threshold), measure what fraction of final answers changed when debate was enabled vs. disabled. A healthy reversal rate of 15-35% confirms the debate is adding signal; below 10% suggests the debate is noise; above 50% suggests the base scoring is too unreliable.",
    "Ablation accuracy comparison on HLE-gold subset: Run three configurations on the same 25-question HLE-gold subset: (a) baseline (current code), (b) +frontier retrieval only, (c) +frontier retrieval + option debate. Report accuracy at each level. This isolates the contribution of each component and detects negative interactions. Use the existing eval_service benchmark harness and direct-call retry logic.",
    "Frontier claim type breakdown: Log which extraction pattern each frontier claim came from (citation, dependency phrase, low-support assertion). After 50+ benchmark questions, compute accuracy improvement segmented by which frontier claim types were triggered. If citation-chasing outperforms low-support assertions, tune extraction weights accordingly.",
    "Debate judge consistency check: On 20 MCQ questions where ground truth is known, run _adversarial_option_debate() three times with temperature > 0 and measure judge answer consistency (target >= 0.75 agreement). Low consistency indicates the debate transcript is not deterministic enough to trust; trigger a fallback to _select_option_from_dossiers() non-debate path.",
    "Cost and latency regression guard: Using the existing per-run cost tracking (SPARKIT_MODEL_PRICING_JSON + ProviderUsage), assert that frontier retrieval adds at most 1.5x Brave Search calls per question vs. baseline, and that option debate adds at most 2.0x synthesis tokens for MCQ questions. Fail the benchmark CI step if either bound is exceeded, preventing runaway cost regressions."
  ],
  "risks": [
    "Frontier claim extraction quality degrades on dense technical text: regex-based citation and dependency extraction may misparse equations, chemical formulas, or non-English author names common in HLE questions, producing garbage retrieval seeds. Mitigation: run extraction through a brief LLM triage step (a single 50-token prompt asking 'is this a real external reference?') before submitting as a query, gated by SPARKIT_FRONTIER_LLM_TRIAGE=0 by default to keep costs low.",
    "Frontier retrieval rounds violate budget guardrails: Adding frontier-driven extra queries increases Brave Search call count and ingestion cost per question. For questions already near the budget ceiling (checked in execute_orchestration() before each stage), frontier seeds may push over the limit and trigger early termination, leaving the synthesis stage with less context than baseline. Mitigation: frontier seeds should only be injected if remaining_budget > 2x typical_retrieval_round_cost, which can be checked using the existing _effort_profile() budget estimates.",
    "Adversarial debate amplifies confident wrong advocates: If the retrieved evidence strongly but incorrectly supports a wrong option (confirmation bias in retrieval), the advocate for that option will produce a persuasive argument and the judge may be swayed even in the face of challenger pushback. This risk is higher for questions where retrieval itself is the limiting factor. Mitigation: ensure the challenger prompt explicitly receives the full dossier for all options (not just the option being challenged) so it can introduce cross-option contradictions.",
    "Debate latency makes real-time use impractical: Running N advocate prompts plus a challenger plus a judge adds at minimum 4 sequential LLM calls for a 4-option MCQ. At typical provider latencies (8-15s per call), this adds 30-60s. For benchmark evaluation this is acceptable, but for interactive use it may not be. Mitigation: gate debate behind SPARKIT_ENABLE_OPTION_DEBATE=0 by default; expose it as a research_max mode feature only.",
    "Frontier query diversity may hurt precision: Injecting author-year citation queries (e.g. 'Smith 2019 kinase inhibitor') into Brave Search may return low-quality results (news articles, blog posts) if the paper is not well-indexed. This dilutes the ingestion budget with irrelevant documents and could lower accuracy by reducing the proportion of high-quality evidence. Mitigation: apply a minimum relevance score floor (e.g. _record_relevance_score() >= 0.15) specifically for frontier-intent records, stricter than the existing general floor, so low-quality frontier retrievals are dropped before ingestion.",
    "Test coverage gap for debate path: The existing test_synthesis_quality.py tests all exit before the debate path since it requires live LLM calls. Unit tests for debate prompt structure and parsing are achievable but end-to-end accuracy validation requires the benchmark harness. This means the debate feature may pass unit tests but fail silently in integration. Mitigation: add a mock-provider path in test_adversarial_debate_prompt_structure() that returns a fixed JSON response so the full debate→judge→answer-extraction pipeline can be exercised without live API keys."
  ]
}
```
