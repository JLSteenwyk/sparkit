```json
{
  "proposal_id": "SPARKIT-P001-CLAIM-ANCHORED-MULTIHOP",
  "title": "Claim-Anchored Multi-Hop Retrieval via LLM-Extracted Evidence Bridges",
  "distinctive_angle": "SPARKIT currently plans all retrieval queries before any documents are seen, then runs fixed-intent rounds (primary, options, methods, adversarial, reference) whose query seeds are derived entirely from the original question text. This proposal inserts a dynamic bridging round AFTER Round 1 retrieval: the planning LLM reads the top-5 retrieved abstracts, identifies specific named entities and factual gaps that block answering the question, and emits targeted bridging queries that slot into the existing round-based framework as retrieval_round_bridge. This is not query expansion (which only rephrases the original question) and not adversarial retrieval (which seeks contradictions). It asks: 'Given what we have retrieved so far, what specific missing sub-fact would change the answer?' — a question that cannot be asked without first reading retrieved evidence.",
  "summary": "After Round 1 retrieval and scoring, pass the top-5 ranked abstracts plus the original question to the planning LLM to extract: (1) key domain entities mentioned in evidence (compound names, reaction mechanisms, numerical thresholds, experimental method names), (2) specific sub-facts still missing that would clinch or refute each MCQ option or sub-claim. Generate up to 4 bridging queries from these outputs using the existing _extract_lexical_anchors() pipeline to preserve verbatim chemistry terms. Execute retrieval_round_bridge as the third round in standard mode (between gap_fill and adversarial) and fourth round in research_max mode. Boost bridge-round documents in ingestion scoring (+0.4) when they share entity overlap with Round 1 top results. Tag chunks as bridge_linked (+0.5 score bonus) when they contain both a bridge entity and a Round 1 claim token. Update synthesis prompt to present bridge-linked claims in a dedicated chain-linked evidence section, instructing the LLM to prioritize multi-document chains when forming its answer.",
  "reasoning": "Hard science questions on HLE-style benchmarks disproportionately require connecting two or three independently stated facts across different papers — e.g., 'Paper A establishes compound X reacts at pH Y; paper B establishes pH Y is only achievable by method Z; therefore the answer is Z.' SPARKIT's current retrieval architecture emits queries derived only from the original question text, so it systematically cannot retrieve intermediate-step documents because those documents do not use the question's vocabulary. The gap-fill round (methods + reference queries) is close in spirit but is planned without reading any retrieved evidence, so it misses the specific chain links that actual retrieved content reveals are needed. The adversarial round looks for contradictions rather than missing links. By reading Round 1 abstracts before planning bridge queries, the LLM can target precisely the sub-facts that would change the answer — which is exactly the information not captured by question-text-only query planning. The implementation slots cleanly into the existing multi-round framework: retrieval_round_bridge is registered as a new round type in _build_round_queries_from_plan() (engine.py lines 753-769), bridge queries are built using the existing query construction pipeline, and the bridge round gates on the adaptive quality check (only fires when Round 1 selected_quality < 0.55) so it does not add cost to questions where initial retrieval is already strong.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _extract_evidence_gaps(question, top_abstracts, retrieval_plan) in engine.py immediately after Round 1 relevance scoring. Prompt the planning LLM with the top-5 retrieved abstracts and the original question and ask it to emit pipe-delimited outputs: `entities: e1|e2|e3` (named entities verbatim from abstracts) and `gaps: g1|g2|g3` (sub-facts missing from evidence). Use the same pipe-delimited parser already used for retrieval plan parsing. Cap to 4 gap statements. Return empty lists on any parse failure.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_bridge_queries(gaps, entities, original_plan) to convert extracted gaps and entities into search queries. Concatenate entity names with gap descriptions. Call _extract_lexical_anchors() on each resulting string to ensure chemistry/compound terms are preserved verbatim and not dropped by query relaxation. Cap to 4 queries. If gap extraction produced empty lists, return an empty list so the bridge round is skipped.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Register retrieval_round_bridge as a new round type in _build_round_queries_from_plan() (engine.py lines 753-769). Insert it as the third round in standard mode (after retrieval_round_2_gap_fill, before retrieval_round_3_adversarial) and as the fourth round in research_max mode (after retrieval_methods, before retrieval_adversarial). Gate the whole bridge round on: (a) SPARKIT_ENABLE_BRIDGE_ROUND env var (default 1 for research_max, 0 for standard) and (b) adaptive quality check — only trigger if Round 1 selected_quality < 0.55.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Upgrade _record_relevance_score() (engine.py lines 121-132) to accept an optional bridge_entity_set frozenset argument. When a record's title or abstract contains any bridge entity string (case-insensitive substring match), add 0.4 to the relevance score before the ingestion selection step. This prevents bridge-round documents from being displaced by Round 1 documents during diversity-aware selection since they now compete from a higher base score.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Upgrade _select_best_section_chunk() (engine.py lines 539-554): if a chunk contains any entity from bridge_entity_set AND at least one token from a Round 1 top-claim text, add a cross-link bonus of 0.5 to its chunk score. Tag these chunks with provenance='bridge_linked' in the ClaimEvidence dataclass. Extend ClaimEvidence (shared/schemas/domain.py) with an optional provenance field defaulting to 'standard'.",
      "owner": "orchestrator/engine.py + shared/schemas/domain.py",
      "effort": "medium"
    },
    {
      "step": "Update _build_synthesis_prompt() (engine.py lines 993-1032) and _build_option_dossiers() (engine.py lines 391-433) to group evidence by provenance. Bridge-linked claims are presented in a dedicated 'Chain-linked evidence' subsection immediately after the main evidence bullets, each paired with its Round 1 antecedent entity. The synthesis LLM system prompt is extended with: 'When chain-linked evidence is present, prioritize constructing an explicit multi-step reasoning chain that cites both the base evidence and the bridge evidence.'",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add bridge_round_stats dict to OrchestrationResult and the corresponding TraceStage: bridge_queries_generated (int), bridge_docs_added (int), bridge_entities_extracted (int), bridge_triggered (bool). Log all fields to the existing observability pipeline. Wrap _extract_evidence_gaps() in a try/except that returns empty lists and logs a warning on any LLM or parse failure — bridge round must never crash the pipeline.",
      "owner": "orchestrator/engine.py + shared/schemas/domain.py",
      "effort": "low"
    },
    {
      "step": "Add unit tests in test_synthesis_quality.py: (1) mock _extract_evidence_gaps() to return known entities/gaps and verify bridge queries are generated with lexical anchors preserved; (2) verify retrieval_round_bridge is inserted between gap_fill and adversarial in standard mode and is absent when selected_quality >= 0.55; (3) verify cross-link bonus is applied to records sharing entity overlap; (4) verify bridge round is skipped gracefully when gap extraction raises an exception; (5) verify provenance='bridge_linked' tag propagates into ClaimEvidence and appears in synthesis prompt output.",
      "owner": "orchestrator/tests/",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Dynamic evidence gap extraction replaces static gap-fill queries: after Round 1, the planning LLM reads top retrieved abstracts and emits named sub-facts still missing from the evidence set. These become the query seeds for the bridge round instead of the pre-planned methods/reference queries that are blind to what was actually retrieved. This directly targets the chain-reasoning failure mode where the correct answer requires connecting two papers that are each individually retrieved but whose connection is never queried because neither paper uses the original question's vocabulary.",
    "Entity-anchored bridging queries with lexical anchor preservation: gap extraction explicitly captures domain-specific named entities (compound names, reaction conditions, numerical thresholds, experimental method names) verbatim from retrieved abstracts using _extract_lexical_anchors(). Bridge queries are constructed by concatenating these entity strings with gap descriptions, ensuring the precise chemistry and biology terms that make these queries targeted are not truncated by the query relaxation logic (aggregator.py lines 148-178) that strips quotes and limits terms to 18.",
    "Cross-document relevance boost in ingestion selection: _record_relevance_score() applies a +0.4 bonus to any bridge-round document whose title or abstract shares entity overlap with Round 1 top results. This prevents bridge documents from being eliminated during the diversity-aware selection step (_select_records_for_ingestion() lines 135-168) where they would otherwise compete against a larger pool of Round 1 documents that already dominate the recency and abstract-overlap scores.",
    "Chain-linked chunk selection with cross-document connectivity signal: _select_best_section_chunk() applies a +0.5 score bonus to chunks that contain both a bridge entity AND at least one token from a Round 1 high-confidence claim. This surfaces the specific passage that makes the cross-paper connection rather than a tangentially related section from the same document. The existing scoring (1.4 * question_overlap + 1.9 * focus_overlap) is additive with this bonus, so strong base-relevance chunks still win when the bridge signal is absent.",
    "Selective domain filter bypass for bridge queries: the bridge round's first query is permitted to bypass the Brave search science-domain allowlist (adapters.py lines 320-321) when the extracted bridge entity matches known patterns for chemical registries, patent databases, or NIST data (e.g., CAS number format, USPTO patterns). This allows retrieval of specialized quantitative data (reaction yields, pKa values, crystallographic parameters) from sources not in the academic adapter set, while the allowlist remains active for all subsequent bridge queries to limit noise."
  ],
  "evaluation_plan": [
    "Controlled A/B on HLE-gold-25 balanced subset with fixed provider config (routed mode, anthropic planning + openai synthesis, research_max): run triplicate with bridge round enabled vs triplicate with SPARKIT_ENABLE_BRIDGE_ROUND=0. Primary metric: MCQ accuracy improvement >= 4 pct points using the same rubric.py exact-match scoring and triplicate-averaging methodology already in place. Secondary metric: mean support_coverage change on questions that previously triggered abstain codes.",
    "Track bridge_round_hit_rate per run: percentage of questions where the bridge round adds >= 1 unique document (by DOI or URL deduplication as in aggregator.py lines 182-191) that survives into the final ingestion set. A hit rate below 20% indicates gap extraction is producing non-actionable gaps and the prompt needs revision. A hit rate above 70% indicates earlier rounds are systematically underfiring and the bridge round is compensating for a deeper retrieval planning failure that should be fixed upstream.",
    "Measure support_coverage recovery on near-abstain cases: on the STEM-exam-200 benchmark, filter questions where the baseline run (bridge disabled) had support_coverage between 0.30 and 0.40 (below the 0.40 citation_coverage_below_threshold abstain threshold). Verify that bridge round enabled converts >= 30% of these cases to support_coverage >= 0.40, representing a direct reduction in hard-abstain rates without relaxing the quality gate thresholds.",
    "Count chain_citation_events in synthesized answers: when the synthesis text references both a bridge-linked and a base-round claim in the same sentence or paragraph (detected by co-occurrence of their source citation indices within a 150-character window), log it as a chain_citation_event. Track mean chain_citation_events per question in research_max mode across the HLE-gold-25 set and verify Pearson r > 0.2 with per-question rubric score improvement.",
    "Ablation of cross-link chunk bonus hyperparameter: compare accuracy on a 50-question HLE subset with bridge_link_bonus in {0.0, 0.25, 0.5, 1.0} while holding all other parameters fixed. Select the value that maximizes accuracy without overfitting, validated on the held-out STEM-exam-200 split. This prevents the default of 0.5 from being an untested arbitrary constant.",
    "Monitor per-question cost delta: bridge round adds one planning LLM call (~2000 input tokens at ~$0.003-0.015 depending on provider) plus up to 4 retrieval queries and potential Brave calls at $0.005 each. Validate across 25-question runs that mean cost increase is < $0.08 per question in routed mode and < $0.15 in research_max mode by diffing provider_usage logs between bridge-enabled and bridge-disabled runs. Alert if bridge round triggers on > 80% of questions (suggests quality threshold of 0.55 is too high and should be lowered)."
  ],
  "risks": [
    "Entity hallucination in gap extraction: the planning LLM may invent entity names not present in the retrieved abstracts — a known failure mode for chemistry and biology domains where parametric knowledge bleeds into extraction tasks. Bridge queries built on hallucinated entities consume retrieval budget without benefit and may introduce noise into the evidence set. Mitigation: constrain the gap extraction prompt to only emit entities that appear verbatim as substrings in the provided abstract text; post-process by filtering out any extracted entity string that does not pass a substring check against the concatenated top-5 abstracts before passing it to _build_bridge_queries().",
    "Latency increase for already-sufficient Round 1 retrieval: adding the gap extraction LLM call (~0.8-1.5s) and bridge retrieval round (~1-3s per query, up to 4 queries) to questions where Round 1 already retrieved strong evidence wastes budget. Mitigation: the quality gate (only trigger bridge when Round 1 selected_quality < 0.55) prevents firing on high-quality runs. Additionally, the existing policy.py budget check (should_stop_early() at engine.py lines 1321-1454) will abort the bridge round if max_latency_s or max_cost_usd would be exceeded, preserving existing budget constraints.",
    "Ambiguous entity names producing off-topic bridge documents: a compound name like 'TEMPO' (a radical used in organic chemistry) also appears in unrelated domains. Bridge queries using such entities without domain context may retrieve off-topic documents that dilute the evidence set. Mitigation: prepend domain-context prefix extracted from the research plan task_type field (e.g., 'organic synthesis TEMPO oxidation...') to each bridge query, using the same domain enrichment logic as the query rewriting substitution table in aggregator.py lines 59-75.",
    "Pipeline fragility from new round type and LLM dependency: adding a new code path (bridge round, gap extraction call, bridge entity set propagation through ingestion and chunk selection) increases the number of failure modes. A timeout or API error in gap extraction must not block the pipeline. Mitigation: wrap _extract_evidence_gaps() in a try/except returning empty lists with a logged warning on any failure. Add an explicit integration test that mocks the gap extraction LLM call to raise a RuntimeError and verifies the pipeline completes normally with bridge_triggered=False.",
    "Benchmark overfitting risk when tuning bridge parameters on HLE-gold-25: the cross-link chunk bonus (0.5), quality trigger threshold (0.55), and bridge query cap (4) are all hyperparameters. Tuning them on the 25-question HLE subset may produce values that do not generalize to other question distributions or topics. Mitigation: use HLE-gold-25 only for go/no-go discovery of the feature. Use STEM-exam-200 as the independent validation set for all hyperparameter selection, ensuring no parameter is chosen based on HLE-gold-25 performance alone."
  ]
}
```
