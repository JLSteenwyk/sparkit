Now I have a thorough understanding of the codebase. Let me formulate the proposal based on the actual implementation details.

```json
{
  "proposal_id": "CAGR-001",
  "title": "Citation-Anchored Gap Requerying (CAGR): Evidence-Posterior Retrieval via Claim Uncertainty and Citation Network Convergence",
  "distinctive_angle": "SPARKIT's retrieval is entirely prospective: all queries are planned upfront from the raw question text before any evidence is seen. CAGR inverts this for rounds 2+: it generates a structured draft answer from round-1 evidence, extracts the specific sub-claims that remain low-confidence or unsupported, derives new queries from the language of retrieved documents (not the question), and simultaneously traverses the citation graph of the most-relevant ingested papers to find the primary sources those papers relied on. This collapses the semantic gap between question vocabulary and literature vocabulary, and targets the exact knowledge holes rather than repeating broad sweeps.",
  "summary": "After round-1 retrieval and ingestion, a lightweight draft synthesis step produces a structured partial answer with per-claim confidence tags (HIGH/MEDIUM/LOW/UNSUPPORTED). The LOW and UNSUPPORTED claims drive round-2 query generation using domain terminology extracted directly from retrieved documents — SPARKIT now searches for what it doesn't know in the language the literature actually uses. Simultaneously, reference sections from the top-3 most-relevant ingested documents are parsed for DOIs and fetched; papers cited by 2+ retrieved documents are prioritized as high-authority convergence anchors. A third mechanism adds pre-retrieval concept aliasing: before round 1, an LLM expands key noun phrases from the question into domain-specific synonyms (IUPAC names, gene symbols, EC numbers, UniProt IDs), producing 4-6 alias-aware query variants that bypass SPARKIT's current token-overlap blind spots.",
  "reasoning": "SPARKIT's _record_relevance_score at engine.py:121-132 uses bag-of-words token overlap, which systematically fails for hard science questions where the same concept appears under different naming conventions in the literature. The existing query plan generated by _decompose_retrieval and _build_round_queries_from_plan at engine.py:194-199 is fixed before any evidence is seen; the adaptive gating at engine.py checks only doc-count novelty and quality-gain thresholds, not which specific sub-questions remain unanswered. Hard HLE questions typically require: (a) primary experimental papers that use specialized vocabulary the question doesn't share, (b) mechanistic details buried in sections beyond the abstract (already partially addressed by chunk selection but bottlenecked by the 1200-char limit and single-best-chunk constraint), and (c) landmark papers that are cited across many review papers but never appear in keyword searches because their titles don't overlap with the question. CAGR attacks all three failure modes: synonym aliasing closes the vocabulary gap before retrieval starts; draft-mediated gap requerying uses retrieved document language to re-target round 2; citation graph traversal recovers the primary sources hidden behind the review layer.",
  "expected_impact": {
    "accuracy_delta_pct_points": 7,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _build_concept_aliases(question, focus_terms, provider, plan) to engine.py. Prompt an LLM to emit 4-6 domain-specific synonyms per key noun phrase (IUPAC systematic names, gene symbols, EC numbers, protein family names, MeSH terms). Inject these alias tokens into the boost_terms list passed to _record_relevance_score and append alias-augmented variants to the first round query list in _build_round_queries_from_plan. Gated by SPARKIT_CONCEPT_ALIAS_ENABLED env var, defaulting to 1.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_draft_gap_analysis(question, claims, section_summaries, provider, plan) to engine.py. After round-1 ingestion, synthesize a ~300-token structured draft answer. Prompt format: 'Rate each sub-question of the question as ANSWERED/PARTIAL/MISSING, quoting the exact domain terms from the evidence that could anchor a follow-up search.' Returns a list of GapQuery objects: {sub_question, gap_terms, retrieved_doc_terms, confidence_tag}. This is distinct from the existing synthesis stage — it runs before round 2, consumes only the cheapest available provider (e.g., grok-4-fast-non-reasoning), and its output is never shown to the user.",
      "owner": "orchestrator/engine.py",
      "effort": "high"
    },
    {
      "step": "Modify _build_round_queries_from_plan to accept an optional List[GapQuery] injected from the draft gap analysis. Gap queries are inserted as a new 'retrieval_round_2_gap_targeted' phase whose queries are formed from gap_terms and retrieved_doc_terms verbatim (not from the original question). This phase replaces the existing generic round_2_gap_fill when gap analysis is available. Controlled by SPARKIT_CAGR_GAP_REQUERY_ENABLED env var.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _extract_reference_dois(parsed_sections) to engine.py. After ingestion via fetch_and_parse, scan parsed section text for DOI patterns (regex: 10\\.\\d{4,}/\\S+) and structured reference list markers. Collect DOIs not already in seen_keys. For the top-3 records by _record_relevance_score, attempt to resolve up to 8 reference DOIs each via the existing Crossref and Semantic Scholar adapters in adapters.py using direct DOI lookup. Papers whose DOIs appear in 2+ ingested document reference sections are flagged as convergence_anchors and prepended to the round-3 ingestion list with a +1.5 relevance score boost. Gated by SPARKIT_CITATION_GRAPH_ENABLED env var.",
      "owner": "services/retrieval_service/app/adapters.py + orchestrator/engine.py",
      "effort": "high"
    },
    {
      "step": "Extend LiteratureRecord in models.py with two optional fields: authority_score: float | None (derived from Semantic Scholar citation_count, normalized log-scale) and source_type: str | None ('primary'|'review'|'preprint'|'unknown'). Populate these in the Semantic Scholar and OpenAlex adapters which already return citation count and publication type fields. Incorporate authority_score into _record_relevance_score with a 0.15 weight so landmark papers bubble up even if their title overlap is low.",
      "owner": "services/retrieval_service/app/models.py + adapters.py + orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend _select_best_section_chunk in engine.py to return the top-2 chunks (not just top-1) when the best chunk score exceeds a threshold. Extend the chunk character limit from 1200 to 2000 chars for chunks flagged as 'results' or 'methods' section type. This doubles evidence density for mechanistic questions where the answer is in experimental detail sections, not abstracts.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add observability logging for CAGR. Record gap_queries_generated, gap_queries_resolved (new docs found), citation_anchors_found, citation_anchors_ingested, and alias_terms_generated to the run_observability_metrics table via ObservabilityStore. These fields are needed to evaluate whether each CAGR mechanism independently contributes.",
      "owner": "orchestrator/observability.py + observability_store.py",
      "effort": "low"
    },
    {
      "step": "Add Alembic migration adding authority_score and source_type columns to the documents table and adding cagr_gap_queries and cagr_citation_anchors JSON columns to run_observability_metrics.",
      "owner": "alembic/versions/",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Pre-retrieval concept alias expansion: before round 1, LLM generates 4-6 domain-specific synonyms per key noun phrase (IUPAC names, gene symbols, EC numbers, MeSH terms). Alias tokens are added to boost_terms in _record_relevance_score and appended as query variants in the round-1 query list, directly fixing the token-overlap blindspot at engine.py:121-132 for terminology-divergent hard science questions.",
    "Draft-mediated gap requerying: after round-1 ingestion, a cheap LLM call synthesizes a structured partial answer rating each sub-question as ANSWERED/PARTIAL/MISSING and extracts domain terms from retrieved documents that map to missing knowledge. These become the explicit query strings for round 2, replacing the generic gap_fill queries at engine.py:194-199 with evidence-posterior, literature-vocabulary queries that target exactly the unsolved parts of the question.",
    "Citation graph traversal: parse DOI patterns from reference sections of the top-3 most-relevant ingested documents. Cross-reference those DOIs against all retrieved documents; papers whose DOI appears in 2+ reference lists are flagged as convergence anchors and prioritized for ingestion in round 3 with a +1.5 relevance score boost. This recovers primary-source landmark papers that are cited everywhere but never surface in keyword searches.",
    "Source authority stratification: extend LiteratureRecord with authority_score derived from Semantic Scholar citation_count (log-normalized) and source_type ('primary'/'review'/'preprint'). Incorporate into _record_relevance_score at weight 0.15 so high-citation primary papers rank above low-citation preprints even when title overlap is equal.",
    "Expanded chunk extraction: increase max chunk size from 1200 to 2000 chars for results/methods section types and return top-2 chunks (not just top-1) when the leading chunk score is above threshold, doubling mechanistic evidence density for hard experimental questions."
  ],
  "evaluation_plan": [
    "Per-question gap closure rate: for each HLE-gold-bio-chem question in benchmarks/hle_gold_bio_chem/, record the number of sub-questions rated MISSING after round 1 draft gap analysis and the number resolved after round 2 targeted queries. Report mean gap closure rate across all 149 questions. Target: >60% of identified gaps resolved by targeted requerying.",
    "Citation anchor precision: for each run where citation graph traversal fires, record whether any convergence anchor document contributed a claim used in the final synthesis (tracked via claim_evidence_links table). Report citation anchor precision (anchors that contributed / anchors ingested). A rate below 0.25 indicates the DOI extraction regex or the 2+ co-citation threshold needs tuning.",
    "Terminology alias hit rate: for each query variant generated by concept alias expansion, measure whether it produced at least one unique document not returned by the original query (DOI deduplication). Report alias_unique_doc_rate. A rate below 0.15 indicates alias quality is too low or sources are too overlapping to benefit.",
    "Claim coverage delta vs baseline: run the full HLE-gold-bio-chem suite with CAGR enabled vs disabled (SPARKIT_CONCEPT_ALIAS_ENABLED=0, SPARKIT_CAGR_GAP_REQUERY_ENABLED=0, SPARKIT_CITATION_GRAPH_ENABLED=0). Compare support_coverage from calibration_features table. Expect CAGR to raise mean support_coverage by >=0.08 points on previously-abstaining questions.",
    "Ablation per mechanism: run three additional variants — alias-only, gap-requery-only, citation-graph-only — against the HLE-25 balanced subset used in current benchmarking. This isolates which mechanism contributes most to accuracy delta and lets the team make cost-aware decisions about which mechanisms to keep in production.",
    "Abstain rate comparison: compare the fraction of questions hitting the answerability gate (_abstain_reasons in engine.py:171-191) with CAGR enabled vs disabled. If CAGR reduces abstain rate by >10 percentage points, evidence coverage improvement is confirmed as the dominant mechanism."
  ],
  "risks": [
    "Draft gap analysis adds one additional LLM call per run before round 2. At the cheapest provider (grok-4-fast-non-reasoning at $0.50/M output tokens), a 300-token draft adds roughly $0.0002 per run — negligible. But if the cheap provider is unavailable and the system falls back to a frontier model, cost impact becomes material. Mitigate by enforcing a strict provider assignment for this stage and hard-capping its output tokens at 400.",
    "The concept alias LLM call could hallucinate domain synonyms — especially IUPAC names for complex molecules — which would generate noise queries that waste retrieval budget. Mitigate by prompting the LLM to only produce aliases it is >90% confident about and to prefer well-known symbols (gene names, EC numbers) over derived synonyms. Validate with a spot-check against PubChem for chemistry questions.",
    "Citation graph traversal relies on DOI regex extraction from parsed document text via fetch_and_parse. Reference section parsing quality in ingestion_service/app/parser.py depends on PDF/HTML structure quality; many preprints have poorly formatted reference sections. If DOI extraction yields <1 DOI per document on average, the mechanism effectively does nothing. Monitor via cagr_citation_anchors_found observability field and consider adding a structured reference-list Semantic Scholar API lookup as a fallback.",
    "Extending ingestion to include citation anchor documents adds latency in round 3. For questions already near the max_cost_usd budget, the citation traversal step may be pre-empted by the budget guard at policy.py. This would silently degrade CAGR's benefit. Mitigate by running citation DOI resolution concurrently with the existing round-3 adversarial queries rather than sequentially.",
    "The top-2 chunk extraction change increases total context fed into synthesis. If both chunks from a single document are high-scoring, this effectively double-weights one paper's evidence, which could bias the synthesis. Mitigate by capping each document's total chunk contribution to 2400 chars regardless of how many chunks are selected."
  ]
}
```
