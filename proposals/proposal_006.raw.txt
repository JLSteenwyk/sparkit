```json
{
  "proposal_id": "SPARKIT-PROP-2026-002",
  "title": "Claim-Gap Iterative Retrieval with Contradiction-as-Pivot and Semantic Source Authority Scoring",
  "distinctive_angle": "Invert retrieval from novelty-chasing to precision gap-filling: run a lightweight partial-synthesis probe after round 1 to surface exactly which claims lack grounding, generate targeted retrieval bursts per knowledge gap, and convert detected contradictions from confidence penalties into active disambiguation queries — while weighting evidence selection by domain-specific citation authority rather than source preference heuristics.",
  "summary": "SPARKIT's adaptive retrieval correctly identifies when to stop (novelty + quality-gain thresholds in SPARKIT_ADAPTIVE_RETRIEVAL), but it does not know what it is missing. The engine stops when new documents plateau, even if critical sub-claims remain completely unsupported. This proposal introduces three interlocked mechanisms that make retrieval precision-guided rather than quantity-guided: (1) Claim-Gap-Driven Retrieval (CGDR) — after round 1, run a stripped synthesis call to enumerate which claims have no passage link (currently tracked as 'unsupported_claims' in the calibration vector), then generate focused queries per gap rather than relying solely on the original decomposition query set; (2) Contradiction-as-Pivot (CAP) — the current verifier marks contradiction signals as a penalty on confidence (max 0.35 reduction), but those contradictions reveal the exact axis of disagreement. CAP uses the contradiction pair as a seed to issue a tie-breaker query (e.g., 'meta-analysis comparing [hypothesis A] vs [hypothesis B]') that can resolve ambiguity rather than merely penalizing it; (3) Semantic Source Authority Scoring (SSAS) — document selection currently blends title overlap, abstract overlap, and a flat recency bonus. SSAS adds a citation-authority dimension pulled from Semantic Scholar's citationCount field (already returned by the Semantic Scholar adapter) and venue-tier metadata to bias ingestion toward high-authority documents for ambiguous or contested claims.",
  "reasoning": "On hard STEM questions (HLE-style), the dominant failure mode is not lack of retrieved documents but lack of the right documents for the specific contested sub-claim. CGDR directly addresses this: instead of issuing three rounds of increasingly relaxed general queries, it narrows to the exact knowledge gap after seeing partial evidence. This is architecturally compatible with SPARKIT because 'unsupported_claims' is already computed per run; CGDR just moves that computation earlier and uses it as a retrieval signal. CAP addresses a structural inefficiency: contradiction_flags currently reduce answer_conf by up to 0.35, but the contradiction itself encodes valuable information about the question's contested nature. Issuing a targeted disambiguation query converts a negative signal into a productive retrieval action, and the Brave Search adapter already supports arbitrary query strings at $0.005/request. SSAS addresses the document selection scoring in _select_documents_for_ingestion which computes 2.0*title_overlap + 1.0*abstract_overlap + 0.25*recency_bonus — this rewards keyword match but not epistemic authority. Semantic Scholar already returns citationCount in the adapter response; normalizing it log-scale and blending it (weight ~0.5) into the selection score would preferentially ingest survey papers and landmark studies over conference abstracts for contested claims, directly improving evidence quality without extra API calls.",
  "expected_impact": {
    "accuracy_delta_pct_points": 4.5,
    "cost_impact": "mixed",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add 'partial_synthesis_probe' function in engine.py that runs a minimal synthesis call (max_tokens=512, no MCQ logic) after round 1 of retrieval and returns a list of claim strings that received no passage link in the current evidence graph. Gate behind SPARKIT_CGDR_ENABLED env var (default 1).",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend _decompose_retrieval to accept an optional 'gap_claims' list. When CGDR is active and gap_claims is non-empty, generate a dedicated 'gap_fill' retrieval round with one query per gap claim (max 4 gaps, LLM-prompted to turn each gap claim into a precise search string). Inject this as round 2, shifting existing rounds back by one.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "In _verify_contradiction, after marking contradicting documents, extract the contradiction pair (doc_a title, doc_b title, signal phrase). If SPARKIT_CAP_ENABLED=1, call _gen_disambiguation_query(pair) using the synthesis provider to produce a targeted meta-analysis or review query. Append it to a new 'contradiction_pivot' retrieval round executed before synthesis.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend the Semantic Scholar adapter response parsing in retrieval_service/app/adapters.py to always extract and forward 'citationCount' and 'venue' fields. Add a 'citation_count: int | None' and 'venue: str | None' field to the shared document schema (DocumentRecord or equivalent dict key).",
      "owner": "services/retrieval_service/app/adapters.py",
      "effort": "low"
    },
    {
      "step": "Refactor _select_documents_for_ingestion scoring in engine.py to include a citation_authority_score = log1p(citation_count) / log1p(10000) clamped to [0, 1]. Blend into final score: 1.8*title_overlap + 0.9*abstract_overlap + 0.2*recency_bonus + 0.5*citation_authority_score. Gate with SPARKIT_SSAS_ENABLED=1 (default 1). Preserve existing diversity guarantee (one doc per source before filling).",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add venue-tier lookup table (Nature/Science/Cell/NEJM/Lancet = tier 1: +0.15, top-20 CS venues = tier 1: +0.10, arXiv preprint = tier 3: -0.05) as a static dict in engine.py. Apply as an additive adjustment to citation_authority_score when venue field is present.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Instrument all three mechanisms with dedicated trace stage entries: 'cgdr_gap_probe' (gap claim count, queries generated), 'cap_pivot_query' (contradiction pair, query string), 'ssas_selection' (top-5 doc scores before/after). Store in run_trace for evaluation analysis.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Update the calibration feature vector to include cgdr_gaps_filled (count of gap claims that received a passage link after the gap-fill round) and cap_resolved (bool: did the contradiction pivot yield a new document). Add these to run_calibration_features and include in confidence formula with small weights (+0.03 per gap filled, +0.04 if cap resolved).",
      "owner": "orchestrator/engine.py, services/orchestrator/app/policy.py",
      "effort": "low"
    },
    {
      "step": "Write unit tests in tests/test_synthesis_quality.py (already modified per git status) covering: (a) partial_synthesis_probe returns correct gap list when claims have no evidence links, (b) _decompose_retrieval injects gap_fill round correctly, (c) citation_authority_score computation for edge cases (None citation_count, zero, large values), (d) CAP disambiguation query format validation.",
      "owner": "services/orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    },
    {
      "step": "A/B benchmark on HLE-25 balanced subset: run baseline (current engine) vs CGDR-only vs CGDR+CAP vs CGDR+CAP+SSAS. Track accuracy, support_coverage, unsupported_claims, contradiction_flags, cost_usd, latency_s per variant. Use existing eval_service infrastructure.",
      "owner": "services/eval_service",
      "effort": "high"
    }
  ],
  "retrieval_improvements": [
    "Claim-Gap-Driven Retrieval (CGDR): After retrieval round 1, probe the evidence graph for claims with no passage links (already tracked as unsupported_claims in calibration). Generate up to 4 targeted queries — one per top-priority gap claim — using the synthesis provider with a short prompt instructing it to convert each claim into a precise literature search string. Execute these as a dedicated 'gap_fill' round before the existing adversarial round. This ensures retrieval addresses specific evidentiary holes rather than expanding generally.",
    "Contradiction-as-Pivot (CAP): When _verify_contradiction identifies a contradiction pair (document A asserts X, document B asserts not-X), extract the pair and issue a targeted disambiguation query (e.g., 'systematic review comparing [X] vs [not-X] in [domain]') via Brave Search or academic federation. This converts contradiction detection from a pure penalty (-0.35 confidence) into an active retrieval trigger, potentially surfacing meta-analyses or authoritative consensus papers that resolve the ambiguity.",
    "Semantic Source Authority Scoring (SSAS): The Semantic Scholar adapter already returns citationCount per document. Add log-normalized citation authority (log1p(n)/log1p(10000)) plus venue-tier adjustment (+0.15 for Nature/Science/NEJM tier, -0.05 for unreviewed arXiv) to the document selection scoring formula in _select_documents_for_ingestion. Weight at 0.5 in the blended score to preferentially ingest high-authority evidence for contested claims without eliminating source diversity.",
    "Orthogonal Query Angle Fan-Out: Current query decomposition in _decompose_retrieval generates queries by intent (primary, methods, adversarial, reference). Add a secondary decomposition axis that explicitly generates one query per semantic dimension of the question: (a) phenomenological (what phenomenon/mechanism), (b) quantitative (what measurement/threshold), (c) comparative (how does it differ from alternatives), (d) temporal (when was this established/revised). These four angles capture different facets of hard STEM questions that single-intent queries miss. Gate with SPARKIT_QUERY_FAN_OUT=1.",
    "Deep Section Extraction with Structure-Aware Chunking: Current ingestion caps at SPARKIT_INGESTION_MAX_CHARS=10000 and section summaries extract only first sentence (max 220 chars) from each section bucket. Increase default to 15000 chars and modify _build_section_summaries to extract the first two sentences plus any sentence containing numerics, measurements, or p-values (regex: r'\\d+\\.?\\d*\\s*(%|±|p\\s*[<=>]|nm|kg|mol|eV)') as a high-signal snippet. This captures quantitative findings that are critical for hard numerical/threshold questions."
  ],
  "evaluation_plan": [
    "Support Coverage Delta by Gap Type: For each run with CGDR enabled, compare support_coverage before vs after the gap_fill round (log both in the cgdr_gap_probe trace stage). A successful CGDR implementation should show support_coverage improvement >= 0.08 on questions where >= 2 gap claims were detected. Fail criterion: gap_fill round adds documents but support_coverage does not improve (indicates query quality issue in gap claim to query conversion).",
    "Contradiction Resolution Rate: Track the fraction of runs where CAP was triggered (contradiction_flags >= 1) and the pivot query returned at least one new document not already in the evidence set. Target: >= 40% resolution rate (new disambiguating document found). Also measure whether cap_resolved=True correlates with lower final contradiction_flags in the calibration vector, verifying the mechanism actually updates the evidence graph.",
    "SSAS Citation Authority vs Accuracy Correlation: For the HLE-25 A/B benchmark, compute the mean citation_count of the top-5 ingested documents per run. Compare this metric between correct vs incorrect answers (using HLE gold labels). A positive correlation (correct answers used higher-authority sources) validates that SSAS improves evidence quality. Target Pearson r >= 0.15 between mean_citation_count and binary correctness.",
    "MCQ Option False-Elimination Rate: For MCQ questions, track how often the correct answer option is eliminated in the elimination pass (a fatal error). Run the HLE MCQ subset with and without CGDR+SSAS. Target: reduce false elimination rate by >= 20% relative (e.g., from 15% to 12%). Measure via gold-label comparison in eval_service.",
    "Latency and Cost Budget Compliance: Verify that CGDR gap_fill round and CAP pivot query do not push runs over max_cost_usd or max_latency_s guardrails. Check the existing should_stop_early budget guard is called before gap_fill and CAP rounds. Acceptance criterion: < 5% of runs exceed budget with all three mechanisms enabled at default settings (max_cost_usd=$3.0, max_latency_s=300s).",
    "End-to-End Accuracy A/B on HLE-25 Balanced Subset: Run four variants — (1) baseline, (2) CGDR-only, (3) CGDR+CAP, (4) CGDR+CAP+SSAS — each with N>=3 replicate runs per question for variance estimation. Primary metric: exact-match accuracy on HLE gold labels. Secondary metrics: mean support_coverage, mean unsupported_claims, mean answer_conf calibration error (|conf - accuracy|). Statistical test: paired t-test with p < 0.05 threshold for declaring improvement."
  ],
  "risks": [
    "CGDR partial synthesis probe adds a full LLM call cost per run (~$0.02-0.10 depending on provider), increasing total run cost by 5-15% for research_max mode. Mitigate by capping probe at max_tokens=512 and using the cheapest configured provider (DeepSeek at $0.42/M output or Kimi at $3.00/M output) rather than the synthesis provider.",
    "CAP pivot queries may target the same contradicting pair repeatedly if the disambiguation query returns documents that themselves contradict, creating a cascade. Mitigate by limiting CAP to one pivot query per run (first detected contradiction pair only) and checking that the returned documents are not already in the evidence set before triggering a second round.",
    "SSAS citation authority scoring may introduce recency bias against cutting-edge research that has high novelty but low citation count (e.g., a 2024 paper with 50 citations vs a 2010 paper with 5000 citations). Mitigate by normalizing citation count within a recency window: use citations_per_year = citationCount / max(years_since_publication, 1) and cap recency adjustment to avoid penalizing recent high-quality work.",
    "Gap claim extraction in the partial synthesis probe depends on LLM output quality — if the partial synthesis generates vague or over-broad claims, the gap queries will be correspondingly vague and retrieve irrelevant documents. Mitigate by constraining the probe prompt to return claims as short, entity-dense noun phrases (max 10 words each) and adding a fallback: if gap_claims contains terms already in the original query set, skip the gap_fill round.",
    "Venue-tier lookup table maintenance: the static venue dict will become stale as publication venues change prestige or new venues emerge. Mitigate by keeping the venue tier adjustment small (+0.15 max) so stale entries do not dominate, and reviewing the table quarterly or when accuracy benchmarks show regression.",
    "Increased latency from gap_fill and CAP rounds may cause runs to hit max_latency_s limits, triggering early termination that skips synthesis entirely. Mitigate by scheduling CGDR and CAP rounds early in the pipeline (before adversarial rounds) and applying the budget guard check before each round to ensure sufficient time/cost remains for synthesis."
  ]
}
```
