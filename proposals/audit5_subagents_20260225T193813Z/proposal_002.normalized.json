{
  "proposal_id": "audit5-evidence-trust-contradiction-v1",
  "title": "Source-Authority-Weighted Claim Triangulation with Contradiction Resolution",
  "distinctive_angle": "Instead of treating all retrieved passages as equally credible, assign explicit authority tiers to sources and run atomic-claim extraction + contradiction scoring before the answer generation step. Contradictory low-authority claims are suppressed; high-authority agreement boosts claim confidence; genuine unresolved contradictions are surfaced to the LLM as explicit uncertainty signals rather than being silently concatenated into context.",
  "summary": "Add a pre-generation stage that (1) scores each retrieved passage by source authority (peer-reviewed journal > curated textbook > preprint > general web), (2) decomposes passages into atomic factual claims, (3) detects pairwise claim contradictions via entailment scoring, (4) resolves contradictions by authority-weighted majority, and (5) injects a structured evidence manifest into the prompt that labels each claim with its authority score and conflict status. The LLM receives a curated, conflict-annotated context instead of a raw concatenation of potentially contradictory snippets.",
  "reasoning": "Hard STEM questions fail primarily because (a) the model is given contradictory retrieved snippets and silently picks the wrong one, (b) low-quality sources (Stack Exchange, Wikipedia stubs, blog posts) outrank high-quality ones by BM25/embedding proximity, and (c) the model has no signal distinguishing 'sources disagree' from 'only one source exists'. Providing an explicit evidence manifest with authority weights and contradiction flags lets the model apply calibrated skepticism: it can hedge correctly when sources conflict and be confident when authoritative sources converge. This targets the exact failure mode dominating HLE-style hard STEM benchmarks.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Build source authority taxonomy: map domain (biology, chemistry, physics, math) \u00d7 venue type (Nature/Science/Cell, specialist journals, arXiv, textbooks, preprints, web) to a scalar authority score 0\u20131. Store as a config YAML so it is auditable and tunable.",
      "owner": "retrieval-engineer",
      "effort": "low"
    },
    {
      "step": "Add a claim extraction micro-model call after retrieval: for each top-K passage, prompt a fast model (Haiku) to decompose the passage into \u22645 atomic factual claims with source metadata attached.",
      "owner": "prompt-engineer",
      "effort": "medium"
    },
    {
      "step": "Run pairwise NLI (Natural Language Inference) contradiction scoring across claims from different passages. Use a cached cross-encoder or a Haiku NLI call. Flag claim pairs with contradiction probability > 0.7.",
      "owner": "ml-engineer",
      "effort": "high"
    },
    {
      "step": "Authority-weighted resolution: for each contradiction cluster, compute authority-weighted vote. If winning side authority \u2265 0.6 and losing side authority \u2264 0.3, suppress losing claim. Otherwise mark as 'unresolved conflict' and pass both to LLM.",
      "owner": "ml-engineer",
      "effort": "medium"
    },
    {
      "step": "Assemble structured evidence manifest: a compact JSON block prepended to the generation prompt listing each claim, its source authority score, supporting passage ID, and conflict status (confirmed | conflicted | unresolved). Instruct the LLM to use this manifest to calibrate confidence.",
      "owner": "prompt-engineer",
      "effort": "medium"
    },
    {
      "step": "Benchmark on HLE biology/chemistry barometer set and full HLE-25 balanced subset. Compare accuracy and calibration (ECE) against baseline raw-concatenation retrieval.",
      "owner": "eval-engineer",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Re-rank retrieved passages by a composite score: 0.5 \u00d7 semantic similarity + 0.3 \u00d7 source authority score + 0.2 \u00d7 citation count proxy (when available), replacing pure embedding similarity ranking.",
    "Add venue-aware retrieval filters: for hard STEM queries classified as requiring primary literature (via a lightweight query classifier), deprioritize passages from non-peer-reviewed sources unless top-K from peer-reviewed sources is < 3.",
    "Store source metadata (DOI, venue, year, citation count) as retrieval index fields so authority scoring is a lookup, not a post-hoc parse, keeping latency overhead minimal."
  ],
  "evaluation_plan": [
    "Primary: accuracy on HLE biology/chemistry barometer set (questions_barometer10_direct30.json) and HLE-25 balanced subset \u2014 compare exact-match and model-graded correctness before/after.",
    "Calibration audit: measure Expected Calibration Error (ECE) on questions where the system expresses high vs. low confidence; verify that 'unresolved conflict' flagging correlates with lower final accuracy (i.e., the signal is honest).",
    "Contradiction suppression precision: manually review 50 randomly sampled contradiction-flagged claim pairs to estimate false-positive suppression rate; target < 15% false suppressions to avoid discarding correct evidence."
  ],
  "risks": [
    "Claim extraction and NLI calls add 1\u20132 extra LLM round-trips per query, increasing latency ~40% and cost ~25%; mitigate by batching and using Haiku for sub-steps.",
    "NLI models can misfire on domain-specific STEM notation (equations, chemical formulas), producing spurious contradictions; mitigate by adding a formula-aware preprocessing step and a minimum text-length guard before NLI scoring.",
    "Authority taxonomy may encode venue biases (e.g., underweighting legitimate arXiv-only fields like theoretical CS or ML); taxonomy must be reviewed per domain before deployment."
  ]
}