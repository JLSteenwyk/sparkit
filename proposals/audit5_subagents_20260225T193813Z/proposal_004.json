```json
{
  "proposal_id": "audit5-calibration-option-rescue",
  "title": "Calibration-Gated Option Rescue: Active Evidence-Confusion Detection and Per-Option Evidence Re-Ranking",
  "distinctive_angle": "Use the calibration signal (Brier/ECE + per-option evidence balance) as an active answer-selection override — not just a reporting metric — by detecting evidence-confusion regimes and triggering a structured MCQ elimination pass before committing to a final answer.",
  "summary": "SPARKIT currently underperforms direct calls (0.2 vs 0.3 on barometer) despite extensive retrieval investment. Error taxonomy from the grading+calibration lens reveals the bottleneck is not retrieval volume but synthesis-stage evidence confusion: retrieved passages are being synthesized into free-text that is then parsed for a letter answer, with no structured per-option evidence weighting. The proposal introduces a calibration-gated option rescue stage: when calibration confidence falls below a tunable threshold after first-pass synthesis, the system triggers a structured per-option evidence scoring pass using already-persisted evidence (zero additional retrieval cost), computes a support minus contradiction delta per MCQ option, and overrides the synthesis-derived answer with the highest-scoring option. This converts the already-existing mcq_option_scorer trace artifact from a passive diagnostic into an active answer-selection pathway.",
  "reasoning": "Three converging signals justify this as the highest-leverage intervention: (1) GRADING — the barometer is 100% MCQ with strict letter matching, meaning every error is a discrete option-selection failure, not a phrasing failure; direct LLM calls score 0.3 while SPARKIT scores 0.2 after all retrieval upgrades, confirming that retrieval is not the limiting factor. (2) CALIBRATION — Brier scores of 0.54-0.62 and ECE of 0.64-0.69 are severely over-confident relative to 0.2 accuracy, indicating the calibration signal is currently capturing real uncertainty that is not being acted upon. (3) ERROR TAXONOMY — the failure mode is almost certainly 'synthesis dilution': the model's pre-trained MCQ prior (which achieves 0.3 direct) is being diluted by off-topic or weakly-relevant evidence that shifts the synthesis away from the correct option. The fix is not more retrieval but smarter use of calibration to detect when synthesis should be overridden by structured option scoring. The mcq_option_scorer stage already produces per-option support/contradiction scores but is never consulted for final answer selection — this is the lowest-cost, highest-signal path to correctness gain.",
  "expected_impact": {
    "accuracy_delta_pct_points": 15,
    "cost_impact": "decrease",
    "latency_impact": "decrease",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add calibration confidence threshold gate in orchestrator answer-selection stage: after first-pass synthesis, check if calibration_confidence < SPARKIT_OPTION_RESCUE_THRESHOLD (default 0.55). If triggered, emit trace stage 'option_rescue_triggered' with reason.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Implement option_rescue_scorer: read already-persisted mcq_option_scorer artifacts (support_score, contradiction_score per option) from the current run trace. Compute net_score = support_score - 1.5 * contradiction_score per option. Select option with highest net_score as rescue_answer. Emit 'option_rescue_result' trace artifact with per-option scores and selection rationale.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Wire rescue_answer as override when option rescue triggers: replace synthesis-derived answer letter with rescue_answer in the final answer payload. Preserve original synthesis text but append structured option-scoring summary to synthesis for grader transparency.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Add fallback logic: if all options have net_score <= 0 (no clear winner from evidence), fall back to direct-model synthesis answer without evidence to exploit the model prior (direct-call-style pass with same question, no context). Gate this with SPARKIT_OPTION_RESCUE_ALLOW_PRIOR_FALLBACK flag.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add calibration feature: record whether option_rescue was triggered and whether rescue_answer matched final graded outcome. Feed into calibration training data for future Brier/ECE improvement.",
      "owner": "eval_service",
      "effort": "low"
    },
    {
      "step": "Run barometer10 SPARKIT with option rescue enabled (SPARKIT_OPTION_RESCUE_THRESHOLD=0.55). Compare against locked direct baseline (0.3) and prior SPARKIT baseline (~0.2). Record rescue trigger rate, override rate, and per-question outcome delta.",
      "owner": "benchmark harness",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "No additional retrieval rounds needed — option rescue reuses already-persisted mcq_option_scorer evidence from existing retrieval runs, making this zero-retrieval-cost at rescue time.",
    "If option rescue triggers frequently, consider tightening retrieval round 3 (adversarial) to require minimum per-option evidence coverage (at least 1 supporting passage per MCQ option) before synthesis — this ensures the rescue scorer has non-zero material to work with.",
    "Add a post-rescue retrieval top-up: if rescue_answer net_score < 0.2 (weak winner), run one targeted retrieval query for the top-2 options and re-score. Gate with SPARKIT_OPTION_RESCUE_TOPUP_ENABLED to avoid latency blowup."
  ],
  "evaluation_plan": [
    "Run barometer10 with option_rescue enabled (threshold sweep: 0.45, 0.55, 0.65) against locked direct-0.3 baseline. Primary metric: average_rubric_score. Secondary: rescue trigger rate, override rate, cases where rescue overrode a correct synthesis answer (regression audit).",
    "Run per-question error taxonomy analysis on barometer10: classify each question outcome as (a) correct synthesis + correct rescue [no change needed], (b) wrong synthesis + correct rescue [rescue win], (c) correct synthesis + wrong rescue [rescue harm], (d) wrong synthesis + wrong rescue [both fail]. This directly measures rescue precision.",
    "Track calibration metrics pre/post: Brier score and ECE on barometer10. Expect Brier to drop toward 0.35-0.40 if rescue aligns answer confidence with true accuracy. If Brier worsens, rescue is selecting with false confidence and threshold needs raising."
  ],
  "risks": [
    "Option rescue may override correct synthesis answers with incorrect option-scorer picks if the per-option evidence scores are noisy (low document count, off-topic passages). Mitigate: only trigger rescue when calibration_confidence < threshold AND at least 2 options have non-zero evidence coverage.",
    "The mcq_option_scorer artifacts may not be populated on all question types or may have been gated off by cost/latency guards in prior runs. Mitigate: make option rescue a no-op (pass-through) if mcq_option_scorer trace is absent, and log a 'rescue_skipped_no_scorer_data' trace event.",
    "Rescue threshold is a new hyperparameter that may overfit to the 10-question barometer. Mitigate: validate on HLE-20 and HLE-5 mixed slices before treating barometer gain as generalizable. Lock threshold only after cross-slice agreement.",
    "Prior-fallback mode (no-context synthesis when evidence is uniformly weak) introduces a code path that effectively routes some questions through direct-call behavior inside SPARKIT — this may obscure attribution and create drift in cost/latency accounting. Mitigate: emit explicit 'prior_fallback_used' trace flag and count separately in benchmark metrics."
  ]
}
```
