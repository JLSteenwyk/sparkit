[
  {
    "proposal_id": "audit5-calibration-option-rescue",
    "title": "Calibration-Gated Option Rescue: Active Evidence-Confusion Detection and Per-Option Evidence Re-Ranking",
    "distinctive_angle": "Use the calibration signal (Brier/ECE + per-option evidence balance) as an active answer-selection override \u2014 not just a reporting metric \u2014 by detecting evidence-confusion regimes and triggering a structured MCQ elimination pass before committing to a final answer.",
    "summary": "SPARKIT currently underperforms direct calls (0.2 vs 0.3 on barometer) despite extensive retrieval investment. Error taxonomy from the grading+calibration lens reveals the bottleneck is not retrieval volume but synthesis-stage evidence confusion: retrieved passages are being synthesized into free-text that is then parsed for a letter answer, with no structured per-option evidence weighting. The proposal introduces a calibration-gated option rescue stage: when calibration confidence falls below a tunable threshold after first-pass synthesis, the system triggers a structured per-option evidence scoring pass using already-persisted evidence (zero additional retrieval cost), computes a support minus contradiction delta per MCQ option, and overrides the synthesis-derived answer with the highest-scoring option. This converts the already-existing mcq_option_scorer trace artifact from a passive diagnostic into an active answer-selection pathway.",
    "reasoning": "Three converging signals justify this as the highest-leverage intervention: (1) GRADING \u2014 the barometer is 100% MCQ with strict letter matching, meaning every error is a discrete option-selection failure, not a phrasing failure; direct LLM calls score 0.3 while SPARKIT scores 0.2 after all retrieval upgrades, confirming that retrieval is not the limiting factor. (2) CALIBRATION \u2014 Brier scores of 0.54-0.62 and ECE of 0.64-0.69 are severely over-confident relative to 0.2 accuracy, indicating the calibration signal is currently capturing real uncertainty that is not being acted upon. (3) ERROR TAXONOMY \u2014 the failure mode is almost certainly 'synthesis dilution': the model's pre-trained MCQ prior (which achieves 0.3 direct) is being diluted by off-topic or weakly-relevant evidence that shifts the synthesis away from the correct option. The fix is not more retrieval but smarter use of calibration to detect when synthesis should be overridden by structured option scoring. The mcq_option_scorer stage already produces per-option support/contradiction scores but is never consulted for final answer selection \u2014 this is the lowest-cost, highest-signal path to correctness gain.",
    "expected_impact": {
      "accuracy_delta_pct_points": 15,
      "cost_impact": "decrease",
      "latency_impact": "decrease",
      "confidence": "medium"
    },
    "implementation_plan": [
      {
        "step": "Add calibration confidence threshold gate in orchestrator answer-selection stage: after first-pass synthesis, check if calibration_confidence < SPARKIT_OPTION_RESCUE_THRESHOLD (default 0.55). If triggered, emit trace stage 'option_rescue_triggered' with reason.",
        "owner": "orchestrator",
        "effort": "low"
      },
      {
        "step": "Implement option_rescue_scorer: read already-persisted mcq_option_scorer artifacts (support_score, contradiction_score per option) from the current run trace. Compute net_score = support_score - 1.5 * contradiction_score per option. Select option with highest net_score as rescue_answer. Emit 'option_rescue_result' trace artifact with per-option scores and selection rationale.",
        "owner": "orchestrator",
        "effort": "low"
      },
      {
        "step": "Wire rescue_answer as override when option rescue triggers: replace synthesis-derived answer letter with rescue_answer in the final answer payload. Preserve original synthesis text but append structured option-scoring summary to synthesis for grader transparency.",
        "owner": "orchestrator",
        "effort": "low"
      },
      {
        "step": "Add fallback logic: if all options have net_score <= 0 (no clear winner from evidence), fall back to direct-model synthesis answer without evidence to exploit the model prior (direct-call-style pass with same question, no context). Gate this with SPARKIT_OPTION_RESCUE_ALLOW_PRIOR_FALLBACK flag.",
        "owner": "orchestrator",
        "effort": "medium"
      },
      {
        "step": "Add calibration feature: record whether option_rescue was triggered and whether rescue_answer matched final graded outcome. Feed into calibration training data for future Brier/ECE improvement.",
        "owner": "eval_service",
        "effort": "low"
      },
      {
        "step": "Run barometer10 SPARKIT with option rescue enabled (SPARKIT_OPTION_RESCUE_THRESHOLD=0.55). Compare against locked direct baseline (0.3) and prior SPARKIT baseline (~0.2). Record rescue trigger rate, override rate, and per-question outcome delta.",
        "owner": "benchmark harness",
        "effort": "low"
      }
    ],
    "retrieval_improvements": [
      "No additional retrieval rounds needed \u2014 option rescue reuses already-persisted mcq_option_scorer evidence from existing retrieval runs, making this zero-retrieval-cost at rescue time.",
      "If option rescue triggers frequently, consider tightening retrieval round 3 (adversarial) to require minimum per-option evidence coverage (at least 1 supporting passage per MCQ option) before synthesis \u2014 this ensures the rescue scorer has non-zero material to work with.",
      "Add a post-rescue retrieval top-up: if rescue_answer net_score < 0.2 (weak winner), run one targeted retrieval query for the top-2 options and re-score. Gate with SPARKIT_OPTION_RESCUE_TOPUP_ENABLED to avoid latency blowup."
    ],
    "evaluation_plan": [
      "Run barometer10 with option_rescue enabled (threshold sweep: 0.45, 0.55, 0.65) against locked direct-0.3 baseline. Primary metric: average_rubric_score. Secondary: rescue trigger rate, override rate, cases where rescue overrode a correct synthesis answer (regression audit).",
      "Run per-question error taxonomy analysis on barometer10: classify each question outcome as (a) correct synthesis + correct rescue [no change needed], (b) wrong synthesis + correct rescue [rescue win], (c) correct synthesis + wrong rescue [rescue harm], (d) wrong synthesis + wrong rescue [both fail]. This directly measures rescue precision.",
      "Track calibration metrics pre/post: Brier score and ECE on barometer10. Expect Brier to drop toward 0.35-0.40 if rescue aligns answer confidence with true accuracy. If Brier worsens, rescue is selecting with false confidence and threshold needs raising."
    ],
    "risks": [
      "Option rescue may override correct synthesis answers with incorrect option-scorer picks if the per-option evidence scores are noisy (low document count, off-topic passages). Mitigate: only trigger rescue when calibration_confidence < threshold AND at least 2 options have non-zero evidence coverage.",
      "The mcq_option_scorer artifacts may not be populated on all question types or may have been gated off by cost/latency guards in prior runs. Mitigate: make option rescue a no-op (pass-through) if mcq_option_scorer trace is absent, and log a 'rescue_skipped_no_scorer_data' trace event.",
      "Rescue threshold is a new hyperparameter that may overfit to the 10-question barometer. Mitigate: validate on HLE-20 and HLE-5 mixed slices before treating barometer gain as generalizable. Lock threshold only after cross-slice agreement.",
      "Prior-fallback mode (no-context synthesis when evidence is uniformly weak) introduces a code path that effectively routes some questions through direct-call behavior inside SPARKIT \u2014 this may obscure attribution and create drift in cost/latency accounting. Mitigate: emit explicit 'prior_fallback_used' trace flag and count separately in benchmark metrics."
    ]
  },
  {
    "proposal_id": "audit5-orch-savl-001",
    "title": "Staged Adversarial Verification Loop (SAVL): Falsification-First Critic Orchestration",
    "distinctive_angle": "Treat falsification as a first-class orchestration primitive rather than a post-hoc review step. A dedicated Critic agent is tasked with disproving each intermediate claim before the Orchestrator accepts it, forcing targeted re-retrieval and re-planning on any disputed node rather than only at the final answer boundary.",
    "summary": "Replace the current single-pass planner\u2192answer\u2192critic flow with a structured multi-stage loop: (1) Planner decomposes the hard STEM question into an explicit DAG of intermediate claims with stated assumptions; (2) Critic agents independently attempt to falsify each node via adversarial retrieval and symbolic checking; (3) Verifier runs dimensional/unit/order-of-magnitude consistency checks on numerical results; (4) Orchestrator routes failed nodes back to the Planner with failure reasons as injected constraints, capping total iterations. Only answers where all nodes survive falsification and pass Verifier are emitted with high confidence.",
    "reasoning": "Hard STEM failures in SPARKIT cluster around two failure modes: (a) plausible-but-wrong intermediate steps that propagate undetected to the final answer, and (b) retrieval gaps where the system hallucinates domain facts instead of fetching them. A falsification-first critic loop directly attacks both: adversarial retrieval on each intermediate claim surfaces gaps before they compound, and the symbolic Verifier catches unit/magnitude errors that language models systematically miss. This tightens the feedback loop from 'answer-level correction' to 'claim-level correction', dramatically reducing error propagation in multi-step STEM reasoning chains. The DAG decomposition also enables parallel critic execution across independent sub-claims, partially offsetting latency cost.",
    "expected_impact": {
      "accuracy_delta_pct_points": 8,
      "cost_impact": "increase",
      "latency_impact": "increase",
      "confidence": "medium"
    },
    "implementation_plan": [
      {
        "step": "Define ClaimDAG schema: each node has claim_text, dependencies[], assumptions[], evidence_refs[], status (unverified|verified|falsified)",
        "owner": "orchestrator-core",
        "effort": "medium"
      },
      {
        "step": "Implement Planner v2 prompt that outputs a ClaimDAG JSON instead of a flat reasoning chain; include explicit assumption listing per node",
        "owner": "planner-module",
        "effort": "medium"
      },
      {
        "step": "Build Critic agent: given a ClaimDAG node, construct a falsification query (negation of claim + retrieval), return (verdict, evidence, rebuttal_hint)",
        "owner": "critic-module",
        "effort": "high"
      },
      {
        "step": "Build Verifier agent: for nodes containing numerical results, run dimensional analysis, order-of-magnitude sanity check, and cross-reference against known constants; flag if result deviates by >1 OOM from expectation",
        "owner": "verifier-module",
        "effort": "high"
      },
      {
        "step": "Implement Orchestrator loop: fan-out Critic calls across independent DAG nodes in parallel; on falsification, inject rebuttal_hint into Planner context and re-plan only the affected sub-DAG; cap at 3 re-plan iterations per node",
        "owner": "orchestrator-core",
        "effort": "high"
      },
      {
        "step": "Add confidence scoring: final answer confidence = f(fraction of nodes verified, number of re-plan iterations consumed, Verifier deviation score)",
        "owner": "scoring-module",
        "effort": "low"
      },
      {
        "step": "Benchmark on HLE hard STEM subset (bio/chem/physics/math) comparing baseline vs SAVL on accuracy, token cost, and latency",
        "owner": "eval-pipeline",
        "effort": "medium"
      }
    ],
    "retrieval_improvements": [
      "Adversarial retrieval per node: query = negation of claim + domain context, forcing the retrieval system to surface contradicting evidence rather than confirming evidence \u2014 this catches hallucinated facts that confirmatory retrieval misses",
      "Assumption-targeted retrieval: for each explicit assumption listed in a ClaimDAG node, run a dedicated retrieval pass to verify the assumption holds in the specific domain context (e.g. ideal gas assumption validity range)",
      "Verifier-triggered retrieval: when Verifier detects a numerical anomaly, automatically fetch canonical reference values (physical constants, reaction enthalpies, etc.) from authoritative sources and inject into the re-plan context"
    ],
    "evaluation_plan": [
      "Primary metric: accuracy on HLE hard STEM subset (target: +6-10 pp over baseline), stratified by domain (math, physics, chemistry, biology) to identify where SAVL helps most",
      "Process metric: track rate at which Critic falsifies intermediate nodes before they reach final answer (high falsification-before-answer rate = SAVL catching errors early as intended)",
      "Cost/latency profiling: measure mean tokens-per-correct-answer and mean wall-clock latency vs baseline, with Pareto frontier analysis to identify optimal iteration cap and parallelism settings"
    ],
    "risks": [
      "Critic over-aggressiveness: adversarial retrieval may surface superficially contradicting but actually irrelevant evidence, causing false falsifications that force unnecessary re-planning and degrade correct answers \u2014 mitigate by requiring Critic to score evidence relevance before issuing falsification verdict",
      "Iteration cap too low: capping at 3 re-plan iterations may be insufficient for deeply nested multi-step problems (e.g. 8-step organic synthesis), leaving errors uncorrected \u2014 mitigate by making cap configurable per problem depth estimate from Planner",
      "Latency blowup on parallel Critic fan-out: for ClaimDAGs with many independent nodes, parallel Critic calls may saturate rate limits \u2014 mitigate with priority-based fan-out that critiques high-centrality nodes first"
    ]
  },
  {
    "proposal_id": "audit5-retrieval-sed-001",
    "title": "Stepwise Epistemic Decomposition with Marginal-Coverage Reranking",
    "distinctive_angle": "Sequential, context-conditioned query decomposition where each sub-query is informed by what prior retrieval steps already established \u2014 eliminating redundant retrieval and targeting only the residual knowledge gap at each stage, with reranking scored on marginal information gain rather than relevance-to-query.",
    "summary": "Hard STEM questions typically require establishing a chain of intermediate facts before the final answer can be derived. Current parallel decomposition retrieves sub-queries independently, causing (a) redundant passages that crowd out genuinely novel evidence, and (b) poor coverage of late-chain facts that depend on early-chain context. SED decomposes questions into an ordered epistemic dependency graph, retrieves sequentially so each step's query is conditioned on prior retrieved context, and reranks candidates at each step by marginal coverage \u2014 the fraction of unresolved claim-slots they fill beyond what is already in the running context window.",
    "reasoning": "On hard STEM questions (chemistry mechanisms, multi-step physics derivations, genomics pathway questions), the information needed is typically compositional: Fact B is only retrievable with the right terminology once Fact A is known. A query for 'inhibition of CYP3A4 by macrolide antibiotics causing QT prolongation' is too broad; but after retrieving a passage establishing CYP3A4's role in pimozide metabolism, the follow-up query 'pimozide plasma concentration cardiac arrhythmia threshold' becomes precise and high-recall. Parallel decomposition misses this because sub-queries are generated before any retrieval context exists. Marginal-coverage reranking then ensures that the top-k passages fed to the reasoning model collectively span all claim-slots rather than converging on a single well-matched but informationally redundant cluster. This directly addresses the three audit lens areas: coverage (sequential retrieval finds late-chain facts), query decomposition (dependency graph ordering), and reranking (marginal gain objective).",
    "expected_impact": {
      "accuracy_delta_pct_points": 7,
      "cost_impact": "increase",
      "latency_impact": "increase",
      "confidence": "medium"
    },
    "implementation_plan": [
      {
        "step": "Add an EpistemicDecomposer module that calls the LLM once to emit an ordered list of (claim_slot, depends_on) tuples from the question, forming a DAG of prerequisite facts.",
        "owner": "retrieval-team",
        "effort": "medium"
      },
      {
        "step": "Implement sequential retrieval loop: for each claim_slot in topological order, construct the sub-query by appending a 'given: {prior_context_summary}' prefix so the embedding captures late-chain specificity.",
        "owner": "retrieval-team",
        "effort": "medium"
      },
      {
        "step": "Replace cosine-similarity reranking with Marginal Coverage Reranker: score each candidate passage by |new_claim_slots_covered| / |total_unresolved_claim_slots|, using a lightweight NLI model to check coverage.",
        "owner": "reranking-team",
        "effort": "high"
      },
      {
        "step": "Add a coverage-completion gate: halt retrieval when all claim_slots reach coverage >= 0.85 or max_steps is reached, preventing over-retrieval on easy questions.",
        "owner": "retrieval-team",
        "effort": "low"
      },
      {
        "step": "Evaluate on HLE biology/chemistry subset and the existing barometer benchmark; compare total tokens retrieved, redundancy ratio (cosine similarity among top-k passages), and final accuracy.",
        "owner": "eval-team",
        "effort": "medium"
      }
    ],
    "retrieval_improvements": [
      "Sequential context-conditioned sub-queries improve recall of late-chain terminology-specific facts that parallel decomposition systematically misses",
      "Marginal Coverage Reranker eliminates passage-cluster redundancy, fitting more distinct evidence into the fixed top-k context window",
      "Coverage-completion gate reduces over-retrieval on simple sub-questions, keeping token budget for genuinely hard claim-slots"
    ],
    "evaluation_plan": [
      "Run on HLE-25 balanced subset (existing benchmark) and report accuracy delta vs current claim-gap baseline, segmented by STEM domain (bio/chem/physics/math)",
      "Measure retrieval quality intrinsically: for a held-out set with annotated claim-slots, report claim-slot recall@k before and after, and inter-passage redundancy (mean pairwise cosine similarity of top-6 passages)",
      "Ablation study: sequential-only (no marginal reranking) vs marginal-reranking-only (parallel decomp) vs full SED, to isolate contribution of each component"
    ],
    "risks": [
      "Sequential retrieval increases latency linearly with DAG depth; mitigation: parallelize independent sibling nodes in the DAG rather than forcing fully sequential execution",
      "EpistemicDecomposer adds one LLM call per question; if decomposition quality is poor (e.g. wrong dependency ordering), downstream retrieval degrades \u2014 needs fallback to parallel decomp when DAG confidence is low",
      "NLI-based coverage scoring is approximate; a weak NLI model may incorrectly mark claim-slots as covered, causing premature gate termination \u2014 validate NLI calibration on STEM-specific entailment pairs before deployment"
    ]
  },
  {
    "proposal_id": "sparkit-audit5-throughput-001",
    "title": "Parallel Speculative Sub-Problem Decomposition with Semantic Intermediate Cache",
    "distinctive_angle": "Hard STEM questions share latent sub-problems (e.g., unit conversions, integral forms, reaction mechanisms) across queries. Parallelizing decomposition+solving while semantically caching solved sub-problems eliminates redundant LLM calls and saturates available compute, directly increasing accuracy per token budget.",
    "summary": "Decompose each hard STEM question into N independent sub-problems via a fast decomposer call, then dispatch all sub-problem solvers in parallel (not sequentially). Cache each (sub-problem-embedding, solution) pair in a vector KV store with a cosine-similarity lookup threshold (~0.93). On cache hit, inject the cached solution directly into the aggregation context. Run M=3 speculative full-solution drafts in parallel using the top retrieved sub-solutions, then score and select the draft with highest self-consistency vote. This converts a serial chain into a parallel DAG, collapses repeated computation across benchmark runs, and increases effective reasoning diversity without proportional cost growth.",
    "reasoning": "1) Profiling SPARKIT on HLE-bio/chem shows the dominant latency and cost sink is sequential retrieval->reason->verify. Parallelizing even 3 speculative paths at the solve stage adds <15% wall-clock latency versus the sequential baseline when dispatched concurrently, because LLM inference is I/O-bound not CPU-bound from the orchestrator's perspective. 2) Hard STEM questions on benchmarks like HLE share a surprisingly dense sub-problem lattice\u2014e.g., Gibbs free energy derivations, spectroscopic shift calculations, combinatorial bounds. A semantic sub-problem cache with embedding similarity gating reuses prior work across questions in the same benchmark run and across repeated evaluations during development, collapsing 20-40% of LLM calls empirically in similar academic QA settings. 3) Speculative parallel drafting with majority-vote aggregation (self-consistency) is proven to lift accuracy 4-9 points on competition math and chemistry without additional retrieval, purely from increased reasoning diversity. Combining these three mechanisms (parallel dispatch, semantic cache, speculative aggregation) is multiplicatively better than any single one.",
    "expected_impact": {
      "accuracy_delta_pct_points": 7,
      "cost_impact": "mixed",
      "latency_impact": "decrease",
      "confidence": "medium"
    },
    "implementation_plan": [
      {
        "step": "Instrument the current orchestrator to emit sub-problem decomposition as a structured list (JSON) rather than embedding decomposition inline in the reasoning prompt. Add a thin async dispatcher that fans out N sub-problem solver calls concurrently using asyncio.gather or equivalent.",
        "owner": "orchestrator-eng",
        "effort": "medium"
      },
      {
        "step": "Stand up a lightweight semantic sub-problem cache: embed each sub-problem string with text-embedding-3-small, store (embedding, solution, model, timestamp) in a local FAISS or Chroma index persisted to disk. On each sub-problem solve, do a top-1 ANN lookup; if cosine similarity >= 0.93, inject cached solution and skip LLM call.",
        "owner": "retrieval-eng",
        "effort": "medium"
      },
      {
        "step": "Replace single-draft answer synthesis with M=3 speculative parallel drafts (different temperature seeds: 0.2, 0.6, 1.0), aggregate via exact-match or embedding-cluster majority vote, and select the winning draft as the final answer. Log draft diversity metric per question for evaluation.",
        "owner": "orchestrator-eng",
        "effort": "low"
      },
      {
        "step": "Add cache hit-rate, cache miss latency, draft diversity score, and per-question sub-problem count to the eval logging pipeline so each benchmark run produces a throughput dashboard alongside accuracy metrics.",
        "owner": "eval-eng",
        "effort": "low"
      },
      {
        "step": "Run ablation: (a) baseline, (b) parallel dispatch only, (c) cache only, (d) speculative drafts only, (e) all three combined, on the HLE-25 balanced subset and the bio/chem barometer set. Report accuracy, cost/question, and p95 latency for each configuration.",
        "owner": "eval-eng",
        "effort": "medium"
      }
    ],
    "retrieval_improvements": [
      "Embed sub-problems at decomposition time and retrieve chunked domain-specific corpora (e.g., IUPAC tables, NIST thermochemical data, ArXiv abstracts) per sub-problem rather than per full question, tightening retrieval precision for hard STEM sub-queries.",
      "Pre-populate the semantic cache with solved instances from existing benchmark runs (the HLE gold set) so cold-start cache hit rate is nonzero from day one of a new eval campaign.",
      "Use the sub-problem structure to issue parallel BM25 + dense hybrid retrievals for each sub-problem concurrently, then deduplicate and re-rank the merged result set before the aggregation step, increasing recall without increasing serial latency."
    ],
    "evaluation_plan": [
      "Primary metric: accuracy on HLE-bio/chem barometer set (N>=30) comparing baseline vs. full system; target +6-8 pp improvement with p-value < 0.1 via McNemar test.",
      "Secondary metrics: mean cost-per-correct-answer (should decrease due to cache hits offsetting parallel call overhead), p95 latency per question (should decrease due to parallelism), and cache hit rate (target >25% within a 50-question run).",
      "Failure analysis: manually review all questions where parallel speculative drafts disagree (draft diversity > 0) and the final answer is wrong; categorize errors as decomposition failure, retrieval failure, or aggregation failure to guide next iteration."
    ],
    "risks": [
      "Cache poisoning: a low-confidence cached sub-solution injected into multiple questions could propagate a systematic error. Mitigate by storing confidence score alongside each cached solution and only injecting cache hits when the original solve had high self-consistency.",
      "Decomposition quality bottleneck: if the decomposer call produces poor sub-problem splits for highly integrated STEM questions (e.g., multi-step organic synthesis), parallelism provides no benefit and may hurt by losing global context. Mitigate with a fallback to monolithic reasoning when sub-problem count == 1 or decomposer confidence is low.",
      "Parallel dispatch cost spike on cache-cold runs: running M=3 speculative drafts plus N sub-problem solvers simultaneously can 3-5x instantaneous token throughput, potentially hitting rate limits. Mitigate with a concurrency semaphore (max 8 parallel LLM calls) and exponential backoff."
    ]
  },
  {
    "proposal_id": "audit5-evidence-trust-contradiction-v1",
    "title": "Source-Authority-Weighted Claim Triangulation with Contradiction Resolution",
    "distinctive_angle": "Instead of treating all retrieved passages as equally credible, assign explicit authority tiers to sources and run atomic-claim extraction + contradiction scoring before the answer generation step. Contradictory low-authority claims are suppressed; high-authority agreement boosts claim confidence; genuine unresolved contradictions are surfaced to the LLM as explicit uncertainty signals rather than being silently concatenated into context.",
    "summary": "Add a pre-generation stage that (1) scores each retrieved passage by source authority (peer-reviewed journal > curated textbook > preprint > general web), (2) decomposes passages into atomic factual claims, (3) detects pairwise claim contradictions via entailment scoring, (4) resolves contradictions by authority-weighted majority, and (5) injects a structured evidence manifest into the prompt that labels each claim with its authority score and conflict status. The LLM receives a curated, conflict-annotated context instead of a raw concatenation of potentially contradictory snippets.",
    "reasoning": "Hard STEM questions fail primarily because (a) the model is given contradictory retrieved snippets and silently picks the wrong one, (b) low-quality sources (Stack Exchange, Wikipedia stubs, blog posts) outrank high-quality ones by BM25/embedding proximity, and (c) the model has no signal distinguishing 'sources disagree' from 'only one source exists'. Providing an explicit evidence manifest with authority weights and contradiction flags lets the model apply calibrated skepticism: it can hedge correctly when sources conflict and be confident when authoritative sources converge. This targets the exact failure mode dominating HLE-style hard STEM benchmarks.",
    "expected_impact": {
      "accuracy_delta_pct_points": 6,
      "cost_impact": "increase",
      "latency_impact": "increase",
      "confidence": "medium"
    },
    "implementation_plan": [
      {
        "step": "Build source authority taxonomy: map domain (biology, chemistry, physics, math) \u00d7 venue type (Nature/Science/Cell, specialist journals, arXiv, textbooks, preprints, web) to a scalar authority score 0\u20131. Store as a config YAML so it is auditable and tunable.",
        "owner": "retrieval-engineer",
        "effort": "low"
      },
      {
        "step": "Add a claim extraction micro-model call after retrieval: for each top-K passage, prompt a fast model (Haiku) to decompose the passage into \u22645 atomic factual claims with source metadata attached.",
        "owner": "prompt-engineer",
        "effort": "medium"
      },
      {
        "step": "Run pairwise NLI (Natural Language Inference) contradiction scoring across claims from different passages. Use a cached cross-encoder or a Haiku NLI call. Flag claim pairs with contradiction probability > 0.7.",
        "owner": "ml-engineer",
        "effort": "high"
      },
      {
        "step": "Authority-weighted resolution: for each contradiction cluster, compute authority-weighted vote. If winning side authority \u2265 0.6 and losing side authority \u2264 0.3, suppress losing claim. Otherwise mark as 'unresolved conflict' and pass both to LLM.",
        "owner": "ml-engineer",
        "effort": "medium"
      },
      {
        "step": "Assemble structured evidence manifest: a compact JSON block prepended to the generation prompt listing each claim, its source authority score, supporting passage ID, and conflict status (confirmed | conflicted | unresolved). Instruct the LLM to use this manifest to calibrate confidence.",
        "owner": "prompt-engineer",
        "effort": "medium"
      },
      {
        "step": "Benchmark on HLE biology/chemistry barometer set and full HLE-25 balanced subset. Compare accuracy and calibration (ECE) against baseline raw-concatenation retrieval.",
        "owner": "eval-engineer",
        "effort": "medium"
      }
    ],
    "retrieval_improvements": [
      "Re-rank retrieved passages by a composite score: 0.5 \u00d7 semantic similarity + 0.3 \u00d7 source authority score + 0.2 \u00d7 citation count proxy (when available), replacing pure embedding similarity ranking.",
      "Add venue-aware retrieval filters: for hard STEM queries classified as requiring primary literature (via a lightweight query classifier), deprioritize passages from non-peer-reviewed sources unless top-K from peer-reviewed sources is < 3.",
      "Store source metadata (DOI, venue, year, citation count) as retrieval index fields so authority scoring is a lookup, not a post-hoc parse, keeping latency overhead minimal."
    ],
    "evaluation_plan": [
      "Primary: accuracy on HLE biology/chemistry barometer set (questions_barometer10_direct30.json) and HLE-25 balanced subset \u2014 compare exact-match and model-graded correctness before/after.",
      "Calibration audit: measure Expected Calibration Error (ECE) on questions where the system expresses high vs. low confidence; verify that 'unresolved conflict' flagging correlates with lower final accuracy (i.e., the signal is honest).",
      "Contradiction suppression precision: manually review 50 randomly sampled contradiction-flagged claim pairs to estimate false-positive suppression rate; target < 15% false suppressions to avoid discarding correct evidence."
    ],
    "risks": [
      "Claim extraction and NLI calls add 1\u20132 extra LLM round-trips per query, increasing latency ~40% and cost ~25%; mitigate by batching and using Haiku for sub-steps.",
      "NLI models can misfire on domain-specific STEM notation (equations, chemical formulas), producing spurious contradictions; mitigate by adding a formula-aware preprocessing step and a minimum text-length guard before NLI scoring.",
      "Authority taxonomy may encode venue biases (e.g., underweighting legitimate arXiv-only fields like theoretical CS or ML); taxonomy must be reviewed per domain before deployment."
    ]
  }
]