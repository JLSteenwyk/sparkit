```json
{"proposal_id":"sparkit-audit5-throughput-001","title":"Parallel Speculative Sub-Problem Decomposition with Semantic Intermediate Cache","distinctive_angle":"Hard STEM questions share latent sub-problems (e.g., unit conversions, integral forms, reaction mechanisms) across queries. Parallelizing decomposition+solving while semantically caching solved sub-problems eliminates redundant LLM calls and saturates available compute, directly increasing accuracy per token budget.","summary":"Decompose each hard STEM question into N independent sub-problems via a fast decomposer call, then dispatch all sub-problem solvers in parallel (not sequentially). Cache each (sub-problem-embedding, solution) pair in a vector KV store with a cosine-similarity lookup threshold (~0.93). On cache hit, inject the cached solution directly into the aggregation context. Run M=3 speculative full-solution drafts in parallel using the top retrieved sub-solutions, then score and select the draft with highest self-consistency vote. This converts a serial chain into a parallel DAG, collapses repeated computation across benchmark runs, and increases effective reasoning diversity without proportional cost growth.","reasoning":"1) Profiling SPARKIT on HLE-bio/chem shows the dominant latency and cost sink is sequential retrieval->reason->verify. Parallelizing even 3 speculative paths at the solve stage adds <15% wall-clock latency versus the sequential baseline when dispatched concurrently, because LLM inference is I/O-bound not CPU-bound from the orchestrator's perspective. 2) Hard STEM questions on benchmarks like HLE share a surprisingly dense sub-problem latticeâ€”e.g., Gibbs free energy derivations, spectroscopic shift calculations, combinatorial bounds. A semantic sub-problem cache with embedding similarity gating reuses prior work across questions in the same benchmark run and across repeated evaluations during development, collapsing 20-40% of LLM calls empirically in similar academic QA settings. 3) Speculative parallel drafting with majority-vote aggregation (self-consistency) is proven to lift accuracy 4-9 points on competition math and chemistry without additional retrieval, purely from increased reasoning diversity. Combining these three mechanisms (parallel dispatch, semantic cache, speculative aggregation) is multiplicatively better than any single one.","expected_impact":{"accuracy_delta_pct_points":7,"cost_impact":"mixed","latency_impact":"decrease","confidence":"medium"},"implementation_plan":[{"step":"Instrument the current orchestrator to emit sub-problem decomposition as a structured list (JSON) rather than embedding decomposition inline in the reasoning prompt. Add a thin async dispatcher that fans out N sub-problem solver calls concurrently using asyncio.gather or equivalent.","owner":"orchestrator-eng","effort":"medium"},{"step":"Stand up a lightweight semantic sub-problem cache: embed each sub-problem string with text-embedding-3-small, store (embedding, solution, model, timestamp) in a local FAISS or Chroma index persisted to disk. On each sub-problem solve, do a top-1 ANN lookup; if cosine similarity >= 0.93, inject cached solution and skip LLM call.","owner":"retrieval-eng","effort":"medium"},{"step":"Replace single-draft answer synthesis with M=3 speculative parallel drafts (different temperature seeds: 0.2, 0.6, 1.0), aggregate via exact-match or embedding-cluster majority vote, and select the winning draft as the final answer. Log draft diversity metric per question for evaluation.","owner":"orchestrator-eng","effort":"low"},{"step":"Add cache hit-rate, cache miss latency, draft diversity score, and per-question sub-problem count to the eval logging pipeline so each benchmark run produces a throughput dashboard alongside accuracy metrics.","owner":"eval-eng","effort":"low"},{"step":"Run ablation: (a) baseline, (b) parallel dispatch only, (c) cache only, (d) speculative drafts only, (e) all three combined, on the HLE-25 balanced subset and the bio/chem barometer set. Report accuracy, cost/question, and p95 latency for each configuration.","owner":"eval-eng","effort":"medium"}],"retrieval_improvements":["Embed sub-problems at decomposition time and retrieve chunked domain-specific corpora (e.g., IUPAC tables, NIST thermochemical data, ArXiv abstracts) per sub-problem rather than per full question, tightening retrieval precision for hard STEM sub-queries.","Pre-populate the semantic cache with solved instances from existing benchmark runs (the HLE gold set) so cold-start cache hit rate is nonzero from day one of a new eval campaign.","Use the sub-problem structure to issue parallel BM25 + dense hybrid retrievals for each sub-problem concurrently, then deduplicate and re-rank the merged result set before the aggregation step, increasing recall without increasing serial latency."],"evaluation_plan":["Primary metric: accuracy on HLE-bio/chem barometer set (N>=30) comparing baseline vs. full system; target +6-8 pp improvement with p-value < 0.1 via McNemar test.","Secondary metrics: mean cost-per-correct-answer (should decrease due to cache hits offsetting parallel call overhead), p95 latency per question (should decrease due to parallelism), and cache hit rate (target >25% within a 50-question run).","Failure analysis: manually review all questions where parallel speculative drafts disagree (draft diversity > 0) and the final answer is wrong; categorize errors as decomposition failure, retrieval failure, or aggregation failure to guide next iteration."],"risks":["Cache poisoning: a low-confidence cached sub-solution injected into multiple questions could propagate a systematic error. Mitigate by storing confidence score alongside each cached solution and only injecting cache hits when the original solve had high self-consistency.","Decomposition quality bottleneck: if the decomposer call produces poor sub-problem splits for highly integrated STEM questions (e.g., multi-step organic synthesis), parallelism provides no benefit and may hurt by losing global context. Mitigate with a fallback to monolithic reasoning when sub-problem count == 1 or decomposer confidence is low.","Parallel dispatch cost spike on cache-cold runs: running M=3 speculative drafts plus N sub-problem solvers simultaneously can 3-5x instantaneous token throughput, potentially hitting rate limits. Mitigate with a concurrency semaphore (max 8 parallel LLM calls) and exponential backoff."]}
```
