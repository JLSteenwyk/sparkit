```json
{
  "proposal_id": "SPARKIT-RET-001",
  "title": "Retrograde Evidence Triangulation: Answer-Hypothesis-Seeded Retrieval with Fingerprint Coverage Scoring",
  "distinctive_angle": "Inverts the standard retrieve-then-reason pipeline by generating speculative answer hypotheses before full retrieval, extracting per-hypothesis evidence fingerprints (atomic fact chains needed to validate each hypothesis), using those fingerprints as targeted retrieval seeds, and scoring document utility by fingerprint coverage rather than topic overlap alone. This forces retrieval to be answer-discriminating rather than merely topic-relevant — a critical distinction for hard multi-hop and domain-specific QA where the correct answer hinges on narrow fact chains that broad queries miss entirely.",
  "summary": "Add a pre-retrieval speculative solver stage that (1) generates K candidate answer hypotheses via a cheap LLM call routed to the lowest-cost provider available (DeepSeek or Kimi), (2) extracts 3-5 atomic evidence fingerprints per hypothesis — the chain of domain assertions that would need to be evidenced to confirm that hypothesis, (3) injects fingerprint-derived queries into SPARKIT's retrieval plan as a new 'fingerprint_queries' intent type alongside existing primary/methods/adversarial/reference/options intents in _build_retrieval_plan, (4) supplements _record_relevance_score with a 0.35-weighted fingerprint coverage term to prefer documents satisfying multiple fingerprints over merely topically-relevant documents, (5) injects a HYPOTHESIS EVIDENCE COVERAGE table into _build_synthesis_prompt giving the synthesis LLM a structured reasoning scaffold anchored to which hypothesis has the strongest retrieved evidence trail. Gated by SPARKIT_ENABLE_RET env var (default 0 until A/B validated).",
  "reasoning": "SPARKIT's retrieval queries are derived from question decomposition (sub_claims, focus_terms, intent_queries) which are effective at finding topically-relevant documents but systematically miss the specific fact chains needed to adjudicate between close answer candidates on hard questions. For HLE-style questions in biochemistry, ML, and mathematics, the correct answer often hinges on a narrow domain fact — a specific reaction mechanism, a dataset characteristic, an algorithm invariant — that topic-level queries rarely surface because those facts are buried in subsections rather than titles/abstracts. By generating answer hypotheses first and deriving evidence fingerprints from them, retrieval queries become answer-discriminating. The current _record_relevance_score (2.0*title_overlap + 1.0*abstract_overlap + 0.25*recency) cannot distinguish between a document that discusses a topic broadly versus one that contains the exact fact chain needed — fingerprint coverage scoring solves this. Documents covering fingerprints from multiple competing hypotheses act as triangulation points, providing evidence about which hypothesis is most strongly supported. Finally, passing hypothesis-coverage tables into the synthesis prompt gives the LLM a structured entailment scaffold that reduces hallucination and improves option discrimination for MCQ without increasing synthesis token length materially. The MCQ option_hypothesis queries already in SPARKIT are the nearest prior art but they are shallow (option text appended to question) — fingerprints are deeper (derived from what would need to be true) and multi-step.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _generate_speculative_hypotheses(question, sub_claims, provider_client, n=3) in engine.py. Issues a single cheap LLM call (routed to SPARKIT_RET_HYPOTHESIS_PROVIDER, defaulting to first of [deepseek, kimi, mistral] that is configured) with a short structured prompt requesting N plausible candidate answers with one-sentence justifications. Returns List[{hypothesis: str, justification: str}]. If the call fails or parses < 1 hypothesis, the function returns an empty list and the rest of the pipeline skips RET silently — zero regression risk. Add SPARKIT_RET_N_HYPOTHESES (default 3) and SPARKIT_RET_HYPOTHESIS_PROVIDER (default 'cheapest') env vars.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _extract_evidence_fingerprints(hypotheses, question, provider_client) in engine.py. For the list of hypotheses, issues a single batched LLM call (same cheap provider) returning for each hypothesis 3-5 atomic fact assertions that would need to be present in evidence to confirm it (e.g., 'compound X inhibits kinase Y at nanomolar concentrations', 'algorithm Z achieves O(n log n) in the worst case'). Parses JSON output with regex fallback. Returns Dict[str, List[str]] mapping hypothesis text to fingerprint list. Add SPARKIT_RET_FINGERPRINTS_PER_HYPOTHESIS (default 4) env var.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify _build_retrieval_plan / _decompose_retrieval in engine.py to add a 'fingerprint_queries' entry to the RetrievalPlan intent_queries dict when SPARKIT_ENABLE_RET=1 and fingerprints is non-empty. Each fingerprint string (possibly combined with top 2 focus_terms for precision) becomes a retrieval query. Cap at SPARKIT_RET_MAX_FINGERPRINT_QUERIES (default 6) to avoid over-expanding retrieval budget. The fingerprint round maps to a new retrieval_round_fingerprint stage executed after retrieval_round_1 and before retrieval_round_2_gap_fill.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _score_fingerprint_coverage(record, all_fingerprints) helper in engine.py. Tokenizes the record's title + abstract snippet using the existing _tokenize helper. For each fingerprint string, checks if >= 2 fingerprint tokens appear in the record tokens. Returns coverage = matched_fingerprints / total_fingerprints in [0, 1]. Modify _record_relevance_score to return: (existing_score * 0.65) + (fingerprint_coverage * 0.35) when SPARKIT_ENABLE_RET=1, otherwise unchanged. This requires passing fingerprints as an optional arg to _record_relevance_score and the selection loop in execute_orchestration.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "low"
    },
    {
      "step": "Modify _build_synthesis_prompt in engine.py to prepend a compact HYPOTHESIS EVIDENCE COVERAGE section when SPARKIT_ENABLE_RET=1 and hypotheses is non-empty. Format: for each hypothesis show (a) the hypothesis text, (b) count of fingerprints found in retrieved evidence vs total, (c) top-1 supporting passage per found fingerprint. Keep under 400 tokens total by truncating passages to 60 chars. Instruct the synthesis LLM to use this table as a primary reasoning anchor before consulting the evidence list.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "For MCQ questions, add fingerprint coverage score per option in the MCQ scoring path. In the option scoring logic, retrieve fingerprint coverage for each option's hypothesis from the fingerprint_coverage dict. Modify the blend formula from current (0.7*llm_net + 0.3*lexical) to (0.55*llm_net + 0.25*lexical + 0.20*fingerprint_coverage) when SPARKIT_ENABLE_RET=1. Add SPARKIT_MCQ_FINGERPRINT_WEIGHT (default 0.20) env var to make this tunable. Fall back to original blend if fingerprint_coverage dict is empty.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add unit tests in test_synthesis_quality.py: (a) hypothesis generation returns correct schema and gracefully returns [] on empty LLM response, (b) fingerprint extraction returns >= 1 fingerprint per hypothesis for a sample question, (c) _score_fingerprint_coverage returns higher score for a record whose abstract matches more fingerprints, (d) retrieval plan includes 'fingerprint_queries' key when SPARKIT_ENABLE_RET=1 and hypotheses are present, (e) synthesis prompt contains 'HYPOTHESIS EVIDENCE COVERAGE' header when enabled and fingerprints non-empty, (f) MCQ blend formula uses fingerprint weight when configured.",
      "owner": "services/orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    },
    {
      "step": "Run A/B benchmark on hle-biochem-20 and hle-25-balanced subsets (already in benchmarks/): SPARKIT_ENABLE_RET=0 control vs SPARKIT_ENABLE_RET=1 treatment, identical provider routing, same random seed. Collect per-question accuracy, ret_stage_cost_usd (add to trace artifacts), total latency, fingerprint_hit_rate (fraction of correct-answer fingerprints found in retrieved docs). If accuracy delta >= 4pp and cost overhead < 20% of baseline, promote SPARKIT_ENABLE_RET to default 1 and add to docs/configuration.md.",
      "owner": "benchmarks/ + docs/",
      "effort": "high"
    }
  ],
  "retrieval_improvements": [
    "New 'fingerprint_queries' retrieval intent injected into _build_retrieval_plan: each atomic evidence fingerprint (a precise domain fact assertion derived from a speculative answer hypothesis) becomes a standalone retrieval query, replacing generic question-decomposition sub-claims with targeted fact-chain queries. These surface documents that contain the specific assertions needed to adjudicate between competing answers — documents that would never appear in topic-level query results because the discriminating fact is buried in results or methods sections, not titles or abstracts.",
    "Fingerprint-coverage-weighted document selection in _record_relevance_score: blends existing lexical overlap scoring (kept at 65% weight) with a new fingerprint coverage term (35% weight) computed as the fraction of all hypothesis fingerprints present in a document's title+abstract. Documents that simultaneously satisfy fingerprints from multiple competing hypotheses — triangulation point documents — rank highest, replacing the current binary relevant/not-relevant judgment with a continuous answer-discriminating utility score.",
    "Hypothesis-diverse adversarial retrieval amplification: fingerprints from competing hypotheses naturally generate contradictory retrieval queries (e.g., hypothesis A fingerprint 'compound X activates receptor Y' vs hypothesis B fingerprint 'compound X blocks receptor Y'), which organically surfaces both supporting and refuting documents for each answer candidate. This expands adversarial round coverage without adding new round types — the fingerprint round subsumes and improves what retrieval_round_3_adversarial attempts to do with generic negative-result queries.",
    "Fingerprint-driven second-pass document diversity: in the second-pass document slot filling (fill remaining ingestion slots by relevance after source-diversity first pass), prefer documents covering underrepresented fingerprints — i.e., fingerprints with zero current coverage in already-selected documents. This ensures the ingested evidence set spans the full hypothesis space rather than clustering around the single most-retrievable topic region, which is the current failure mode for hard questions with multiple plausible answer paths.",
    "Dynamic retrieval depth escalation triggered by hypothesis uncertainty: if the speculative solver returns multiple hypotheses with similar justification confidence (assessed by justification length and specificity heuristics), automatically increase SPARKIT_INGESTION_TARGET_DOCS_FLOOR by 3 for that run to ensure sufficient evidence density for multi-hypothesis triangulation. This provides adaptive depth tuning tied to question difficulty without modifying global config or requiring manual tuning."
  ],
  "evaluation_plan": [
    "Accuracy delta on hle-biochem-20 and hle-25-balanced: matched-pair comparison (same question, same provider routing, SPARKIT_ENABLE_RET=0 vs =1). Compute per-question correct/incorrect, McNemar's test for statistical significance (relaxed to p<0.15 given small N). Log per-question: hypothesis_count, fingerprint_count_total, fingerprint_hit_rate (fraction of correct-hypothesis fingerprints found in retrieved docs), and whether the top-coverage hypothesis matched the correct answer. Accept proposal if accuracy improves >= 4pp.",
    "Fingerprint recall audit (manual): for 10 hard HLE questions where SPARKIT currently answers incorrectly, manually examine the top-10 retrieved documents under RET=0 vs RET=1. Count how many documents under RET=1 contain a passage directly supporting the correct answer's fingerprints vs RET=0. Track as fingerprint_recall_lift = (RET=1 hit docs) / (RET=0 hit docs). Target: >= 1.4x lift (40% more answer-relevant documents retrieved).",
    "Document selection rank correlation: for a held-out set of 20 questions with known correct answers, compute Kendall's tau between document selection rank and a manually-labeled 'answer-relevant' binary label (does this document contain a fact that directly supports or refutes the correct answer). Compare tau under RET=0 (baseline _record_relevance_score) vs RET=1 (fingerprint-blended score). Improvement in tau validates that fingerprint coverage scoring is selecting more useful documents.",
    "Hypothesis-answer alignment rate: for all MCQ questions where SPARKIT answers correctly under RET=1, verify that the hypothesis with the highest fingerprint coverage matches the correct MCQ option >= 65% of the time. If this alignment rate is below 50%, the speculative solver is generating misleading hypotheses and the SPARKIT_RET_HYPOTHESIS_PROVIDER selection needs adjustment (likely switch from cheapest provider to a stronger one for this stage).",
    "Cost overhead regression: measure total tokens and cost_usd for hypothesis generation + fingerprint extraction LLM calls across a 20-question benchmark run. Assert that added RET stage cost is <= 18% of baseline total run cost. Add 'ret_stage_cost_usd' and 'ret_stage_latency_s' keys to the run trace artifact (alongside existing provider_usage entries) so cost attribution is transparent in every benchmark result.",
    "Latency regression gate: measure median orchestration latency over 20 questions with SPARKIT_ENABLE_RET=1. The hypothesis and fingerprint extraction calls run sequentially before retrieval planning, adding approximately 2-4 seconds on cheap providers. Assert added median latency < 8s. If exceeded, implement parallel fingerprint extraction across hypotheses using asyncio.gather() (the extraction prompt is stateless per hypothesis, making parallelization trivial) and re-benchmark."
  ],
  "risks": [
    "Speculative hypothesis quality degrades on highly specialized domain questions (niche organic chemistry mechanisms, obscure mathematical theorems) where cheap LLM providers (DeepSeek, Kimi) lack the domain knowledge to generate plausible hypotheses, resulting in low-quality fingerprints that inject noise into retrieval queries. Mitigation: if parsed hypothesis count < 1 or all justifications are shorter than 15 tokens (indicating the LLM punted), skip RET stage entirely and fall back to current pipeline with no degradation. Log 'ret_skipped_reason: low_quality_hypotheses' in trace.",
    "Confirmation bias amplification: fingerprint-targeted retrieval preferentially surfaces documents supporting the speculative solver's prior, potentially over-weighting the wrong hypothesis if the cheap provider's prior is incorrect for a hard question. Mitigation: (a) fingerprint coverage term is capped at 35% of document selection weight, never dominant; (b) standard primary, gap-fill, and adversarial retrieval rounds run at full weight unchanged; (c) synthesis prompt explicitly instructs the LLM that the coverage table is a weak prior and retrieved evidence may contradict it.",
    "Two additional LLM calls per run (hypothesis generation and fingerprint extraction) add both cost and latency. For budget-constrained single-question API runs or latency-sensitive deployments this overhead may be unacceptable. Mitigation: SPARKIT_ENABLE_RET=0 default keeps the feature off until validated; add SPARKIT_RET_MAX_COST_USD (default 0.03) budget guard that skips RET if remaining run budget is insufficient; batch both calls into one prompt to halve the added latency.",
    "Fingerprint extraction prompt is sensitive to output format compliance. If the LLM provider returns free-form prose instead of a parseable JSON list of fingerprint strings, the extraction silently fails. Mitigation: use strict JSON schema output format in the extraction prompt (compatible with OpenAI-format providers); add a regex fallback parser that extracts quoted strings or bullet-point lines; if < 2 total fingerprints are parsed across all hypotheses, skip fingerprint scoring for that run and log 'ret_skipped_reason: parse_failure'.",
    "MCQ fingerprint coverage blending (0.20 weight shift away from llm_net) may hurt performance on questions where the speculative solver generates plausible but incorrect hypotheses and the fingerprint queries amplify evidence for wrong options. Mitigation: SPARKIT_MCQ_FINGERPRINT_WEIGHT is tunable (default 0.20); track per-question MCQ accuracy separately from free-text accuracy in benchmark results to isolate the effect; if MCQ accuracy regresses >= 3pp while free-text improves, reduce weight to 0.10 or disable MCQ fingerprint blending independently via SPARKIT_MCQ_FINGERPRINT_WEIGHT=0."
  ]
}
```
