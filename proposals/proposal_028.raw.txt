{"proposal_id":"HDRR-SES-001","title":"Hypothesis-Driven Retroactive Retrieval with Semantic Evidence Scoring","distinctive_angle":"Inverts SPARKIT's retrieve-then-synthesize paradigm by generating N=4 candidate answer hypotheses after round-1 retrieval, then replacing the generic gap-fill and adversarial rounds with precision hypothesis-falsification searches — each hypothesis gets 2-3 targeted queries designed to surface confirming AND refuting evidence. Final answer selection uses semantic embedding cosine similarity instead of SPARKIT's current token-overlap ranking, grounding the decision in evidence that semantically entails the winning hypothesis rather than merely sharing keywords with the question.","summary":"After retrieval_round_1, the planning LLM generates N=4 plausible candidate answers from initial evidence. For each candidate, the retrieval-role LLM generates 2-3 targeted queries specifically designed to confirm or falsify it. These replace retrieval_round_option_hypotheses and retrieval_round_2_gap_fill with hypothesis-targeted rounds, while retrieval_round_3_adversarial is augmented with citation graph traversal (fetching top-5 cited references of the highest-ranked round-1 papers via existing Semantic Scholar and OpenAlex adapters). All retrieved passages are scored against each hypothesis using embedding cosine similarity via a lightweight embedding API call, replacing token-overlap as the re-ranking signal for the top-30 candidate passages. The hypothesis with the highest net evidence support wins and its top passages become the synthesis context. Quantitative claims (numbers, percentages, p-values) extracted via regex from ingested sections receive 1.5x scoring weight. For MCQs, this augments the current elimination and lexical scoring passes with hypothesis-level evidence strength, replacing 30% of the lexical blend with 30% semantic entailment score.","reasoning":"SPARKIT's current retrieval is query-term-driven: it searches for documents containing words from the question or its focus terms. On HLE-level hard STEM questions, the critical distinguishing evidence is typically in papers that use different terminology than the question itself — e.g., a question about 'transfer efficiency' may be answered by a paper discussing 'coupling yield' — or in primary experimental sources cited by initially retrieved review papers, not in the review papers themselves. By generating candidate answers first, SPARKIT can issue search queries for the specific numerical thresholds, experimental conditions, or methodological details that distinguish between plausible answers, producing far higher-precision retrieval signal than generic keyword search. The token-overlap weakness is especially damaging for hard questions: the existing aggregator.py relevance score gives equal credit to 'achieves 94.2% accuracy' and 'improves substantially over prior work' if they share the same question tokens, yet only the former is useful for a quantitative comparison question. Semantic embedding scoring resolves this by capturing meaning rather than surface form. The citation graph traversal directly addresses the secondary failure mode: hard STEM QA often requires tracing a cited claim back to the primary experimental paper that measured it, and SPARKIT's current retrieval never goes beyond the initial search surface. Together these three changes — hypothesis targeting, semantic scoring, and citation traversal — attack the two most impactful failure modes on hard QA: retrieving the wrong documents and selecting the wrong answer from correctly retrieved documents.","expected_impact":{"accuracy_delta_pct_points":6,"cost_impact":"mixed","latency_impact":"increase","confidence":"medium"},"implementation_plan":[{"step":"Add hypothesis_generator() in engine.py at the end of the retrieval_round_1 stage: prompt the planning-role LLM with question + round-1 evidence to produce exactly N=4 candidate answers as a structured list. For MCQs, generate one paraphrased justification per answer choice rather than raw option text, to prevent lexical leakage into retrieval. Always include the most-cited claim from round-1 evidence as one of the N=4 to ensure at least one evidence-grounded hypothesis.","owner":"engine.py:orchestrate_question","effort":"medium"},{"step":"Add hypothesis_query_builder() in engine.py _build_retrieval_plan(): for each of the N=4 hypotheses, call the retrieval-role LLM to generate 2-3 targeted queries with explicit intents ('confirm' or 'falsify'). Cap total hypothesis queries at 10 to bound Brave Search cost. Route queries through arXiv, Semantic Scholar, and OpenAlex first; only escalate to Brave Search for queries returning zero academic results.","owner":"engine.py:_build_retrieval_plan","effort":"medium"},{"step":"Replace retrieval_round_option_hypotheses and retrieval_round_2_gap_fill logic in engine.py with hypothesis-targeted rounds. Tag each retrieved document with its originating hypothesis_idx so downstream scoring can attribute evidence to the hypothesis that surfaced it. Preserve round-level adaptive gating logic unchanged.","owner":"engine.py:_execute_retrieval_round","effort":"medium"},{"step":"Add citation graph traversal in retrieval_round_3_adversarial: after ranking round-1+2 documents, identify top-3 by relevance score that have a Semantic Scholar paper_id or OpenAlex work_id. Fetch their top-5 cited references using the existing S2 and OpenAlex adapter fetch methods. Add retrieved references to the candidate pool with source label 'citation_traversal'. Apply existing deduplication by DOI. This requires no new external API — S2 and OpenAlex adapters already exist in adapters.py.","owner":"services/retrieval_service/app/adapters.py + aggregator.py","effort":"medium"},{"step":"Create services/orchestrator/app/semantic_scorer.py implementing passage_embed_score(passages, hypotheses): call a lightweight embedding REST API (Voyage AI voyage-3-lite at $0.02/M tokens or Jina embeddings v3) to embed the top-30 passages and all N=4 hypotheses. Compute cosine similarity matrix [passages × hypotheses]. Cache embeddings by sha256(text) in a module-level LRU cache (maxsize=512) to avoid re-embedding repeated passages across rounds. Expose a synchronous token-overlap fallback that activates if the API call exceeds 3s or returns an error.","owner":"new file: services/orchestrator/app/semantic_scorer.py","effort":"high"},{"step":"Replace the final token-overlap re-ranking step in retrieval_service/app/aggregator.py with semantic cosine similarity. Token-overlap scoring remains for the initial candidate-selection pass (fast, no network call) on all N results; semantic scoring is applied only to the top-30 candidates. Add SPARKIT_ENABLE_SEMANTIC_SCORING env var (default 1) to allow disabling for cost-sensitive or offline runs.","owner":"services/retrieval_service/app/aggregator.py:_score_relevance","effort":"medium"},{"step":"Add hypothesis_evidence_aggregator() in engine.py after ingestion: for each hypothesis k, compute net_support[k] = sum(semantic_similarity_to_hypothesis_k × quantitative_weight) across all ingested passages, where quantitative_weight=1.5 if passage contains a numeric claim (regex match: digit+unit, percentage, p-value, confidence interval) else 1.0. Track confirm_queries_hit[k] and falsify_queries_hit[k] separately; penalize hypothesis k by 0.15 per falsify-targeted document that scores >0.6 similarity to it.","owner":"engine.py:_synthesize_answer","effort":"medium"},{"step":"Add quantitative_claim_extractor() in the ingestion block of engine.py: after _select_best_section(), run a regex pass over the extracted section text to identify numeric claims. Store as claim_metadata.quantitative_anchors list alongside the existing claim text. These anchors are later used by hypothesis_evidence_aggregator() for 1.5x weighting and by the MCQ lexical scorer as high-signal anchors.","owner":"engine.py:_ingest_documents","effort":"low"},{"step":"For MCQ, integrate hypothesis_evidence_aggregator() output into the existing MCQ blend formula in engine.py: replace the 30% lexical component with 30% semantic entailment score (hypothesis net_support normalized to [0,1]) and keep 70% LLM net score. This requires updating _blend_mcq_scores() to accept an optional semantic_scores dict.","owner":"engine.py:_blend_mcq_scores","effort":"low"},{"step":"Update calibration.py to add hypothesis_consensus_score feature: if exactly one hypothesis has net_support > 2x the second-best, add +0.08 to raw_confidence (reflects high evidence selectivity). Cap the feature contribution at +0.08 to prevent overconfidence. Reduce ensemble_agreement weight from 0.10 to 0.05 for single-provider runs where it carries no real signal, redirecting the 0.05 to hypothesis_consensus_score.","owner":"services/orchestrator/app/calibration.py","effort":"low"},{"step":"Update policy.py cost estimation: add embedding API cost line (voyage-3-lite: $0.02/M tokens; 30 passages × 512 tokens avg = ~$0.0003 per run). Add hypothesis_generator and hypothesis_query_builder LLM call estimates (~$0.01 per run for planning-tier model). These are negligible vs synthesis costs but should be tracked for exact cost reporting.","owner":"services/orchestrator/app/policy.py","effort":"low"},{"step":"Write tests in test_synthesis_quality.py: (a) hypothesis_generator output parse with 4-element list assertion, (b) hypothesis_evidence_aggregator correctness with synthetic passage-hypothesis similarity matrix, (c) semantic_scorer token-overlap fallback activation on mock API timeout, (d) citation traversal deduplication (traversal-fetched DOI already in round-1 results must not create duplicate), (e) quantitative_claim_extractor regex coverage across 10 numeric patterns.","owner":"services/orchestrator/tests/test_synthesis_quality.py","effort":"medium"}],"retrieval_improvements":["Hypothesis-targeted query generation replacing generic gap-fill: instead of 3-4 queries derived from question keywords, each of N=4 candidate answers generates 2-3 queries specifically designed to surface confirming or falsifying evidence. This produces 8-10 high-precision queries per run vs current 3-4 generic ones. For a question like 'Which catalyst achieves the highest turnover frequency for CO2 reduction?', a hypothesis-targeted query for 'copper-based catalyst' would be 'copper catalyst CO2 electroreduction turnover frequency benchmark comparison' rather than the generic question restatement — dramatically improving recall of the specific papers that measured and compared catalyst performance.","Citation graph traversal for primary-source evidence: after round-1 retrieval, use the already-integrated Semantic Scholar and OpenAlex adapters to fetch cited references of the top-3 retrieved papers. Hard STEM questions routinely hinge on specific experimental measurements that appear only in the primary source paper, not in the review or methods paper that initial keyword search surfaces. A question about a specific benchmark result may retrieve the benchmark paper but not the model paper that achieved the result — citation traversal closes this gap with zero new API dependencies.","Semantic embedding re-ranking of top-30 candidates replacing token-overlap final ranking: the current aggregator.py relevance score is a weighted token intersection of query tokens against title and abstract tokens. This fails whenever a relevant paper uses synonymous but non-overlapping terminology, which is common in interdisciplinary STEM questions. Voyage AI voyage-3-lite embeddings (trained on scientific literature) capture semantic equivalence across terminology differences, with Spearman rank correlation against human relevance judgments measurably higher than token-overlap for scientific passages.","Quantitative claim extraction with numeric anchor weighting: the current ingestion step extracts the first sentence of the best section as the claim, which is often a topic sentence with no quantitative content. A dedicated regex pass extracting numeric claims (values+units, percentages, p-values, effect sizes, confidence intervals) from the full section provides high-signal anchors for hypothesis scoring. These anchors receive 1.5x weight because hard STEM questions disproportionately hinge on specific numerical thresholds — the difference between 'significantly outperforms' and '94.2% vs 91.7%' is the difference between a wrong and correct answer on quantitative comparison questions."],"evaluation_plan":["A/B accuracy comparison on HLE-gold-50: run HDRR-SES vs current SPARKIT baseline on the identical 50 questions with the same provider configuration and $3.00 budget. Primary metric: exact-match accuracy on MCQ questions. Stratify results by question difficulty tertile (bottom 17 hardest questions by baseline accuracy) — the proposal is specifically designed for the hard tail and should show largest gains there. Minimum threshold for adoption: +3pp overall, +6pp on hardest tertile.","Hypothesis candidate set coverage audit: for 20 randomly sampled questions, manually inspect the N=4 generated hypotheses. Verify that the correct answer appears in the candidate set at least 90% of the time. If the correct answer is never generated as a hypothesis, HDRR-SES cannot win regardless of retrieval quality. Track candidate_set_coverage_rate as a mandatory health metric in the observability store; if it drops below 85% on a held-out validation set, revert to generic gap-fill retrieval.","Semantic scorer rank correlation validation: for 100 passage-question pairs with known relevance labels extracted from existing benchmark ground truth annotations, compute Spearman rank correlation between (a) token-overlap score and (b) embedding cosine similarity, each compared to human-labeled relevance. Adoption threshold: semantic scoring must achieve ρ ≥ 0.65 vs token-overlap ρ ≥ 0.45. Run this as a one-time offline evaluation before enabling SPARKIT_ENABLE_SEMANTIC_SCORING=1 in production.","Citation traversal yield and novelty analysis: instrument the traversal step to log for each run: (a) how many traversal-fetched papers were unique (not already in the candidate pool by DOI), (b) of those unique papers, how many were selected for ingestion, and (c) of ingested traversal papers, how many were in the top-8 evidence bullets used for synthesis. Target thresholds: ≥30% unique rate, ≥40% ingestion selection rate among unique papers, ≥15% contribution to final synthesis evidence. If traversal yield falls below these thresholds on 10 consecutive runs, the traversal step should be demoted to an optional mode.","Latency and cost budget compliance regression: add a benchmark regression test that runs HDRR-SES on 5 questions and asserts (a) total cost ≤ $3.00 for each run, (b) median wall-clock latency increase vs baseline ≤ 60 seconds, (c) semantic scoring API call completes within 3 seconds or triggers the token-overlap fallback. These checks must pass in CI before any merge to prevent budget violations from reaching production runs."],"risks":["Hypothesis quality dependency creates a single point of failure: if the planning LLM generates N=4 poor candidate answers (all wrong, or all paraphrases of the same wrong answer), the targeted retrieval will systematically reinforce that error with high precision. Mitigation: always include as one of the N=4 hypotheses the most-cited claim from round-1 evidence regardless of LLM output, ensuring at least one hypothesis is evidence-grounded. Also log whether the final winning hypothesis was LLM-generated or evidence-grounded for post-hoc analysis.","Embedding API introduces a new external dependency and failure mode: adding Voyage AI or Jina as a required API means a new key to manage, a new latency source, and a new failure surface. If the embedding API is down or slow, SPARKIT should not degrade silently. Mitigation: implement the token-overlap fallback as a synchronous code path in semantic_scorer.py activated automatically on timeout (3s) or HTTP error. Gate the entire semantic scoring subsystem behind SPARKIT_ENABLE_SEMANTIC_SCORING env var so it can be disabled instantly in operations.","Citation graph traversal may surface retracted, superseded, or low-quality primary sources: primary sources fetched via citation traversal may include older papers with findings later corrected or retracted. The existing recency bonus in relevance scoring would deprioritize these, but not eliminate them. Mitigation: apply a citation_traversal_min_year filter (default 2000) to traversal-fetched papers. For papers older than 2010, require ≥50 Semantic Scholar citations to be included, preventing obscure low-quality sources from entering the evidence pool.","Hypothesis-targeted retrieval may increase Brave Search cost beyond budget: N=4 hypotheses × 3 queries each = 12 additional queries. If many of these escalate to Brave Search (because arXiv/S2/OpenAlex return zero results), the per-query $0.005 cost could add $0.04-0.06 per run, which is significant for budget-constrained configurations. Mitigation: cap hypothesis-targeted Brave Search queries at 4 total per run (regardless of how many hypothesis queries fail academic lookup). Route all hypothesis queries through academic APIs first, with Brave Search as the last resort.","MCQ hypothesis paraphrasing may introduce semantic drift from the original answer choices: when paraphrasing answer choices into hypothesis queries, the LLM may generate paraphrases that are subtly different from what the choice actually means, leading to retrieval of misleadingly irrelevant evidence. For example, paraphrasing 'option C: stochastic gradient descent' as a hypothesis about 'gradient-based optimization' could surface papers about Adam or RMSprop instead. Mitigation: for MCQ, include the raw choice text verbatim as a required substring in the paraphrased hypothesis query, so the paraphrase always contains the exact option label as an anchor term."]}
