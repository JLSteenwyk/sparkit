Now I have deep enough context. Let me write the proposal.

```json
{
  "proposal_id": "SPARKIT-P001-FINGERPRINT-NUCLEATION-v1",
  "title": "Evidence Dimension Fingerprinting with Gap-Driven Retrieval and Multi-Nugget Per-Document Extraction",
  "distinctive_angle": "SPARKIT's adaptive gate halts retrieval based on document-count novelty (new_unique_docs < 2 AND quality_gain < 0.03) — a signal that is orthogonal to whether the question's specific factual requirements are actually met. Simultaneously, the ingestion pipeline discards the vast majority of each paper by extracting only the first sentence of one highest-scoring section (capped at 10 000 chars), meaning SPARKIT routinely has the correct answer present in its retrieved corpus but never surfaces it. This proposal replaces these two structural bottlenecks with: (1) a pre-retrieval Answer Dimension Fingerprint — an LLM-derived structured list of irreducible factual sub-questions whose joint answers uniquely determine the correct response — and (2) a per-document multi-nugget extractor that draws up to five evidence nuggets from distinct sections, scored against individual fingerprint dimensions rather than against the whole question. The adaptive gate becomes dimension-coverage-aware, stopping retrieval only when a target fraction of fingerprint dimensions are satisfied or documentary novelty is genuinely exhausted. For MCQ questions, the fingerprint dimensions are recast as option-discriminating evidence pairs, forcing retrieval to gather comparative rather than merely confirmatory evidence.",
  "summary": "SPARKIT executes four fixed intent rounds of retrieval (primary, options, methods, adversarial), then ingests each selected document by picking the single top-scoring section and extracting only its first sentence as the claim text. On hard questions this pipeline fails in three compounding ways: the fixed round schedule is agnostic to what the question actually needs; the first-sentence extraction discards quantitative results buried in later paragraphs; and the MCQ scoring pipeline independently rates each option against generic evidence rather than evidence specifically chosen to discriminate between competing options. The proposal adds: (A) _generate_answer_fingerprint() called immediately after _decompose_question(), producing N=4-8 atomic factual dimensions per question; (B) _compute_dimension_coverage() replacing the inner adaptive gate check, halting retrieval when ≥70% of dimensions have ≥1 supporting claim, not merely when document count stops growing; (C) _extract_multi_nuggets() replacing the single _select_best_section_chunk() call with up to five ranked section-chunk extracts per document, each scored against the nearest uncovered fingerprint dimension; (D) contrastive option-pair query generation injected into _decompose_retrieval() for MCQ task types, emitting one query per option pair (A vs B, A vs C, etc.) in addition to current per-option hypothesis queries; and (E) quantitative-claim anchoring — a lightweight regex pass over all extracted nuggets that identifies numeric assertions (values with units, p-values, percentages) and elevates them to a dedicated claim_type='quantitative' with 1.5x synthesis weight in the prompt builder.",
  "reasoning": "Hard benchmark questions (HLE-style) systematically require one or more of: exact numerical recall, multi-hop causal chains, discrimination between near-identical option wordings, and synthesis of findings scattered across methods and results sections. SPARKIT's current architecture addresses none of these reliably. The first-sentence claim extraction means that a paper reporting 'quantum yield = 0.42 under UV illumination at 365 nm' in its Results section will contribute only the abstract's introductory sentence to the evidence pool. The document-novelty adaptive gate allows the pipeline to stop after finding three papers whose abstracts are topically relevant but whose results sections were never read. MCQ scoring with 0.7*llm_net + 0.3*lexical rewards papers that mention an option's keywords, not papers that explicitly argue for or against an option relative to its competitors — exactly the comparative evidence that distinguishes correct from plausible distractors on hard exam questions. The fingerprinting mechanism addresses all three: by making individual factual dimensions explicit before retrieval begins, the system can (i) know when enough evidence has actually been gathered, (ii) direct per-document extraction toward whichever dimension each document is best positioned to satisfy, and (iii) generate retrieval queries whose specificity matches the precision required by the question. The multi-nugget extractor directly addresses the evidence surface area problem: a document contributing five section-specific nuggets rather than one first sentence increases the probability that the answer-bearing sentence is actually captured. On HLE biochemistry and chemistry questions, where the correct answer often hinges on a single reported measurement, this alone is expected to be the highest-leverage change. The contrastive MCQ queries complement the existing intent_queries['options'] round by targeting retrieval at discriminative evidence rather than confirmatory evidence — the difference between finding papers that discuss option B and finding papers that compare option B unfavorably to option C. Combined, these mechanisms attack the four most common failure modes on hard QA without requiring new external APIs or provider contracts.",
  "expected_impact": {
    "accuracy_delta_pct_points": 7,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _generate_answer_fingerprint(question, task_type, decomp_result, provider, max_tokens=512) in engine.py, called immediately after _decompose_question(). Prompt asks for N=4-8 atomic factual dimensions as a JSON list of {dimension_id, dimension_text, required_for_options (list of option letters if MCQ)}. Cache result in orchestration state. Introduce FingerprintDimension dataclass alongside ClaimEvidence. Graceful fallback: if parse fails, treat the sub_claims from _decompose_question as dimensions.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Implement _compute_dimension_coverage(fingerprint_dims, claims) in engine.py. For each dimension, compute token-overlap score against all current claim_texts; a dimension is 'covered' if max score ≥ 0.15. Return coverage_ratio = covered_dims / total_dims. Integrate into the adaptive gate in the retrieval loop: replace the compound (low_novelty AND low_gain) check with (low_novelty AND low_gain) OR (coverage_ratio >= SPARKIT_FINGERPRINT_COVERAGE_TARGET, default 0.70). Add SPARKIT_FINGERPRINT_COVERAGE_TARGET env var. Emit coverage_ratio into the retrieval_adaptive_gate trace stage payload for observability.",
      "owner": "orchestrator/engine.py + policy.py",
      "effort": "medium"
    },
    {
      "step": "Modify _select_best_section_chunk() to _extract_top_n_nuggets(sections, question_tokens, focus_terms, fingerprint_dims, n=5, max_chars_each=2000). Score each section chunk with the existing 1.4*question_overlap + 1.9*focus_term_overlap formula, then add a 1.5*nearest_dimension_overlap bonus using the fingerprint_dims not yet covered by prior nuggets from this same document. Return up to n distinct (section_name, chunk_text, matched_dimension_id) triples. Ensure total chars across all nuggets ≤ SPARKIT_INGESTION_MAX_CHARS (10000 default) so budget is unchanged. Store all nuggets as separate passages in EvidenceStore, each linked to the same document but carrying their matched_dimension_id as metadata.",
      "owner": "orchestrator/engine.py + evidence_store.py",
      "effort": "high"
    },
    {
      "step": "Add a quantitative-claim anchoring pass after nugget extraction. Apply regex r'[-+]?\\d+\\.?\\d*\\s*(?:%|nm|eV|kJ|mol|Hz|μM|nM|mg|kg|ms|s\\b|cm|pm|K\\b|Å|Da)' over each nugget's text. If ≥1 match found, tag that nugget's claim with claim_type='quantitative'. In _build_synthesis_prompt(), prepend quantitative claims to the evidence bullets section with a 'QUANTITATIVE EVIDENCE (high priority):' header before the general claim list. This ensures numeric results are not buried in long evidence lists and receive LLM attention.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "For MCQ task types, inject contrastive option-pair queries into _decompose_retrieval(). After generating intent_queries['options'] (one hypothesis query per option), add intent_queries['contrastive_pairs']: for each pair (A, B) from answer_choices, emit the query '{question_stem} evidence that {option_A_text} is incorrect if {option_B_text} is true'. Limit to C(n,2) pairs, max 6 queries total (n≤4 choices). Add these queries as a new retrieval round labelled 'retrieval_round_contrastive_pairs' inserted after the options round and before the methods round. Update the round schedule in execute_orchestration to accommodate this round in standard mode.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add gap-driven query generation: after each retrieval round, if coverage_ratio < SPARKIT_FINGERPRINT_COVERAGE_TARGET, call _generate_gap_queries(uncovered_dims, question, focus_terms, provider) with a compact prompt (max_tokens=256) that returns 1-2 targeted queries per uncovered dimension (max 4 queries total). Inject these as an additional micro-round before the next scheduled intent round. Guard with the existing budget check (should_stop_early) to ensure gap rounds do not violate cost/latency constraints. Track gap_rounds_triggered in the trace.",
      "owner": "orchestrator/engine.py",
      "effort": "high"
    },
    {
      "step": "Extend QualityGates schema (shared/schemas/domain.py) with fingerprint_coverage_ratio: float | None and dimension_count: int | None. Populate from compute_dimension_coverage result in execute_orchestration. Update calibrate_answer() in calibration.py to incorporate coverage_ratio: add +0.08*max(0, coverage_ratio - 0.5) term (zero contribution below 50% coverage, up to +0.08 at 100%) so answer confidence reflects whether all required factual dimensions were actually retrieved. Subtract 0.03 per uncovered dimension above 2.",
      "owner": "orchestrator/calibration.py + shared/schemas/domain.py",
      "effort": "low"
    },
    {
      "step": "Add unit tests in test_synthesis_quality.py covering: (a) _generate_answer_fingerprint fallback to sub_claims on bad JSON; (b) _compute_dimension_coverage returns 0.0 for empty claims and 1.0 when all dimensions are token-overlapping; (c) _extract_top_n_nuggets returns ≤n entries and does not exceed max_chars_each*n total; (d) quantitative regex anchoring detects '%', 'nm', 'μM' correctly; (e) contrastive pair query count = C(n_options,2) for n≤4. Run existing 25-test suite to confirm no regressions.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Dimension-gap micro-round injection: after each retrieval round, uncovered fingerprint dimensions drive 1-2 laser-targeted queries generated by LLM instead of reusing the fixed intent categories. A question whose 'protein binding affinity' dimension has zero coverage will generate a query like 'dissociation constant Kd measurement [compound] [target]' rather than relying on the generic methods round to stumble upon it.",
    "Contrastive option-pair retrieval for MCQ: for each pair of answer choices, emit a query seeking evidence that option X is incorrect under the assumption that option Y is correct. This retrieves papers that explicitly compare, contradict, or rank competing mechanisms or values — the high-discriminating evidence class that SPARKIT's current per-option hypothesis queries systematically miss.",
    "Multi-nugget per-document extraction scored against nearest uncovered fingerprint dimension: instead of choosing the single best section chunk globally, the extractor selects up to five chunks from distinct sections, each maximizing overlap with the fingerprint dimension it is closest to filling. A Results section reporting numerical measurements gets extracted even when the Methods section scores higher on generic focus-term overlap.",
    "Dimension-coverage-aware adaptive gate replacing the document-novelty gate: the retrieval loop now has a semantically grounded stopping criterion. It continues if uncovered dimensions exist AND the budget allows, rather than stopping whenever document-count growth slows. This prevents the common HLE failure mode where retrieval stops at 12 topically-relevant papers whose abstracts look good but whose key numeric results were never ingested.",
    "Quantitative evidence elevation: numeric claims (regex-detected values with scientific units) are promoted to claim_type='quantitative' and placed in a dedicated high-priority section of the synthesis prompt, ensuring LLMs weigh reported measurements over qualitative descriptions when both are present — directly addressing chemistry and physics hard questions where answer correctness depends on exact numerical recall."
  ],
  "evaluation_plan": [
    "Dimension coverage correlation study on HLE-25 balanced subset: for each question, log fingerprint_coverage_ratio at answer time and the binary correctness label. Compute Pearson r and compare mean coverage_ratio for correct vs incorrect answers. If the mechanism works, coverage_ratio should be a statistically significant predictor of correctness (expected r > 0.25).",
    "Controlled A/B accuracy benchmark on HLE-25: run current SPARKIT (control) and fingerprint-gated SPARKIT (treatment) at identical $3.00 cost cap and routed mode on the same 25 questions. Primary metric: exact-match accuracy. Secondary: cost per correct answer. Confirm treatment does not regress on questions the control already handles correctly.",
    "Evidence nugget utilization audit on failed questions: for a random 20-question sample of HLE questions the current system answers incorrectly, manually inspect the ingested documents to check whether the answer-bearing sentence appears in the documents but was not captured by the single first-sentence extractor. Measure the hit rate of multi-nugget extraction on those same documents — this validates whether the extraction bottleneck is the primary failure mode before investing in the full rollout.",
    "Gap-query precision measurement: for each generated gap-query (targeting a specific uncovered dimension), retrieve the top-5 results and score whether the returned abstracts contain tokens from the target dimension description (≥0.2 overlap). Compare precision@5 of gap-queries vs the generic adversarial/methods round queries on the same questions. Accept the mechanism if gap-query precision@5 exceeds generic round precision@5 by ≥10 percentage points.",
    "Contrastive query discriminative yield test on MCQ subset: for 10 MCQ questions with known correct answers, compare the top-5 documents returned by contrastive pair queries vs per-option hypothesis queries. Score each retrieved document by whether it explicitly compares or ranks the two targeted options. Contrastive queries should yield ≥2x more explicitly comparative documents per query to justify the extra retrieval cost.",
    "Regression test suite: run all 25 existing test_synthesis_quality.py tests after implementing multi-nugget extraction and fingerprint coverage to confirm no regressions in claim clustering, section bucketing, MCQ option scoring, abstain logic, deduplication, or lexical anchor preservation."
  ],
  "risks": [
    "LLM fingerprint quality degrades on ambiguous or underspecified questions: if _generate_answer_fingerprint() returns vague dimensions like 'explain the mechanism' rather than 'intermediate compound formed in step 2 of pathway X', the dimension-coverage gate and gap-queries inherit that vagueness and provide no precision benefit over the current system. Mitigation: validate fingerprint specificity via token entropy on generated dimension texts; fall back to sub_claims from _decompose_question() if entropy < threshold.",
    "Latency increase may exceed user tolerance for synchronous requests: fingerprint generation (1 LLM call), per-round gap-query generation (1 LLM call per round), and contrastive pair queries (up to 6 additional retrieval calls) add roughly 3-8 seconds to the critical path in routed mode. For research_max mode this could push total latency past 120 seconds. Mitigation: gate gap-query generation behind SPARKIT_ENABLE_FINGERPRINT_GAPS=1 env flag defaulting to 0; allow opt-in during benchmark tuning before wider rollout.",
    "Multi-nugget extraction may amplify noise from low-quality sections: extracting 5 chunks per document rather than 1 increases the risk that boilerplate text (funding acknowledgements, figure captions, reference list artifacts) enters the evidence pool. If the dimension-overlap scoring does not adequately distinguish signal from boilerplate, synthesis prompt length grows without accuracy benefit. Mitigation: apply a minimum content_length ≥ 80 chars and max stopword_ratio ≤ 0.6 filter per nugget before inclusion.",
    "Contrastive option-pair queries are only effective when Brave web search is enabled: academic API adapters (arXiv, Crossref, Semantic Scholar, OpenAlex, Europe PMC) index by keyword and return papers by relevance to query tokens, rarely returning papers that explicitly compare two specific experimental options by name. The contrastive query format is designed for web-search results (review articles, blog posts, textbook summaries) that appear in Brave results. With SPARKIT_ENABLE_WEB_SEARCH=0, these queries degrade to equivalent coverage as the existing per-option hypothesis queries, yielding no lift at added cost.",
    "Fingerprint dimension count calibration: generating N=8 dimensions for a simple factual question over-engineers retrieval, multiplying gap-queries and adding unnecessary latency and cost. Generating N=4 for a complex multi-step mechanism question under-covers the answer requirements. Mitigation: tie dimension count to task_type from _decompose_question(): factual/numerical → 3-4 dims, mechanism → 5-6 dims, comparative → 4-5 dims, MCQ → dims = max(4, n_options+1). Expose SPARKIT_FINGERPRINT_DIM_OVERRIDE env var for benchmark-time tuning."
  ]
}
```
