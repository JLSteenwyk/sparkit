```json
{
  "proposal_id": "SPARKIT-PROP-RCG-001",
  "title": "Residual-Claim Gap-Filling with Citation-Graph Amplification",
  "distinctive_angle": "Invert the retrieval-synthesis dependency: run a fast draft synthesis first, mine it for factual claims that lack evidence citations, treat each orphaned claim as a precision retrieval query, traverse the citation graph of the highest-relevance beachhead papers, then re-synthesize with fully provenance-tracked evidence. The synthesis model's own uncited assertions become the strongest retrieval signal.",
  "summary": "SPARKIT currently retrieves then synthesizes. RCG adds a third phase: extract every factual claim from the draft synthesis, check which ones have zero evidence citations, issue targeted retrieval queries for each orphaned claim (including a 1-hop citation-graph expansion of the top-ranked papers via Semantic Scholar's /paper/{id}/references API), score retrieved passages by source-credibility tier (Nature/Science/top-venue = tier-1; arXiv preprint = tier-3), then re-synthesize using the augmented evidence set with per-claim provenance flags. Orphaned claims that still find no evidence after gap-fill are down-weighted in confidence and explicitly annotated as unverified in the final answer.",
  "reasoning": "The dominant failure mode on HLE-class hard questions is not retrieval breadth—it is synthesis grounding. The model confidently asserts correct-sounding but unsupported facts from parametric memory. The current revision pass only checks lexical anchor coverage, which does not distinguish between well-grounded anchors and hallucinated ones. By treating uncited synthesis claims as first-class retrieval signals, RCG converts the model's internal uncertainty into actionable queries. Citation-graph traversal amplifies this further: the most relevant found paper is likely to cite the exact sub-papers needed to settle hard sub-questions. Source credibility stratification ensures synthesis context is dominated by peer-reviewed, high-impact literature rather than equal-weight preprints. Together these changes attack the top-3 root causes of wrong answers on hard QA: (1) orphaned parametric claims, (2) shallow retrieval stopping at direct keyword matches, (3) low-quality evidence crowding out high-quality evidence.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add claim extraction pass: after draft synthesis, call a cheap fast model (claude-haiku-4-5 or gpt-4o-mini) with prompt 'List every distinct factual claim in this text as a JSON array of strings'. Intersect claim tokens against evidence store citations to classify each claim as GROUNDED or ORPHANED.",
      "owner": "orchestrator/engine.py – new function _extract_and_classify_claims(synthesis_text, evidence: list[ClaimEvidence]) -> list[OrphanedClaim]",
      "effort": "medium"
    },
    {
      "step": "Add gap-fill retrieval stage: for each ORPHANED claim (cap at SPARKIT_RCG_MAX_GAP_CLAIMS, default 5), call search_literature(orphaned_claim_text, max_results=6). Merge new records into the existing evidence corpus deduplicated by DOI/URL. Integrate as a new TraceStage named 'gap_fill_retrieval' inserted between synthesis and synthesis_revision.",
      "owner": "orchestrator/engine.py – new stage block in execute_orchestration after initial synthesis",
      "effort": "medium"
    },
    {
      "step": "Add Semantic Scholar citation-graph traversal: for the top-3 records by relevance score (post-initial-retrieval), call GET https://api.semanticscholar.org/graph/v1/paper/{s2_paper_id}/references?fields=title,abstract,year,externalIds&limit=10. Parse returned papers as LiteratureRecord objects and add to retrieval pool. Gate behind SPARKIT_ENABLE_CITATION_GRAPH env flag (default false until benchmarked).",
      "owner": "retrieval_service/app/aggregator.py – new function fetch_citation_graph_neighbors(record: LiteratureRecord, depth=1) and caller in orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add source credibility tier scoring: build a static lookup VENUE_TIER = {Nature: 1, Science: 1, Cell: 1, NEJM: 1, NeurIPS: 1, ICML: 1, ICLR: 1, ...arXiv: 3, other: 2}. Extend _record_relevance_score() to multiply by tier_weight = {1: 1.25, 2: 1.0, 3: 0.85}. Parse venue from record.doi prefix or title-matched journal name.",
      "owner": "orchestrator/engine.py – extend _record_relevance_score() and add VENUE_TIER dict",
      "effort": "low"
    },
    {
      "step": "Add passage-level semantic deduplication before synthesis context assembly: compute pairwise Jaccard similarity on token sets of section_text chunks (window: top-12 passages). If similarity > 0.72 between two passages, keep the one from the higher credibility tier; discard the other. This frees context budget for genuinely novel evidence.",
      "owner": "orchestrator/engine.py – new function _deduplicate_passages_by_jaccard(chunks: list[ClaimEvidence], threshold=0.72) -> list[ClaimEvidence]",
      "effort": "low"
    },
    {
      "step": "Wire gap-fill evidence into re-synthesis: after gap-fill retrieval completes, re-build evidence store with merged corpus (original + gap-fill + citation-graph neighbors), re-score and re-select top passages, then call the synthesis stage a second time using the enriched evidence. Annotate orphaned claims that are still ungrounded after gap-fill with [UNVERIFIED] tag in the synthesis prompt instruction.",
      "owner": "orchestrator/engine.py – refactor synthesis stage to accept evidence_corpus parameter; add _build_gap_fill_synthesis_prompt()",
      "effort": "medium"
    },
    {
      "step": "Add configuration knobs and budget guard: SPARKIT_ENABLE_RCG (bool, default false for rollout), SPARKIT_RCG_MAX_GAP_CLAIMS (int, default 5), SPARKIT_RCG_CITATION_GRAPH_DEPTH (int, default 1), SPARKIT_RCG_MIN_ORPHAN_CONFIDENCE (float, default 0.0 = always run). Enforce max added cost: if budget_state.spent + estimated_gap_fill_cost > max_cost_usd * 0.85, skip gap-fill.",
      "owner": "orchestrator/engine.py env config block + policy.py budget guard",
      "effort": "low"
    },
    {
      "step": "Extend TraceStage and ObservabilityMetrics to capture RCG-specific signals: orphaned_claims_count, gap_fill_grounded_count, gap_fill_hit_rate, citation_graph_papers_fetched, passage_dedup_dropped, credibility_tier_distribution. Persist in run_observability_metrics for offline analysis.",
      "owner": "orchestrator/observability.py + shared/schemas/domain.py",
      "effort": "low"
    },
    {
      "step": "Add unit tests: (a) _extract_and_classify_claims correctly identifies uncited claims; (b) gap-fill retrieval is skipped when budget guard fires; (c) Jaccard deduplication removes near-duplicates and prefers higher-tier source; (d) citation graph traversal returns valid LiteratureRecord objects; (e) synthesis prompt includes [UNVERIFIED] annotation for persistently orphaned claims.",
      "owner": "services/orchestrator/tests/test_rcg_gap_fill.py (new file)",
      "effort": "medium"
    },
    {
      "step": "Run ablation benchmark: HLE-gold-25 balanced subset × {baseline, +citation_graph_only, +gap_fill_only, +dedup_only, +credibility_tier_only, all_RCG_combined}. Report per-variant accuracy, citation_coverage_rate, cost_usd, latency_s. Promote to default if combined delta >= +4pp accuracy with <= +35% cost increase.",
      "owner": "eval_service + benchmarks/hle_gold/",
      "effort": "high"
    }
  ],
  "retrieval_improvements": [
    "Orphaned-claim gap-fill queries: extract every factual claim from draft synthesis that has zero evidence citations, issue one search_literature() call per orphaned claim (capped at SPARKIT_RCG_MAX_GAP_CLAIMS=5), merge results into the evidence corpus before re-synthesis. This converts the model's parametric memory into precision retrieval queries rather than unverified hallucinations.",
    "Citation-graph 1-hop traversal via Semantic Scholar API: for the top-3 records by adjusted relevance score, fetch up to 10 cited papers using /graph/v1/paper/{id}/references. Add returned papers as new LiteratureRecord candidates and score them through the standard relevance pipeline. This systematically discovers the sub-papers that domain experts (the authors of the most relevant paper) have identified as foundational, which keyword search frequently misses.",
    "Source credibility tier multiplier in relevance scoring: assign each retrieved record a credibility tier based on publication venue (tier-1: Nature/Science/Cell/NEJM/top ML conferences; tier-2: peer-reviewed journals/workshop papers; tier-3: arXiv preprints/unknown). Multiply the existing relevance score by tier_weight to ensure synthesis context is dominated by high-quality peer-reviewed evidence rather than equal-weight preprints.",
    "Passage-level Jaccard deduplication before synthesis context assembly: compute pairwise token-set Jaccard similarity across the top-12 candidate passages. When two passages exceed 0.72 similarity, keep the one from the higher credibility tier and discard the other. This prevents near-duplicate boilerplate passages (common in literature that cites each other) from consuming context budget and crowding out genuinely novel evidence.",
    "Semantic Scholar S2 paper ID extraction from DOIs: extend LiteratureRecord parsing to extract the Semantic Scholar paper ID from crossref/DOI responses, enabling reliable citation-graph traversal without a separate lookup round-trip. Store as record.s2_paper_id. Fall back to title-based S2 search if DOI is absent."
  ],
  "evaluation_plan": [
    "Citation coverage rate gate: after RCG re-synthesis, measure fraction of final synthesis claims that map to at least one evidence citation. Target >= 0.75 citation coverage on HLE-gold questions (vs current ~0.40-0.60 baseline). Fail the run-level quality gate if coverage < 0.55 in RESEARCH_MAX mode.",
    "Gap-fill hit rate metric: log the fraction of orphaned claims for which gap-fill retrieval found at least one new relevant record (relevance score > 0.15). Track this per-domain (chemistry, biology, physics, math). Low hit rate in a domain signals that retrieval sources are inadequate for that domain, triggering a source-expansion alert.",
    "Answer accuracy ablation on HLE-gold balanced subset: run HLE-gold-25 × {baseline_SPARKIT, RCG_enabled} at least 3× each for statistical stability. Report mean accuracy, 95% CI, cost_usd, p50/p95 latency. Require p < 0.10 for accuracy delta to promote RCG to default.",
    "Hallucination audit on hard failures: for any question where SPARKIT answers incorrectly with confidence > 0.7, check whether the wrong claim was an orphaned claim that gap-fill failed to ground. Classify failures as: (A) gap-fill found contradicting evidence but synthesis ignored it, (B) gap-fill failed to find any evidence, (C) gap-fill found supporting evidence that is itself incorrect. This disambiguates model synthesis errors from retrieval coverage errors.",
    "Confidence calibration check: compute ECE and Brier score on RCG-enabled vs baseline runs. Verify that persistent [UNVERIFIED] annotations correctly lower confidence on wrong answers (i.e., wrong-confident answers decrease). A regression in ECE > 0.03 blocks promotion of RCG to default.",
    "Credibility tier distribution audit: for each benchmark run, log the proportion of evidence passages used in final synthesis that come from each tier. Verify tier-1 proportion >= 20% of synthesis context. If tier-1 proportion < 10% on a given question, flag as low-evidence-quality and surface in trace."
  ],
  "risks": [
    "Serial latency dependency: gap-fill retrieval depends on draft synthesis completing first, adding a serial I/O-bound step. Expected p50 latency increase: +8-15 seconds per question in RESEARCH_MAX mode. Mitigation: parallelize gap-fill retrieval calls across orphaned claims; gate gap-fill on budget/latency guards.",
    "Draft synthesis quality bootstrapping problem: if initial evidence is too sparse, the draft synthesis may be low-quality or fabricated, producing noisy orphaned claims that generate irrelevant gap-fill queries. Mitigation: require minimum 4 ingested documents before enabling RCG; fall back to standard synthesis if draft confidence < 0.25.",
    "Citation graph traversal returns tangentially-related papers: if the seed paper is only partially relevant, its cited papers may be even less relevant and pollute the evidence corpus. Mitigation: score citation-graph papers through the same _record_relevance_score() pipeline and only add if score > 0.12; enforce the existing 50%-per-source dominance cap to prevent citation-graph flooding.",
    "Source credibility tier lookup has poor coverage for non-English or niche venues: the static VENUE_TIER dict will default tier-2 for most venues, limiting its discriminative power. Mitigation: integrate impact-factor-based fallback via Crossref journal metadata; default tier-2 is a safe no-op baseline.",
    "Cost overrun in RESEARCH_MAX mode: RCG adds up to 5 gap-fill retrieval rounds + citation graph fetches + a second synthesis call. Estimated additional cost: +$0.40-0.80 per question in RESEARCH_MAX. Mitigation: hard cap at 85% of max_cost_usd before RCG phases; reduce SPARKIT_RCG_MAX_GAP_CLAIMS to 3 in cost-constrained runs.",
    "Re-synthesis with augmented evidence may regress on questions where initial synthesis was already correct: adding new evidence can introduce distracting contradictions. Mitigation: compare initial synthesis confidence vs post-RCG confidence; if post-RCG confidence drops > 0.15, fall back to initial synthesis answer (similar to the existing synthesis revision rejection logic)."
  ]
}
```
