Now I have everything I need. Let me compose the proposal JSON based on the deep audit.

```json
{
  "proposal_id": "SPARKIT-PERF-2026-001",
  "title": "Sub-claim Coverage Ledger with Evidence-Gap-Triggered Targeted Retrieval",
  "distinctive_angle": "SPARKIT already generates sub_claims inside _decompose_question() but discards them as planning metadata once the RetrievalPlan is built. This proposal wires those sub_claims into a live per-round coverage ledger that intercepts the adaptive_retrieval_gate decision: retrieval continues—up to budget—until every sub-claim is matched by at least one retrieved passage above a token-overlap threshold, not merely until doc count is low or global avg-relevance gain is small. Uncovered sub-claims automatically emit targeted gap-fill queries into a bounded extra retrieval round via the existing search_literature() path. This converts SPARKIT's decomposition artifact from a prompt-formatting tool into an active retrieval completeness driver, without requiring dense embeddings, cross-encoders, or any new external infrastructure.",
  "summary": "Extend the SPARKIT orchestration loop in engine.py with three interlocking changes: (1) a sub-claim coverage ledger that scores each sub-claim from ResearchPlan.sub_claims against every retrieved passage after each round using the existing tokenized overlap logic; (2) a modified _adaptive_retrieval_gate that continues retrieval if coverage_ratio < configurable threshold, overriding the low_novelty+low_gain early-stop; (3) a _generate_gap_queries() step that calls generate_text() on the planning provider to produce targeted academic queries for each uncovered sub-claim, then executes one bounded extra round via search_literature(). Downstream, a new subclaim_coverage_ratio field is added to CalibrationFeatures and wired into the calibrate_answer() formula, and _build_synthesis_prompt() surfaces uncovered sub-claims as explicit evidence-gap bullets so the synthesizing model can hedge appropriately rather than confabulate.",
  "reasoning": "Hard SPARKIT questions fail in a specific pattern: the adaptive gating stops after round 2-3 because new_unique_docs < adaptive_min_new_docs=2 AND quality_gain < 0.03, even though only 1-2 of 4 sub-claims are evidenced. The global avg-relevance metric (_avg_relevance using document-level title+abstract token overlap) rewards retrieving many loosely relevant papers without checking whether any passage addresses the specific mechanism, numerical threshold, or experimental design the question tests. The ResearchPlan already contains the granular sub-claim list but _build_round_queries_from_plan() never looks back at it after round 1. The result is synthesis over uniformly shallow evidence that appears broad (10+ docs) but is deep on the main topic and empty on the sub-questions. Sub-claim coverage ledger directly patches this: it creates a per-claim boolean coverage map, and gap-triggered queries are generated specifically for uncovered slots. For HLE-level STEM questions, which are almost always multi-step (measure X, then compare to Y, then determine which mechanism explains the difference), full sub-claim coverage is a reliable proxy for answer correctness that the current gating metric cannot detect.",
  "expected_impact": {
    "accuracy_delta_pct_points": 7,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _score_subclaim_coverage(sub_claims, records) to engine.py: for each sub-claim string, compute max token-overlap score across all retrieved record abstracts/section_text using the existing _tokenize() and overlap logic; return {sub_claim: max_score} dict and coverage_ratio = fraction above threshold (default 0.15, env-controlled as SPARKIT_SUBCLAIM_COVERAGE_THRESHOLD).",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Modify the adaptive_retrieval_gate block (engine.py lines 1398-1454) to call _score_subclaim_coverage() after each round when research_plan.sub_claims is non-empty; add subclaim_coverage_ratio to the gate artifacts dict; extend the early-stop condition so that low_novelty AND low_gain only triggers break if coverage_ratio >= SPARKIT_SUBCLAIM_MIN_COVERAGE (default 0.75). Gate still obeys adaptive_max_rounds as hard ceiling.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Add _generate_gap_queries(uncovered_sub_claims, planning_provider) that calls generate_text() with a concise prompt: 'For each research sub-claim below, emit one precise academic literature search query (<= 12 terms) on a separate line. Sub-claims: {list}'. Parse the response into a list of query strings. Cap at 6 queries regardless of sub-claim count.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "After the main retrieval loop (before the final _dedupe_records call at line 1456), if any sub-claims remain uncovered and budget allows: call _generate_gap_queries() for uncovered sub-claims, execute search_literature() with those queries using intent 'gap_fill', append results to all_records, and record a new TraceStage 'retrieval_gap_fill' with artifacts {uncovered_sub_claims, gap_queries, new_docs_found}.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add subclaim_coverage_ratio (float 0-1) to CalibrationFeatures dataclass in calibration.py; adjust calibrate_answer() formula to include +0.10 * subclaim_coverage_ratio and -0.05 * (1 - subclaim_coverage_ratio) terms, rebalancing existing weights to keep the 0.25 baseline anchor; add subclaim_coverage_ratio to features_to_dict() for persistence.",
      "owner": "calibration",
      "effort": "low"
    },
    {
      "step": "Extend _build_synthesis_prompt() (engine.py line 993): after the evidence bullets block, append an 'Evidence gaps' section listing any sub-claims with coverage_ratio < threshold as '- UNRESOLVED: {sub_claim}'. Instruct the synthesizer: 'For UNRESOLVED items, state explicitly that evidence is insufficient rather than inferring.' Pass uncovered_sub_claims into the prompt builder via a new parameter.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Add SPARKIT_SUBCLAIM_COVERAGE_THRESHOLD (default 0.15) and SPARKIT_SUBCLAIM_MIN_COVERAGE (default 0.75) to configuration.md and validate in the existing _env_float helper pattern; add subclaim_coverage_ratio to the QualityGates dataclass so it surfaces in /v1/runs/{id}/trace artifacts.",
      "owner": "config",
      "effort": "low"
    },
    {
      "step": "Add unit tests to test_synthesis_quality.py: (a) _score_subclaim_coverage returns 1.0 for perfectly matching records and 0.0 for empty records; (b) adaptive gate does not stop when coverage_ratio < 0.75 even if low_novelty and low_gain are both true; (c) gap_fill stage appears in trace when sub-claims are uncovered; (d) synthesis prompt contains UNRESOLVED bullets for uncovered sub-claims; (e) subclaim_coverage_ratio in CalibrationFeatures shifts answer_confidence in expected direction.",
      "owner": "testing",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Replace global avg-relevance as the sole adaptive gating criterion with per-sub-claim passage coverage scoring: compute max token-overlap between each ResearchPlan.sub_claim and every retrieved record's abstract+section_text; report coverage_ratio = fraction of sub-claims above threshold. Retrieval continues when coverage_ratio < SPARKIT_SUBCLAIM_MIN_COVERAGE regardless of doc count or global quality gain.",
    "Add a bounded gap-fill retrieval round after the main loop: uncovered sub-claims are passed to _generate_gap_queries() which calls the planning provider to emit precise academic queries per uncovered slot (up to 6 queries, <= 12 terms each). These queries execute against all live retrieval sources via search_literature() with the same dedup/domain-diversity logic. The round is budget-gated and produces a dedicated 'retrieval_gap_fill' trace stage.",
    "Extend _select_records_for_ingestion() to apply a sub-claim affinity boost during record selection: records whose abstract+section_text overlaps with an uncovered sub-claim receive a +0.25 relevance bonus on top of existing focus_term scoring. This ensures gap-fill records displace less-relevant redundant papers from the ingestion window rather than being diluted by the existing pool.",
    "For MCQ questions, add option-specific passage isolation inside the ingestion loop: when answer_choices is non-empty, after selecting the best section chunk via _select_best_section_chunk(), run a secondary scan for a passage most overlapping with each answer choice label text and store it as an option_anchor_text per choice. Thread these anchors into the MCQ option scoring stage alongside the existing dossier snippets, improving the lexical scorer's 30% contribution by anchoring it to option-specific passages rather than the full claim corpus.",
    "Add passage-level answerability classification as a section-selection filter inside _select_best_section_chunk(): tag each candidate section with an answerability type (numerical: contains digits/units/percentages; mechanism: contains causal connectors; comparative: contains comparison phrases; declarative: other). When the task_type from ResearchPlan is 'numerical' or 'mechanism', deprioritize 'overview' and 'other' sections even if they have higher focus-term overlap, preferring 'results' or 'methods' sections that match the task type. This is a pure heuristic requiring no LLM call."
  ],
  "evaluation_plan": [
    "Per-sub-claim coverage audit on HLE-25: after each run, log the {sub_claim: max_passage_overlap} dict per question; compute mean coverage_ratio for correct vs. incorrect answers; test hypothesis that questions with coverage_ratio >= 0.75 have >= 15 pp higher accuracy than questions with coverage_ratio < 0.50.",
    "A/B gate comparison on HLE-25 with 3 replicate runs each: (arm A) baseline adaptive gating using only low_novelty+low_gain; (arm B) sub-claim coverage gating enabled. Report accuracy delta, cost delta, mean retrieval rounds, and calibration ECE/Brier. Accept the proposal if arm B shows >= 5 pp accuracy improvement without ECE regression > 0.02.",
    "Gap-fill retrieval effectiveness check: for runs where gap-fill round triggered, measure what fraction of uncovered sub-claims gained coverage > threshold after the gap-fill round; measure how many gap-fill documents were selected for ingestion; verify that questions receiving gap-fill coverage show higher rubric scores than questions where gap-fill did not help (as a regression analysis, not a gate).",
    "Calibration feature validation: verify that subclaim_coverage_ratio has positive Pearson correlation with rubric_score across HLE-25 runs (expected r >= 0.30); confirm that the updated calibrate_answer() formula reduces Brier score by >= 0.01 on the benchmark set compared to the formula without the subclaim_coverage_ratio terms.",
    "Synthesis prompt UNRESOLVED bullet check: for runs where at least one sub-claim is flagged UNRESOLVED, verify that the synthesized answer text contains explicit uncertainty hedging (detected by presence of phrases: 'insufficient evidence', 'not established', 'unclear from evidence', 'cannot determine'); verify that answer_confidence <= 0.55 for those runs.",
    "Cost regression guard: measure mean cost per question on HLE-25 before and after; flag as regression if gap-fill round triggers on more than 60% of questions (indicating the coverage threshold is too low and the feature is firing indiscriminately). Tune SPARKIT_SUBCLAIM_MIN_COVERAGE upward if regression is detected."
  ],
  "risks": [
    "Sub-claim quality from _decompose_question() is LLM-generated and may be noisy or misaligned with what the question actually tests: a poorly decomposed sub-claim could be trivially covered by any retrieved paper (making coverage_ratio artificially high) or pathologically uncoverable (making the gating never stop). Mitigate by running sub-claim quality audits on HLE-25 ground truth and tuning SPARKIT_SUBCLAIM_COVERAGE_THRESHOLD empirically.",
    "Gap-fill retrieval adds one extra round per question with uncovered sub-claims, increasing cost ~15-30% and latency ~8-15 seconds per round on the current retrieval stack. If coverage gating is too aggressive (threshold too low), almost every question triggers gap-fill, making the cost increase systematic rather than selective. Budget guard via SPARKIT_SUBCLAIM_MIN_COVERAGE threshold and hard check against SPARKIT_ADAPTIVE_MAX_ROUNDS.",
    "The planning provider call inside _generate_gap_queries() adds a synchronous LLM call before the gap-fill round. If the planning provider is slow or fails, the gap-fill round is skipped (graceful degradation: fall back to no extra retrieval). Need to add a short timeout (e.g., 8s) and exception catch consistent with the existing generate_text() error handling pattern.",
    "For questions without LLM-decomposed sub-claims (heuristic fallback from _heuristic_decomposition()), the sub-claims are derived from question segments and may be too coarse to drive meaningful coverage scoring. Coverage ledger should be disabled or treated as advisory-only when research_plan.sub_claims originates from the heuristic path (detectable by checking if disambiguations is empty).",
    "Passage-level answerability type classification (task_type matching to section type) may conflict with the existing focus-term scoring in edge cases where the most relevant passage is in an 'overview' section but the task_type is 'numerical'. The section type boost should be a soft preference (+/- score) not a hard filter, to avoid discarding the only highly relevant passage."
  ]
}
```
