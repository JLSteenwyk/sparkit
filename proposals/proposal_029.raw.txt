Now I have everything I need for a thorough, SPARKIT-specific proposal. Let me produce the strict JSON output.

```json
{
  "proposal_id": "SPARKIT-PROP-2026-001",
  "title": "Atomic Sub-Claim Retrieval with LLM Evidence Quality Gating, Structured Scientific Database Sources, and Table-Aware Ingestion",
  "distinctive_angle": "Shift SPARKIT retrieval from quantity-optimized topic-search to quality-optimized per-claim evidence verification: each sub-claim from ResearchPlan becomes an explicit falsifiability query, records are pre-filtered by an LLM relevance judge before ingestion (replacing the purely lexical _record_relevance_score), PubChem REST and UniProt REST become first-class retrieval sources for structured scientific facts, and HTML ingestion is upgraded to extract tables alongside paragraphs so quantitative data in paper results sections actually reaches synthesis.",
  "summary": "SPARKIT currently retrieves records via broad intent-based queries (primary, options, adversarial), ranks them by bag-of-words overlap (2*title_overlap + 1*abstract_overlap + 0.25*recency in _record_relevance_score), and ingests the top-k sections by lexical focus-term overlap. For hard HLE-level STEM questions this pipeline consistently retrieves topically related but evidentially non-specific papers: correct papers that use different phrasing score low, and high-scoring papers often describe the topic without containing the precise mechanistic or quantitative fact needed. This proposal couples five targeted changes: (1) per-sub-claim targeted retrieval queries generated from ResearchPlan.sub_claims; (2) a lightweight LLM evidence quality gate in _select_records_for_ingestion that batch-scores each abstract against the question before expensive URL fetching; (3) PubChem PUG REST and UniProt REST clients added as first-class sources in aggregator.py, activated by domain hint; (4) HTML table extraction in parser.py so results-section tabular data reaches synthesis; (5) a SPARKIT_EVIDENCE_QUALITY_GATE env flag to enable/disable gracefully. Together these changes attack the single biggest source of hard-question failure: synthesis is bounded by evidence quality, and evidence quality is currently bounded by lexical matching against broad topic queries.",
  "reasoning": "Three structural weaknesses bound SPARKIT performance on hard HLE questions. First, query formulation: _decompose_retrieval() produces intent buckets (primary, options, methods, adversarial) from heuristic patterns and an LLM planner, but none of the queries are shaped as explicit claim verification requests. A question about whether Braveheart expression increases or decreases produces queries like 'Braveheart cardiac development' (primary) rather than 'Braveheart expression quantification lncRNA cardiac mesoderm increases' which would retrieve the specific paper that measured this. Per-sub-claim queries derived from ResearchPlan.sub_claims directly encode what evidence must exist and produce a retrieval signal an order of magnitude more specific. Second, record selection: _record_relevance_score computes 2*title_overlap + 1*abstract_overlap, which cannot distinguish 'this paper mentions CRISPR' from 'this paper measures the specific efficiency delta the question asks about'. A batched LLM call scoring the abstract against the question (e.g., 'does this abstract describe evidence for or against [sub-claim]?') costs roughly $0.001 per batch of 20 abstracts at gpt-5.2 pricing and eliminates the top cause of synthesis failure: ingesting topically correct but evidentially irrelevant passages. Third, source coverage: chemistry hard questions about compound properties, stereochemistry, reaction mechanisms, and molecular formula are not indexable from arXiv/Crossref titles—these facts live in PubChem. Biology questions about protein function, GO terms, gene expression data, and disease associations live in UniProt. Neither is currently queried. Adding these structured REST sources (no API key required, rate-limit friendly) is the highest-leverage retrieval change for the bio-chem benchmark domain used in HLE-gold.",
  "expected_impact": {
    "accuracy_delta_pct_points": 9,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add per-sub-claim retrieval query synthesis in engine.py _build_round_queries_from_plan(). For each sub_claim in research_plan.sub_claims (max 4), emit one targeted query: '{stem} {sub_claim}' appended to the primary intent bucket in round 1. Mark these with intent prefix 'sub_claim_verify:' so _build_option_dossiers can later weight them separately. Gate under SPARKIT_ENABLE_SUBCLAIM_QUERIES env flag (default 1 in research_max, 0 in standard). Ensure _dedupe_queries cap remains at 8 per round so budget is not blown.",
      "owner": "orchestrator-engine",
      "effort": "low"
    },
    {
      "step": "Implement PubChemClient in services/retrieval_service/app/sources/pubchem.py using PubChem PUG REST API. Function search_pubchem(query: str, max_results: int) -> list[LiteratureRecord]. Flow: GET /rest/pug/compound/name/{url_encoded_query}/JSON to get CIDs, then GET /rest/pug/compound/cid/{cid}/property/MolecularFormula,IUPACName,CanonicalSMILES,InChIKey/JSON, then GET /rest/pug/compound/cid/{cid}/description/JSON for compound description. Map to LiteratureRecord(source='pubchem', title=IUPACName, abstract=description_text+' Formula:'+formula+' SMILES:'+smiles, url='https://pubchem.ncbi.nlm.nih.gov/compound/'+cid, year=None). No API key. Rate limit 5 req/s enforced via time.sleep. Timeout 8s.",
      "owner": "retrieval-service",
      "effort": "medium"
    },
    {
      "step": "Implement UniProtClient in services/retrieval_service/app/sources/uniprot.py using UniProt REST API. Function search_uniprot(query: str, max_results: int) -> list[LiteratureRecord]. Flow: GET rest.uniprot.org/uniprotkb/search?query={url_encoded}&fields=protein_name,gene_names,organism_name,function,go,cc_disease&format=json&size={max_results}. Map to LiteratureRecord(source='uniprot', title=protein_name+' ('+gene+') '+organism, abstract=function_text+' GO:'+go_terms, url='https://www.uniprot.org/uniprotkb/'+accession). No API key. Timeout 8s.",
      "owner": "retrieval-service",
      "effort": "medium"
    },
    {
      "step": "Integrate PubChemClient and UniProtClient into aggregator.py search_literature(). Accept optional domain_hint: str | None parameter (propagated from engine.py execute_orchestration via new field). If domain_hint contains 'chemistry' or 'biochemistry', call search_pubchem(query, per_source) and add results to combined. If domain_hint contains 'biology' or 'biochemistry', call search_uniprot(query, per_source) and add results to combined. Add 'pubchem' and 'uniprot' to the existing _limit_source_dominance logic with per_source_cap=max(1, max_results//4) so they cannot dominate. Add SPARKIT_ENABLE_STRUCTURED_DB env flag (default 1). Update stats dict to track pubchem/uniprot request counts.",
      "owner": "retrieval-service",
      "effort": "low"
    },
    {
      "step": "Add LLM evidence quality gate in engine.py _select_records_for_ingestion(). Before the two-pass source-diversity selection, batch all candidate records and call an LLM (planning provider) with a compact prompt: given question stem + up to 20 record titles+abstracts (truncated to 200 chars each), score each record 0-3 (0=irrelevant, 1=topically related, 2=likely contains specific evidence, 3=directly answers a sub-claim). Parse integer scores from response. Multiply each record's _record_relevance_score by (1 + llm_quality_score * 0.5) before the diversity pass. Cost: ~$0.005 per batch at gpt-5.2 pricing. Gate under SPARKIT_ENABLE_EVIDENCE_QUALITY_GATE env flag (default 0 standard, 1 research_max). Add llm_evidence_quality_scores to trace artifacts.",
      "owner": "orchestrator-engine",
      "effort": "high"
    },
    {
      "step": "Upgrade HTML ingestion in parser.py _parse_html() to extract tables. After paragraph extraction, additionally parse <table> elements: for each <table>, render header row (from <th>) + data rows (from <td>) as pipe-delimited text 'Col1 | Col2 | Col3 / Val1 | Val2 | Val3'. Prepend '[Table N]' heading. Append table text to the existing body section. Cap table extraction at 3 tables per page, each at 500 chars. This surfaces quantitative results (dose-response tables, IC50 values, expression ratios) that the question may hinge on but that currently never reach synthesis.",
      "owner": "ingestion-service",
      "effort": "low"
    },
    {
      "step": "Propagate domain_hint from benchmark question metadata through execute_orchestration() into search_literature(). Add optional domain_hint: str | None = None parameter to execute_orchestration() signature. Pass it through to retrieval stage calls. In eval harness (eval_service or benchmark runner), populate domain_hint from question 'domain' field in HLE question JSON (e.g., 'biology', 'chemistry'). This enables the structured DB gating without changing the public API semantics.",
      "owner": "eval-harness",
      "effort": "low"
    },
    {
      "step": "Write new tests in test_synthesis_quality.py or a new test_retrieval_quality.py: (a) search_pubchem returns LiteratureRecord-compatible output with non-empty abstract containing SMILES or formula, (b) search_uniprot returns LiteratureRecord with non-empty abstract containing GO term or function text, (c) _select_records_for_ingestion with LLM quality gate assigns higher composite scores to quality=3 records vs quality=0 records for same lexical overlap, (d) _parse_html extracts pipe-delimited table text from fixture HTML containing a <table>, (e) sub_claim queries are generated and deduplicated correctly from a ResearchPlan with 3 sub_claims.",
      "owner": "test",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Per-sub-claim targeted queries: extend _build_round_queries_from_plan() to emit one falsifiability query per ResearchPlan.sub_claim (e.g., 'Braveheart lncRNA expression cardiac mesoderm quantification increases'), making each retrieval query explicitly encode what evidence must exist to confirm a specific claim rather than broadly probing a topic—this is the single highest-leverage change because it steers all downstream source ranking and ingestion toward evidence that can actually resolve the question",
    "PubChem REST as first-class retrieval source: add search_pubchem() to aggregator.py activated when domain_hint contains 'chemistry'; PubChem returns authoritative structured facts (molecular formula, canonical SMILES, InChIKey, compound descriptions) that are absent from arXiv/Crossref/Semantic Scholar title+abstract fields, directly addressing the systematic gap where hard chemistry questions about compound identity, stereochemistry, and molecular properties fail because no textual source abstractly describes these facts",
    "UniProt REST as first-class retrieval source: add search_uniprot() to aggregator.py activated when domain_hint contains 'biology'; UniProt returns curated protein function descriptions, GO term annotations, disease associations, and canonical gene-protein name mappings that allow SPARKIT to resolve biology questions about protein roles, pathway membership, and expression contexts with authoritative reference-DB facts rather than relying on paper abstracts that may use non-canonical names or abbreviations",
    "LLM evidence quality pre-filter before ingestion: batch-score all retrieved record abstracts against the question using the planning provider in _select_records_for_ingestion(), multiplying each record's _record_relevance_score by (1 + quality_score * 0.5); this replaces purely lexical selection with a lightweight semantic relevance judgment and prevents URL fetches on records that are topically correct but evidentially useless (e.g., a CRISPR review paper when the question requires a specific efficiency measurement)",
    "HTML table extraction in parser.py: parse <table> elements into pipe-delimited text appended to the section body; results-section tables containing IC50 values, expression fold-changes, reaction yields, binding affinities, and comparative dose-response data are currently lost at ingestion, meaning synthesis never sees the quantitative data that often directly answers a hard STEM question"
  ],
  "evaluation_plan": [
    "Ablation accuracy delta on HLE-25 MCQ subset: run three configs back-to-back—(a) baseline, (b) baseline + structured DB sources (PubChem + UniProt), (c) baseline + structured DB + sub-claim queries + LLM evidence gate (research_max profile)—and record per-question correct/incorrect, noting which questions flip and whether flipped questions had chemistry or biology domain hints; target: config-c accuracy >= config-a + 6 pp",
    "Evidence quality precision audit: for 30 questions (15 chemistry, 15 biology), manually inspect the top-3 ingested record passages per question and score 0/1 whether the passage contains a specific fact (not just topic mention) relevant to selecting the correct answer; measure precision@3 for baseline vs proposal; target: proposal precision@3 > 0.55 vs baseline expected ~0.30",
    "Structured DB source hit rate: for the 25 chemistry HLE questions, check what fraction of runs return at least one PubChem record with non-empty SMILES or formula in its abstract; for the 13 biology HLE questions, check what fraction return at least one UniProt record with function text > 50 chars; target: >70% hit rate for each domain to confirm the sources are useful",
    "LLM evidence gate score correlation: for 20 questions where the correct answer is known, compare the LLM quality gate score (0-3) of the ingested passage that contains the ground-truth fact vs all other ingested passages; if the gate score correctly ranks the ground-truth passage in top-2 for >60% of questions, the gate is discriminative enough to justify its cost",
    "Table extraction coverage check: parse 20 HTML paper pages from chemistry/biology domains using the upgraded _parse_html(); measure the fraction that contain at least one extracted table with >2 rows; manually verify 5 extracted tables contain quantitative results data (not navigation or formatting tables); target: >40% pages yield useful table extracts",
    "Cost and latency regression: run the full HLE-25 set with all improvements enabled at research_max profile and verify average cost per question stays within 1.8x of the research_max baseline (LLM quality gate adds ~$0.005/question, structured DB calls add ~$0.001/question, table extraction adds ~10ms latency—all should be well within the $3 max_cost_usd default); fail the evaluation if any single question exceeds $6"
  ],
  "risks": [
    "LLM evidence quality gate latency: batching 20 records into one quality-scoring call adds ~1-3s per retrieval round (network + generation); at 3 rounds this is +3-9s total, which may push research_max runs past reasonable wall-clock limits; mitigate by (a) making the gate async-parallel with the next round's query dispatch and (b) capping batch size at 15 records, skipping the gate for rounds where retrieval returns fewer than 5 records",
    "PubChem rate limiting: PubChem PUG REST imposes 5 requests/second globally and may throttle servers under burst load; mitigate by implementing exponential backoff with jitter in search_pubchem(), catching HTTP 429, and treating PubChem failure as a non-fatal error (errors dict entry) rather than a pipeline abort—this preserves the existing SPARKIT graceful degradation pattern",
    "UniProt returns canonical protein names that may not match question phrasing: a question about 'Braveheart' (lncRNA gene name) will not match UniProt records for heart proteins—mitigate by also querying UniProt with extracted lexical anchors from _extract_lexical_anchors() rather than only the raw question text, and by checking if the question contains gene-name patterns (all-caps identifiers) before invoking UniProt",
    "LLM quality gate circular bias: using the planning provider to pre-score evidence before synthesis may encode the provider's own priors into evidence selection, amplifying provider-specific blind spots rather than neutralizing them; mitigate by using a different provider for the quality gate than for synthesis when routing_mode='routed' (e.g., use verification provider from ProviderPlan for the gate call)",
    "Table extraction false positives from navigation and formatting tables: HTML pages frequently use <table> for layout (navigation, sidebars, citation lists) rather than data; unfiltered extraction will pollute passages with irrelevant text; mitigate by filtering tables where (a) the table has fewer than 2 columns, (b) the rendered text has fewer than 30 characters, or (c) more than 80% of cells contain only single digits or punctuation—this heuristic already exists in similar form in _select_best_section_chunk() scoring",
    "Sub-claim queries may be too narrow for underspecified questions: if ResearchPlan.sub_claims are vague (e.g., ['cardiac development', 'gene expression']) the per-claim queries will be no more specific than existing primary queries; mitigate by requiring sub_claims to pass a minimum length threshold (>12 tokens) before generating targeted queries, and falling back to the existing _heuristic_retrieval_plan() if fewer than 2 qualifying sub_claims exist"
  ]
}
```
