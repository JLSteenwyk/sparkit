```json
{
  "proposal_id": "SPARKIT-P-2026-001",
  "title": "Reasoning-Trace-Conditioned Retroactive Retrieval (RTRR)",
  "distinctive_angle": "Close the synthesis-retrieval loop by mining the LLM's own intermediate reasoning trace for specific propositions, then launching targeted retroactive retrieval queries derived from those propositions rather than the original question. This converts synthesis reasoning from a retrieval consumer into a retrieval driver.",
  "summary": "SPARKIT currently runs retrieval before synthesis—the LLM reasons from whatever evidence happened to be returned by question-level queries. For hard questions this creates a ceiling: the LLM may correctly identify the right mechanism in its reasoning but the evidence pool contains only weak proxies for it. RTRR inserts a new orchestration stage between initial MCQ option scoring and final blending: extract propositions from the LLM's chain-of-thought, convert each proposition to targeted search queries, run a focused retroactive retrieval round, score proposition-level support, and feed those scores back into the final blended answer with elevated weight. The result is a system where synthesis evidence is grounded specifically in claims the model chose to invoke, not just in documents loosely related to the original question.",
  "reasoning": "Hard questions on HLE-gold fail in a specific pattern: the best LLM model often identifies the correct qualitative mechanism in its reasoning trace but then picks the wrong answer because its chain-of-thought cites mechanisms that are weakly supported by the available evidence. In SPARKIT's current architecture, the MCQ option scorer calls an LLM that returns structured JSON with support_score, contradiction_score, and key_evidence_phrases. Those key_evidence_phrases are gold—they are the specific propositions the model is leaning on to justify its scoring. Today SPARKIT discards the reasoning trace after extracting scores. RTRR re-uses those phrases as retrieval seeds. For example, if the scorer reasons 'FRET efficiency depends on R^-6 distance dependence between donor-acceptor pairs' then SPARKIT should retrieve papers specifically about Förster R^-6 distance law rather than about the original question stem. This proposition-targeted evidence either corroborates the model's reasoning (confidence up, answer stable) or falsifies it (score revised, potential answer flip). The mechanism also naturally extends to free-text questions by extracting propositions from draft synthesis text. Three architectural properties of SPARKIT make RTRR directly implementable: (1) the MCQ scorer already returns structured JSON with reasoning text; (2) the adaptive retrieval loop in engine.py supports injecting additional retrieval rounds with custom query sets; (3) the blended scoring formula has a configurable evidence weight coefficient that can be overridden per evidence source.",
  "expected_impact": {
    "accuracy_delta_pct_points": 5,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add proposition_extractor() function in engine.py after MCQ option scoring (around the existing blended_option_selection block). Input: raw LLM scorer JSON text per option. Output: list[Proposition(text, option_id, confidence)]. Use a short regex + heuristic pass first (extract sentences containing 'because', 'since', 'due to', 'given that', 'is known to', 'has been shown to'). Then pass extracted sentences through a small LLM call (Gemini Flash or Claude Haiku at ~$0.001) that returns a JSON list of atomic propositions stripped of hedging language.",
      "owner": "engine.py",
      "effort": "medium"
    },
    {
      "step": "Add proposition_to_queries() in aggregator.py. Maps each Proposition.text to 1-3 retrieval query strings using the existing query_rewriter logic plus a proposition-specific transform: strip first-person modality ('may', 'could', 'might'), nominalize verb phrases ('FRET efficiency decreases with distance' → 'FRET efficiency distance dependence'), and append domain-specific qualifiers from the question's detected domain (chemistry: 'reaction mechanism', biology: 'pathway', physics: 'theoretical derivation').",
      "owner": "retrieval_service/aggregator.py",
      "effort": "low"
    },
    {
      "step": "Wire a new retrieval_round named 'retroactive_verification' in execute_orchestration(). It fires after the existing round sequence only if: (a) mode is MCQ and top two options are within SPARKIT_MCQ_BLEND_MARGIN of each other, OR (b) mode is RESEARCH_MAX. Query set = proposition_to_queries() output. Use max_results=8 per query, source mix same as primary round. Cap total cost at min(0.15 USD, remaining_budget * 0.1) to avoid blowing the budget on this stage.",
      "owner": "engine.py",
      "effort": "medium"
    },
    {
      "step": "Score retroactive evidence per proposition. For each Proposition, compute token-overlap relevance of each retroactively retrieved document against the proposition text (not the original question). Assign a proposition_support_score (0-1) and proposition_contradiction_score (0-1). Use the same contradiction keyword markers already in the verifier plus a lightweight semantic check: if the proposition appears in a document's 'Results' section with a negation prefix ('was not observed', 'failed to demonstrate', 'contradicts'), increment contradiction score.",
      "owner": "engine.py",
      "effort": "medium"
    },
    {
      "step": "Feed proposition scores back into blended option scoring. For each option, compute retroactive_support = mean(proposition_support_score) - mean(proposition_contradiction_score) across all propositions attributed to that option. Modify the blended formula: instead of fixed 0.70/0.30 LLM/lexical split, use (0.55 * llm_net_score) + (0.30 * lexical_score) + (0.15 * retroactive_support). Expose the retroactive weight as SPARKIT_RTRR_WEIGHT env var (default 0.15, range 0.0-0.30) so it can be tuned against HLE-gold.",
      "owner": "engine.py",
      "effort": "low"
    },
    {
      "step": "Add a RTRR-triggered synthesis revision flag. If the retroactive round causes an answer flip (top option changes) AND retroactive_support for the new top option is > 0.5, set synthesis_revision_pass=True and run one revision call using the new evidence as primary context. Log the flip event to the trace under a new stage 'rtrr_answer_flip' for analysis.",
      "owner": "engine.py",
      "effort": "low"
    },
    {
      "step": "Add RTRR metadata to RunTraceResponse: proposition_count, retroactive_docs_fetched, answer_flipped (bool), retroactive_support_per_option dict. Expose proposition texts in /v1/runs/{run_id}/trace artifacts so reviewers can inspect what the LLM was reasoning about.",
      "owner": "services/api_gateway/app/main.py + shared/schemas/",
      "effort": "low"
    },
    {
      "step": "Add SPARKIT_ENABLE_RTRR (default True) and SPARKIT_RTRR_MIN_MARGIN_TO_TRIGGER (default 0.10) config flags. RTRR is skipped if the top option already dominates by more than the margin, saving cost on easy questions.",
      "owner": "engine.py",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Proposition-conditioned query generation: instead of question-stem-derived queries (current), generate queries from specific factual claims extracted from the LLM's intermediate reasoning. For the question 'Which dye has the highest FRET efficiency?' the current system queries 'FRET efficiency dye comparison'—RTRR queries 'Förster radius R0 Cy3 Cy5 measured value' or 'FRET efficiency donor-acceptor distance R^-6 dependence experimental' based on what the scorer actually invoked. This narrows the retrieval target from topic to proposition.",
    "Cross-proposition deduplication and novelty gating: before running retroactive retrieval, check if the proposition query is semantically similar (Jaccard > 0.5) to any query already run in prior rounds. Skip redundant retroactive queries to avoid paying again for evidence already in the pool. This is different from the current global query deduplication (which deduplicates on query string equality) because it operates on semantic content of the proposition.",
    "Counter-proposition adversarial injection: for each extracted proposition P, automatically construct a negated query 'evidence against [P]' or 'failure mode [P]' and include it in the retroactive round at a 1:1 ratio with supporting queries. This mirrors the existing 'adversarial' round concept but is targeted at the model's own stated reasoning rather than at the original question's framing. For example, if the model states 'penicillin inhibits transpeptidase crosslinking', the adversarial retroactive query is 'penicillin resistance transpeptidase mutation mechanisms'. Documents retrieved this way become counter-evidence candidates.",
    "Source-stratified retroactive retrieval: for propositions that are mechanistic/chemical in nature (detected by presence of reaction arrows, chemical formulae, enzyme names), bias retroactive retrieval toward Crossref and arXiv chemistry categories rather than the default source mix that treats all sources equally. For biological pathways, bias toward Europe PMC. This uses the domain signals already extracted during planning (question type detection in engine.py) to improve source targeting in the retroactive stage.",
    "Proposition trust decay by LLM confidence: extract the model's hedging language around each proposition ('it is well established that' vs 'it is possible that'). High-hedging propositions get their retroactive queries run with higher max_results (10 vs 5) and lower min_quality_gain threshold for adaptive gating—the system spends more retrieval budget on uncertain claims and less on confident ones."
  ],
  "evaluation_plan": [
    "Track answer_flip_rate per HLE-gold run: what fraction of RTRR-eligible questions (top two options within SPARKIT_MCQ_BLEND_MARGIN) have their final answer changed by the retroactive stage. Cross-reference flips with ground truth correctness to compute flip_to_correct_rate and flip_to_wrong_rate. Target: flip_to_correct_rate > 2x flip_to_wrong_rate.",
    "Measure calibration impact using ECE split by RTRR_triggered (True/False). RTRR questions that were answered correctly should show higher confidence than baseline; questions where retroactive evidence failed to support any option should show lower confidence (and trigger abstain more often). Compare Brier scores between RTRR-on and RTRR-off A/B runs on HLE-25 balanced subset.",
    "Audit proposition_count vs accuracy correlation: bin HLE-gold questions by how many distinct propositions the scorer extracted (0, 1-2, 3-5, 6+). Plot accuracy by bin for RTRR-on vs RTRR-off. Expect that questions with extractable propositions (the RTRR mechanism can activate) show larger accuracy gains than questions with opaque or numeric reasoning.",
    "Retroactive document relevance quality check: for a sample of 25 HLE-gold questions post-RTRR, manually review the proposition texts extracted vs the actual answer explanation. Verify that propositions are specific (not generic) and that at least 60% of retroactively retrieved documents directly address the proposition text. This validates the proposition_to_queries() conversion step.",
    "Cost efficiency check: measure incremental cost per RTRR-triggered question (retroactive retrieval API calls + proposition extractor LLM call + optional synthesis revision LLM call). Verify that average incremental cost stays below $0.20 per question given default config. Compare accuracy-per-dollar improvement (accuracy_delta / incremental_cost) against other enhancement options (e.g., simply running an extra standard retrieval round).",
    "Regression check on easy questions: ensure SPARKIT_RTRR_MIN_MARGIN_TO_TRIGGER correctly skips RTRR when the top option dominates by > 0.10. Run HLE-25 with RTRR enabled and verify that questions already answered correctly at high confidence (> 0.75) before RTRR have zero answer flips, and that their total latency increase is < 2s (from proposition extraction only, with retroactive retrieval gated out)."
  ],
  "risks": [
    "Proposition extraction hallucination: the proposition extractor LLM call may itself confabulate propositions not actually present in the scorer reasoning. If extracted propositions are spurious, the retroactive retrieval round wastes budget and may introduce misleading counter-evidence. Mitigate by requiring propositions to be verbatim substring-matchable to the scorer's raw output text before they are accepted.",
    "Budget exhaustion on hard questions: RTRR adds a retrieval round (8 docs × 3-5 queries) plus LLM extraction and optional revision. For runs already near the max_cost_usd limit this may get capped mid-round, leaving partial retroactive evidence that skews scores without the full retrieval benefit. The cost cap (min(0.15 USD, remaining_budget × 0.1)) mitigates this but may still cause partial execution edge cases.",
    "Latency regression for close-margin MCQ questions: RTRR is triggered specifically when the top options are close (within SPARKIT_MCQ_BLEND_MARGIN), which are exactly the questions users care most about getting right. These questions will see 15-30s additional latency from the retroactive round. If the max_latency_s constraint is tight, RTRR may be forced to skip, defeating its purpose on constrained runs.",
    "Retroactive evidence circularity: if the LLM scorer invokes a proposition that is directly stated in the original question stem (e.g., 'FRET requires close proximity of donor and acceptor' from a question that already says 'given that FRET operates at nanometer distances'), retroactive retrieval returns documents already likely to be in the evidence pool. The cross-proposition deduplication check mitigates this but does not eliminate it entirely.",
    "Scoring formula sensitivity: the new 3-term blended formula (0.55/0.30/0.15 split) is chosen heuristically. If the retroactive_support signal is noisy (low-quality propositions, sparse document returns), it could destabilize the final score for questions that the existing 2-term formula handled correctly. The SPARKIT_RTRR_WEIGHT env var allows rapid adjustment, but the right value will require systematic grid search against HLE-gold rather than being predictable in advance.",
    "Inconsistency between MCQ and free-text handling: the initial implementation targets MCQ questions (where option-attributed propositions are cleanly separable). Free-text question support requires extracting propositions from draft synthesis text rather than structured scorer JSON, which is a harder NLP problem. If free-text RTRR is shipped prematurely, proposition extraction quality will be lower, creating a heterogeneous improvement profile that complicates calibration."
  ]
}
```
