```json
{
  "proposal_id": "SPARKIT-P-DISC-01",
  "title": "Option-Discriminative Gap-Filling Retrieval with Corpus-Aware BM25 Passage Reranking",
  "distinctive_angle": "After round 1 ingestion, compute per-option dossier_score gaps; for option pairs whose scores are within a configurable margin or both below a floor, emit targeted pairwise-discriminative queries designed to find evidence that separates the two options rather than supporting either in isolation. Simultaneously replace _select_best_section_chunk's single-record token-overlap scoring (engine.py:1479) with a BM25 index built over the full retrieved-document corpus, scoring each section chunk against both the question stem and all surviving option texts jointly.",
  "summary": "SPARKIT's MCQ evidence pipeline assigns retrieved passages to options via token overlap (_build_option_evidence_packs, engine.py:380-384; _build_option_dossiers, engine.py:410-419). This fails when options use synonyms or domain jargon absent from the question stem. Retrieval is also strictly front-loaded: if round 1 yields sparse dossiers for some options, no feedback loop injects targeted follow-up queries. This proposal adds four mechanisms: (1) post-round-1 per-option dossier gap detection that gates a dedicated fill round, (2) pairwise discriminative query generation for option pairs within a margin, (3) BM25 corpus-level passage reranking replacing per-record lexical scoring, and (4) cross-source claim corroboration scoring that down-weights claims appearing in only one source. Together these directly address the root causes of wrong answers on hard STEM MCQ: stale evidence distribution and lexical retrieval blindspots.",
  "reasoning": "Hard benchmark questions (HLE biology/chemistry) fail for three identifiable reasons in SPARKIT: (a) The passage selected from a paper is the one with the most lexical overlap with the question, but for hard questions the most relevant section is often a Results table or Methods subsection whose vocabulary diverges from the question stem. BM25 across the full corpus naturally down-weights common terms and up-weights rare technical terms that actually discriminate. (b) Option evidence dossiers (engine.py:427-432) are populated by the same round-1 retrieval regardless of whether any retrieved paper actually speaks to option A vs B; for hard distractors this means dossier_scores for the correct and incorrect options are often nearly equal and synthesis picks wrong. A discriminative gap query for the option pair 'X vs Y' searches directly for comparative studies rather than hoping that generic evidence falls out. (c) The verifier (verifier.py) uses keyword spotting and therefore cannot detect when two retrieved papers make numerically inconsistent claims about the same phenomenon. Cross-source corroboration scoring catches this without an LLM pass by flagging claims that appear in only one source and suppressing their contribution to dossier_score.",
  "expected_impact": {
    "accuracy_delta_pct_points": 4,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _compute_option_dossier_gaps(dossiers: dict, gap_threshold: float = 1.5) -> list[tuple[str,str]] in engine.py. Returns ordered list of (labelA, labelB) pairs where abs(dossier_score_A - dossier_score_B) < gap_threshold or max(score_A, score_B) < 1.0. This is the trigger signal for the fill round.",
      "owner": "orchestrator-engine",
      "effort": "low"
    },
    {
      "step": "Add _build_pairwise_discriminative_queries(stem: str, answer_choices: dict, gap_pairs: list[tuple[str,str]], planning_provider: str, max_queries: int = 6) -> list[str] in engine.py. For each pair, prompt the planning_provider with: 'Generate a 12-term scholarly search query that would find comparative evidence distinguishing [{choice_A}] from [{choice_B}] in the context of [{stem}]. Return only the query text.' De-duplicate results with _dedupe_queries.",
      "owner": "orchestrator-engine",
      "effort": "medium"
    },
    {
      "step": "Inject discriminative gap queries into the adaptive retrieval loop (engine.py:1321-1454). After round 1 completes and option_dossiers are first computed (currently deferred to line 1643), do an early preliminary dossier computation using abstract-only claim_texts. If gap_pairs is non-empty and adaptive gate has not triggered, insert a synthetic 'retrieval_round_discriminative' stage using the gap queries before round 2. Gate behind SPARKIT_DISCRIMINATIVE_RETRIEVAL env var (default 1).",
      "owner": "orchestrator-engine",
      "effort": "medium"
    },
    {
      "step": "Build a corpus-level BM25 index in _select_records_for_ingestion (engine.py:1457). After deduplication, build an in-process BM25 index (using rank_bm25 library, already common in Python ML stacks, or a simple TF-IDF via collections.Counter if no new deps are allowed). Score each candidate section chunk against (question + ' ' + ' '.join(surviving_option_texts)) rather than the question alone. Replace the current 1.4x question_overlap + 1.9x focus_term_overlap heuristic in _select_best_section_chunk with BM25 scores. Gate behind SPARKIT_BM25_PASSAGE_RERANK env var (default 1).",
      "owner": "orchestrator-engine",
      "effort": "medium"
    },
    {
      "step": "Add cross-source claim corroboration scoring. After all claims are extracted (engine.py:1507), build a token-fingerprint index: for each claim_text, compute a frozenset of its top-8 content tokens. For each claim, count how many other claims share >= 3 tokens (same factual assertion from a different source). Claims with corroboration_count == 0 have their support_score reduced from 0.8 to 0.55 in evidence_store.insert_claim. Claims with corroboration_count >= 2 get support_score boosted to 0.92. Update base_claim_conf dict accordingly.",
      "owner": "orchestrator-engine",
      "effort": "low"
    },
    {
      "step": "Augment MCQ blended scoring (engine.py:1690) with a discriminative_bonus term. For each label that appeared in a winning discriminative-round dossier (i.e., its pairwise query returned a document that scored higher for it than for its competitor), add +0.15 to its blended score before the _select_confident_blended_option call. Store the discriminative_round_winners set in orchestration context.",
      "owner": "orchestrator-engine",
      "effort": "low"
    },
    {
      "step": "Add SPARKIT_DISCRIMINATIVE_GAP_THRESHOLD (default 1.5), SPARKIT_DISCRIMINATIVE_MAX_PAIRS (default 3), and SPARKIT_BM25_PASSAGE_RERANK (default 1) to configuration.md and the env var registry.",
      "owner": "docs-config",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Pairwise discriminative gap queries: for option pairs with dossier_score delta < SPARKIT_DISCRIMINATIVE_GAP_THRESHOLD, emit queries of the form 'comparative evidence [{option_A_key_entity}] versus [{option_B_key_entity}] [{domain_context}]' targeting the exact decision boundary rather than broad question-stem retrieval. These are routed through the existing search_literature() pipeline including arXiv, Semantic Scholar, and Brave (when enabled), maximizing the chance of finding comparative studies or reviews that directly address both options.",
    "BM25 corpus-level passage reranking in _select_best_section_chunk: replace the current (1.4 * question_overlap + 1.9 * focus_term_overlap) lexical heuristic with BM25 scoring across all section chunks from all ingested documents jointly. BM25 naturally penalizes ubiquitous domain terms (e.g., 'protein', 'cell') and boosts rare discriminative terms (e.g., specific enzyme names, reaction mechanisms), surfacing the section of a paper that contains the actual finding rather than the abstract which merely restates the question.",
    "Early preliminary dossier computation using abstract-only text after round 1 to detect evidence gaps before committing to round 2 query budget. Currently, option_dossiers are not computed until after all retrieval rounds complete (engine.py:1643), so the adaptive gate (engine.py:1398-1454) cannot account for per-option evidence quality. By computing preliminary dossiers after round 1 using record.abstract only, the system can steer round 2 queries toward discriminative gap-filling rather than generic replication of round 1.",
    "Cross-source corroboration index used as a retrieval quality signal: claims appearing in only one source are downweighted in calibration, creating implicit pressure for retrieval to surface independent corroborating sources. This steers the source diversity cap (_limit_source_dominance, aggregator.py:193) to be more effective by making single-source concentration visible in calibration output.",
    "Discriminative gap queries bypass the 18-term relaxation limit (_relax_query, aggregator.py:48) since they are already targeted and concise. Their results are tagged with source='discriminative' in the trace so cost accounting (Brave per-request) can be tracked separately via the existing stats dict."
  ],
  "evaluation_plan": [
    "Before/after accuracy on HLE-149 hardest-quintile questions (the 30 questions where direct-call baselines from all providers are wrong): run SPARKIT with SPARKIT_DISCRIMINATIVE_RETRIEVAL=0 and SPARKIT_BM25_PASSAGE_RERANK=0 versus both flags enabled. Report per-question delta and aggregate accuracy change. Target: +3-6 pp on this subset.",
    "Option dossier discrimination rate: after each MCQ run, log max(dossier_score) - second_max(dossier_score) (the margin). Compare margin distributions between baseline and proposal runs. The proposal should increase mean margin by >= 20% on questions where the correct answer is eventually selected, indicating clearer evidence separation.",
    "Passage reranking precision check: for 20 held-out HLE questions with known correct answers, manually inspect the section chunk selected by _select_best_section_chunk under both lexical and BM25 modes. Count how many times the BM25-selected chunk contains a direct factual statement relevant to the correct answer vs. only background context.",
    "Cross-source corroboration rate: log corroboration_count distribution across all claims in a 50-question run. Measure what fraction of correctly-answered questions had at least one corroborated claim (corroboration_count >= 2) versus incorrectly-answered questions. This validates whether corroboration is a meaningful signal.",
    "Discriminative query yield rate: for runs where gap queries are triggered, measure what fraction of discriminative-round documents have a BM25 score above the median of round-1 documents (i.e., are they actually more targeted). If yield rate < 40%, the query generation prompt needs revision.",
    "Calibration ECE before/after: run the existing eval_service metrics.py ECE computation on a 50-question subset. Verify that corroboration-adjusted support_scores produce better-calibrated confidence estimates (lower ECE) without increasing abstention rate on answerable questions."
  ],
  "risks": [
    "Discriminative query generation requires a planning_provider LLM call per gap pair, adding latency and token cost. With SPARKIT_DISCRIMINATIVE_MAX_PAIRS=3, this is at most 3 additional generate_text() calls of ~60 tokens output each. At Claude Opus pricing this adds roughly $0.03-0.05 per question. Mitigate by caching discriminative query outputs when the same option pair appears across question reruns in batch evaluation.",
    "BM25 index construction over the full retrieved corpus (up to ingestion_target_docs=16 documents, each potentially 50+ sections) may add 200-400ms latency in the ingestion phase. Mitigate by limiting the BM25 index to the top-20 section chunks per document by character length, and building the index only when SPARKIT_BM25_PASSAGE_RERANK=1.",
    "Early preliminary dossier computation using abstract-only text may trigger gap queries for options that actually have good support in full-text that wasn't visible at abstract stage. This could send redundant discriminative queries. Mitigate by setting a lower preliminary gap threshold (e.g., 0.8) than the final threshold (1.5) so only severe gaps trigger the fill round.",
    "Pairwise discriminative queries for hard science MCQ may still fail to retrieve relevant papers if neither option text contains standard scholarly terminology (common for HLE questions that use colloquial descriptions of phenomena). Add a fallback: if discriminative round yields 0 new unique docs, log and skip rather than blocking.",
    "Cross-source corroboration scoring penalizes claims from genuinely novel or recent findings that have not yet been replicated (e.g., a 2024 preprint). For such questions, the correct answer may depend on exactly this single-source claim. Mitigate by exempting claims from records with year >= current_year - 1 from the corroboration penalty, preserving their original support_score=0.8."
  ]
}
```
