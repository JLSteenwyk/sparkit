```json
{
  "proposal_id": "SPARKIT-EGB-001",
  "title": "Iterative Evidence-Gap Bridging with Falsification-First Retrieval",
  "distinctive_angle": "Rather than generating all retrieval queries upfront and running them in fixed rounds, SPARKIT should dynamically analyze which sub-claims remain unsupported after each round and synthesize targeted gap-filling queries. Then, once a draft answer exists, a dedicated falsification round actively searches for evidence that would refute it. Correctness is earned by surviving targeted falsification—not by accumulating maximum supporting evidence volume. This inverts the usual retrieval-then-synthesize paradigm and is calibrated specifically against SPARKIT's adaptive gating architecture.",
  "summary": "This proposal adds two tightly-scoped interventions to SPARKIT's existing multi-round retrieval loop in engine.py (lines 1321–1454) and its MCQ/free-form synthesis pipeline (lines 1636–1854). First, after each completed retrieval round, a lightweight LLM call (or token-overlap heuristic fallback) identifies which question sub-claims have zero or low supporting evidence in the currently-selected record pool, then generates 2–4 targeted follow-up queries for the next round—replacing SPARKIT's current static round-plan. Second, immediately after draft synthesis but before calibration, a 'falsification round' is injected that queries for contradicting evidence against the specific draft answer, re-runs the verifier on that evidence, and—if high-confidence contradiction is found—triggers re-synthesis with the falsifying evidence added to the dossier. Supporting this, structured LLM-based claim extraction replaces the current first-sentence heuristic (engine.py line 1476–1517) to yield richer, quantitatively-grounded claims for STEM questions.",
  "reasoning": "SPARKIT's current retrieval loop pre-plans queries at the retrieval-planning stage (engine.py:674–750) and then executes them in rounds with adaptive gating that stops on low novelty or low quality gain. This works for broad coverage but fails for hard questions that require precise, highly specific evidence: if the initial query set misses the relevant narrow technical concept, subsequent rounds execute the same planned queries and the gap persists. The falsification gap is equally critical: SPARKIT's verifier (verifier.py:28–82) scores adversarial records by marker-word hit counts, not by semantic relationship to the specific draft answer. This means a wrong answer that happens to have high corpus support can pass verification and be returned with high confidence. For hard STEM and factual QA (HLE-gold style), both gaps—undiscovered supporting evidence and unchallenged wrong answers—are the dominant failure modes. The proposed interventions address both directly within SPARKIT's existing pipeline without requiring architectural upheaval: evidence-gap bridging slots into the existing adaptive-gating loop, falsification sits between synthesis and calibration, and structured claim extraction replaces a single function.",
  "expected_impact": {
    "accuracy_delta_pct_points": 9,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add `_identify_evidence_gaps(question, sub_claims, selected_records, focus_terms)` to engine.py. For each sub-claim from the decomposition (or heuristically-derived segment), compute max token overlap with any selected record's title+abstract. Sub-claims with max_overlap < 0.12 are flagged as 'unsupported gaps'. Return list of (gap_description, gap_query) pairs. Use LLM call (with heuristic fallback identical to existing decomposition fallback at engine.py:596–627) to generate 2–4 targeted queries per gap.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify the multi-round retrieval loop body (engine.py:1380–1420) so that after rounds >= adaptive_min_rounds, gap queries returned by `_identify_evidence_gaps` are appended to the next round's query list before `search_literature()` is called. Cap gap queries at 4 per round to bound cost. Preserve existing adaptive gating logic: the gating decision runs after gap-augmented results are merged.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add `_run_falsification_round(draft_answer, question, selected_records, providers, budget_guard)` to engine.py. This function: (a) generates 3 falsification queries of the form 'evidence against [draft_answer]', 'why [draft_answer] is wrong [question_domain]', '[alternative_correct_answer] [question_domain]'; (b) calls search_literature() with these queries; (c) runs the existing verifier on the results with contradiction markers tuned for direct negation of draft_answer; (d) returns falsification_score (0–1) and top falsifying snippets. Insert this call between synthesis exit (engine.py:1823 / 1854) and calibration entry (engine.py:2130).",
      "owner": "orchestrator/engine.py + verifier.py",
      "effort": "high"
    },
    {
      "step": "In the calibration formula (calibration.py:20–36), add `falsification_score` as a new feature: subtract `0.12 * falsification_score` from the raw confidence before clamping. If falsification_score > 0.7, set a new uncertainty_reason flag `high_falsification_risk` and trigger re-synthesis by returning a sentinel that the synthesis dispatcher catches and re-routes through the existing fallback chain (engine.py:1808–1822) with falsifying snippets prepended to the dossier.",
      "owner": "orchestrator/calibration.py + engine.py",
      "effort": "medium"
    },
    {
      "step": "Replace the first-sentence claim extraction heuristic (engine.py:1472–1517) with `_extract_structured_claims(section_text, question, model)`. This LLM call receives the ingested section and question and returns structured JSON: [{claim_text, claim_type (mechanism|numerical|comparison|causal), quantitative_value (nullable), confidence_qualifier (e.g. 'statistically significant', 'reported', 'suggested')}]. Degrade gracefully: if the LLM call fails or times out, fall back to the existing first-sentence heuristic. Cap the LLM call at 256 output tokens. Budget: 1 call per ingested document, counted against the synthesis stage cost.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add cross-document claim triangulation in `_build_claim_clusters` (engine.py:918–939). After clustering, for each cluster of 3+ documents: (a) extract the key numerical values mentioned in their structured claims, (b) flag the cluster as 'triangulated' if >= 3 sources report the same value within 5% relative tolerance, (c) flag as 'disputed' if >= 2 sources report conflicting values. Pass triangulation flags into the synthesis prompt as a new 'convergence_evidence' block, and into calibration as a +0.08 bonus (triangulated) or -0.06 penalty (disputed).",
      "owner": "orchestrator/engine.py + calibration.py",
      "effort": "medium"
    },
    {
      "step": "Add query-type routing in `_decompose_retrieval` (engine.py:674–750). Classify the expected answer type from the question: numerical_value (regex for numbers/units in stem), causal_mechanism (keywords: 'why', 'causes', 'drives', 'leads to'), comparison ('better', 'advantage', 'difference between'), definition ('what is', 'define'). Map each type to a specialized query template set that augments the existing intent_queries dict. Example for numerical_value: add 'experimental measurement [entity] value', '[entity] reported range literature'. For causal_mechanism: add '[subject] pathway mechanism review', '[effect] causal factor meta-analysis'.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add `falsification_score`, `triangulation_bonus`, `disputed_clusters` to the run_observability_metrics table (via Alembic migration) and log them in observability.py. Emit these fields in the final response under quality_gates so the eval harness can track them per-question.",
      "owner": "orchestrator/observability.py + alembic/migrations",
      "effort": "low"
    },
    {
      "step": "Add unit tests for: _identify_evidence_gaps (zero-overlap detection, query count cap), _run_falsification_round (mock retrieval returns contradicting snippet, check falsification_score > 0.5), _extract_structured_claims (LLM mock returns structured JSON, fallback on timeout), triangulation detection (3-source same-value cluster detection, disputed flag on divergent values), query-type routing (numerical vs causal regex classification).",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Evidence-gap bridging: After each retrieval round, compute per-sub-claim coverage as max token overlap against the current selected record pool. Sub-claims below a coverage threshold (0.12 by default, tunable via SPARKIT_GAP_COVERAGE_THRESHOLD) generate targeted follow-up queries injected into the next round, ensuring narrow technical concepts that the initial query set missed are explicitly hunted in subsequent rounds rather than hoping round-over-round query repetition accidentally surfaces them.",
    "Falsification-first retrieval round: A dedicated search pass executes after draft synthesis, using the specific draft answer text to construct queries that actively seek contradicting evidence. Because queries are anchored to the concrete draft answer rather than the original question, they retrieve documents that directly refute that answer—not just documents generally related to the question. This catches cases where the corpus has clear refutations that the question-anchored query set never surfaced.",
    "Answer-type-specialized query templates: The retrieval planner classifies the expected answer type (numerical, causal, comparison, definition) from the question stem and maps each type to specialized query templates beyond the existing generic intent_queries dict. Numerical questions get queries targeting experimental measurement ranges and reported values; causal questions get mechanism review and meta-analysis queries; comparison questions get queries explicitly framed as 'A vs B advantage'. This improves precision of the retrieved set for hard STEM questions where the question type tightly constrains the relevant evidence form.",
    "Structured LLM claim extraction replacing first-sentence heuristics: The current claim extraction takes the first sentence of the best-matched section, which often contains only boilerplate or section headers for technical papers. Replacing this with a structured LLM extraction call that identifies the key empirical claim, its quantitative value, and its confidence qualifier provides the synthesis stage with richer, more discriminative evidence—especially critical for questions where the correct answer hinges on a specific reported value or a precisely stated mechanism.",
    "Cross-document numerical triangulation: After ingestion, structured claims are compared across documents in the same cluster. If 3+ independent sources report a numerical value within 5% tolerance, the cluster is flagged as triangulated and a convergence_evidence block is injected into the synthesis prompt. This gives the synthesis LLM an explicit signal that the evidence is robust and not cherry-picked from a single source, while disputed clusters (conflicting values across sources) trigger additional targeted retrieval before synthesis proceeds."
  ],
  "evaluation_plan": [
    "Re-run the full HLE-gold bio-chem benchmark with the evidence-gap bridging enabled vs. disabled (A/B), measuring per-question accuracy, answer_confidence, and support_coverage. For questions where the correct answer required a narrow technical sub-claim (identifiable by post-hoc retrieval analysis), measure the fraction where gap-bridging queries were the only round that retrieved the decisive document.",
    "Inject synthetic adversarial questions into the eval harness where the correct answer is factually supported but the wrong answer also has broad corpus support (e.g., common misconceptions in chemistry). Measure whether the falsification round correctly lowers confidence and triggers re-synthesis for the wrong-answer-high-support case. Target: falsification_score > 0.5 for at least 70% of adversarial injections where SPARKIT's draft answer was incorrect.",
    "Measure structured claim extraction quality by sampling 50 HLE-gold questions and manually comparing the extracted claim_text from the LLM call vs. the first-sentence heuristic against the gold-standard answer. Score each on: (a) contains the key discriminating fact, (b) includes the relevant quantitative value if present, (c) does not hallucinate content not in the source section. Target: LLM extraction should win on criteria (a) and (b) for >= 65% of sampled questions.",
    "Track triangulation_bonus and disputed_clusters per question across the benchmark. For questions where triangulation_bonus was awarded (>= 3 sources agree), measure accuracy rate vs. baseline. For disputed_clusters questions, measure whether the additional retrieval triggered by the dispute flag improved or maintained accuracy. Expected: triangulated questions should have >= 5 points higher accuracy than non-triangulated questions of similar difficulty.",
    "Run a latency and cost profiling pass on the full pipeline with all interventions enabled. Measure: (a) p50/p95 added latency from gap-bridging LLM calls, (b) p50/p95 added latency from falsification round, (c) added cost per question from structured claim extraction LLM calls. Confirm that the combined additions stay within the existing max_cost_usd budget ($3.00 default) for at least 90% of HLE-gold questions in ROUTED mode, and document the required budget increase for RESEARCH_MAX mode.",
    "Calibration quality check: Compute Brier Score and ECE separately for the subset of questions where falsification_score > 0.5 (high falsification risk) vs. < 0.2 (low risk). If the calibration formula's -0.12 * falsification_score adjustment is well-tuned, the high-risk subset should show lower answer_confidence values but not lower accuracy (i.e., the system should be appropriately uncertain, not incorrectly penalized). Flag for re-tuning if ECE for the high-risk subset exceeds 0.15."
  ],
  "risks": [
    "Evidence-gap bridging adds a sequential LLM call per retrieval round (to identify gaps and generate follow-up queries), which increases per-question latency by approximately the LLM generation time for that call (estimated 1–4s per round at ROUTED mode provider speeds). For questions with 4 retrieval rounds, worst-case added latency could exceed 12–16s. Mitigation: implement the heuristic fallback (token-overlap-based gap detection with template queries, no LLM call) as the default, and gate the LLM gap-analysis call behind a new SPARKIT_GAP_ANALYSIS_LLM env flag defaulting to 0.",
    "The falsification round adds a fifth retrieval pass plus a synthesis LLM call unconditionally after every synthesis, increasing per-question cost by approximately $0.05–$0.20 depending on the synthesis provider. For the default $3.00 max_cost_usd budget, this is a significant fraction. Mitigation: gate the falsification round behind a budget check (only run if remaining_budget > 0.25 USD after synthesis), and skip if answer_confidence is already < 0.35 (the answer is already flagged as uncertain).",
    "Structured LLM claim extraction introduces one LLM call per ingested document. At 10–14 documents per question (ingestion_target_docs default), this adds 10–14 LLM calls per question. Even at Haiku-class pricing, this is non-trivial cost overhead. Mitigation: cap structured extraction to the top 5 documents by relevance score, run extraction calls in parallel (they are independent), and use the cheapest available provider (not the synthesis provider) for extraction.",
    "Falsification queries anchored to the draft answer could introduce answer-leakage bias into the retrieval set: if the falsification round mostly fails to find contradicting evidence (which is the expected case when the draft answer is correct), the absence of falsification evidence could be misinterpreted as confirmation. The calibration adjustment must be carefully tuned so that low falsification_score does NOT boost confidence, only high falsification_score suppresses it. Mitigation: implement the calibration adjustment as a one-sided penalty (subtract when high, no bonus when low), enforced in calibration.py.",
    "Cross-document numerical triangulation requires parsing quantitative values from structured claim text, which is noisy for domain-specific units and notation (e.g., '10^-3 mol/L', 'T_1 relaxation ~1.2ms ± 0.3ms'). A naive regex-based parser will have low recall for hard STEM questions. Mitigation: implement a conservative triangulation check that only fires when values are cleanly parseable as floats; otherwise treat the cluster as 'inconclusive' rather than 'disputed'. This reduces false disputed flags at the cost of lower triangulation bonus coverage.",
    "Query-type classification by regex on the question stem will misclassify complex or multi-part questions (e.g., a question asking for a numerical value using qualitative phrasing like 'which is larger'). Misclassification causes the wrong query template set to be applied, potentially worsening retrieval precision. Mitigation: treat query-type routing as an additive augmentation of the existing intent_queries dict, not a replacement. The existing primary/options/methods/adversarial/reference queries run regardless of type classification, so a wrong classification only fails to add the specialized queries rather than removing valid ones."
  ]
}
```
