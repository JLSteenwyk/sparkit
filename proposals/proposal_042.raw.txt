```json
{
  "proposal_id": "SPARKIT-2026-CCRF-001",
  "title": "Claim-Conditioned Reflexive Retrieval with Post-Synthesis Faithfulness Anchoring",
  "distinctive_angle": "SPARKIT currently treats retrieval and synthesis as sequential one-pass stages. This proposal inserts a structured claim-extraction layer between ingestion and synthesis, then closes the loop by running a draft synthesis first, identifying knowledge gaps in the draft, firing reflexive targeted queries against those gaps, and enforcing per-sentence faithfulness anchoring on the final answer — turning SPARKIT's linear pipeline into a self-correcting evidence loop without requiring new external APIs.",
  "summary": "Add four tightly-integrated components to the SPARKIT orchestration pipeline: (1) a Claim Extraction pass that converts raw ingested section text into structured factual assertions; (2) a Reflexive Gap Query generator that inspects a scratch-pad synthesis draft and emits targeted search queries for any asserted fact not grounded in extracted claims; (3) a Contrastive Option Retrieval stage for MCQ questions that fires cross-option discriminative queries when the blended scorer margin falls below SPARKIT_MCQ_BLEND_MARGIN; and (4) a Post-Synthesis Faithfulness Anchor pass that matches every sentence of the final answer to its nearest extracted claim, removes or qualifies sentences below a similarity floor, and annotates the OrchestrationResult with per-sentence provenance.",
  "reasoning": "SPARKIT's two hardest failure modes on HLE questions are (a) synthesis hallucination — the synthesizer asserts facts not in the retrieved evidence because raw section text is long, ambiguous, and mixed with irrelevant prose, making it easy for the LLM to confabulate; and (b) retrieval blindness — adaptive retrieval stops when it stops finding new documents, but it has no knowledge of what the synthesis draft needs that it has not found. Extracting explicit claims first compresses evidence into verifiable atomic facts that are harder to confabulate around. Reflexive gap queries give retrieval a semantic target ('find evidence for X') rather than a count target ('find N new docs'), directly closing the accuracy gap caused by retrieval terminating before all synthesis-required facts are covered. Contrastive MCQ retrieval addresses the specific pattern where HLE biology/chemistry questions have two plausible options separated by a single mechanistic detail — a detail that only surfaces when the retrieval system is explicitly asked to discriminate between options rather than score each independently. Faithfulness anchoring enforces that the final answer is strictly entailed by retrieved claims, converting a probabilistic synthesis into a citation-grounded extraction.",
  "expected_impact": {
    "accuracy_delta_pct_points": 11,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add `_extract_claims_from_passage(passage_text, source_record_id, provider) -> list[ExtractedClaim]` in engine.py. Prompt: 'Extract all falsifiable factual claims from this passage as a JSON list: [{fact, domain_tag, confidence}]. Omit hedges and method descriptions. Max 8 claims.' Call with the synthesis provider after Stage 4 ingestion, one call per ingested document. Cache results in the evidence store under a new `extracted_claims` column. Use the cheapest available provider (Mistral or Haiku) to keep cost low.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add `_generate_scratch_synthesis(question, claims, provider) -> str` that calls the synthesis provider with only the extracted claims (not raw section text) to produce a fast first-pass draft answer. This draft is internal and never returned to the caller. It runs before Stage 8 (Synthesis) using at most 512 output tokens.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add `_identify_unsupported_assertions(draft_text, extracted_claims) -> list[str]` that prompts an LLM: 'Below is a draft answer and a set of evidenced claims. List every specific factual assertion in the draft that cannot be directly inferred from any provided claim. Return as a JSON list of assertion strings.' These unsupported assertion strings become the seed for reflexive queries.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add `_build_reflexive_queries(unsupported_assertions, question) -> list[str]` that converts each unsupported assertion into 1-2 targeted search queries using a prompt template: 'Convert this assertion into a precise literature search query that would retrieve papers confirming or refuting it: {assertion}'. Cap at 6 total reflexive queries. Insert these into a new retrieval round `retrieval_round_4_reflexive` inside the adaptive retrieval loop, gated by `SPARKIT_ENABLE_REFLEXIVE_QUERIES=1` env flag and only if `len(unsupported_assertions) > 0`.",
      "owner": "orchestrator/engine.py + retrieval_service/aggregator.py",
      "effort": "medium"
    },
    {
      "step": "Extend `_select_confident_blended_option()` to detect when the margin between the top-2 blended scores is less than `SPARKIT_MCQ_CONTRASTIVE_MARGIN` (default 0.10). When triggered, call `_build_contrastive_option_queries(top_option_a, top_option_b, question_stem)` which emits queries of the form: 'mechanism distinguishing {text_of_A} from {text_of_B}', '{stem} evidence against {text_of_A}', '{stem} exclusive to {text_of_B}'. Run these through `search_literature()` and re-score only the top-2 options using the new evidence before returning the final selection.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add `_faithfulness_anchor_pass(answer_text, extracted_claims, provider) -> FaithfulnessResult` that splits the final synthesis output into sentences and for each sentence calls a lightweight LLM check: 'Is this sentence directly supported by any of these claims? Answer YES or NO and give the supporting claim text if YES.' Sentences that receive NO are either (a) removed if `SPARKIT_FAITHFULNESS_STRICT=1`, or (b) suffixed with ' [unanchored — retrieved evidence does not directly support this claim]' in default mode. Attach per-sentence provenance to `OrchestrationResult.quality_audit`.",
      "owner": "orchestrator/engine.py",
      "effort": "high"
    },
    {
      "step": "Add `SPARKIT_ENABLE_CLAIM_EXTRACTION`, `SPARKIT_ENABLE_REFLEXIVE_QUERIES`, `SPARKIT_MCQ_CONTRASTIVE_MARGIN`, `SPARKIT_FAITHFULNESS_STRICT` env flags to docs/configuration.md and validate them in the engine startup checks.",
      "owner": "docs/configuration.md + engine.py",
      "effort": "low"
    },
    {
      "step": "Extend `test_synthesis_quality.py` with unit tests covering: `_extract_claims_from_passage` output schema validation, `_identify_unsupported_assertions` correct isolation of hallucinated facts, `_build_reflexive_queries` query count cap, `_build_contrastive_option_queries` output format, `_faithfulness_anchor_pass` strict vs. annotate mode behavior.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Reflexive Gap Queries: After the scratch-pad synthesis draft identifies facts not grounded in extracted claims, SPARKIT fires targeted queries specifically for those unsupported assertions rather than relying on count-based adaptive retrieval termination. This changes the stopping criterion from 'are we finding new documents?' to 'do we have evidence for every claim in the expected answer?' — a semantically meaningful target.",
    "Contrastive MCQ Option Retrieval: When the blended scorer margin between the top-2 options is narrow (below SPARKIT_MCQ_CONTRASTIVE_MARGIN), SPARKIT fires cross-option discriminative queries ('mechanism distinguishing X from Y') that exploit the academic literature's tendency to frame key distinctions as comparative studies. This improves retrieval precision for exactly the hard cases where SPARKIT currently returns an uncertain margin.",
    "Claim-as-Query Expansion: Extracted structured claims from ingested documents become first-class query seeds for subsequent retrieval rounds. A claim like 'inhibitor X reduces Y activity by 40% via Z pathway' spawns queries like 'Z pathway inhibition confirmation' and 'X inhibitor independent replication' — finding corroborating or refuting follow-on papers that the original intent-query plan would not reach.",
    "Evidence Compaction for Section Chunk Selection: Replacing `_select_best_section_chunk`'s single-chunk output with a claim-extraction pass across all sections allows SPARKIT to surface key facts from methods, results, and discussion sections simultaneously. Currently a question about a numerical result may score the abstract highest while the actual measurement is buried in the results section — claim extraction retrieves across all section types regardless of lexical overlap with the question.",
    "Faithfulness-Driven Reretrieval: If the faithfulness anchor pass finds that more than 30% of sentences in the final answer are unanchored, it triggers a final emergency retrieval round using those unanchored sentences as queries. This creates a backstop against confident hallucination in domains where the initial retrieval was sparse."
  ],
  "evaluation_plan": [
    "Anchoring Rate Metric: After implementing faithfulness anchor pass, report `anchored_sentence_ratio` (anchored sentences / total sentences) as a new field in OrchestrationResult and aggregate it across all HLE-25 benchmark runs. Compare anchoring rate between SPARKIT with and without claim extraction to verify the claim layer increases anchoring by at least 15 percentage points.",
    "Reflexive Query Hit Rate: Instrument `retrieval_round_4_reflexive` to log whether each reflexive query retrieved at least one new document not seen in prior rounds. Report `reflexive_hit_rate` = (queries that found new docs) / (total reflexive queries). A hit rate above 50% validates that the gap-detection logic is generating meaningful queries rather than noise.",
    "MCQ Contrastive Margin Improvement: For all HLE MCQ questions where the blended scorer margin was below SPARKIT_MCQ_CONTRASTIVE_MARGIN before contrastive retrieval, measure the margin after contrastive retrieval and the accuracy on those questions. Compare accuracy on low-margin MCQ questions with vs. without contrastive retrieval across the HLE-25 balanced subset.",
    "Ablation: Claim Extraction Only vs. Full Pipeline: Run HLE-25 with four configurations: (a) baseline SPARKIT, (b) SPARKIT + claim extraction only, (c) SPARKIT + claim extraction + reflexive queries, (d) SPARKIT + all four components. Report accuracy at each step to isolate contribution of each component and confirm additive gains.",
    "Hallucination Rate Audit: For 20 randomly sampled HLE questions from the full benchmark, manually annotate each sentence in the synthesis output as (supported, partially-supported, hallucinated) against the retrieved evidence. Compare hallucination rate before and after faithfulness anchor pass. Target: reduce hallucinated sentences from estimated 20-30% baseline to below 10%.",
    "Cost vs. Accuracy Pareto: Plot accuracy on HLE-25 against total cost per question for: direct call only, SPARKIT baseline, SPARKIT + claim extraction (Mistral-only for extraction), SPARKIT + full pipeline (Mistral extraction + synthesis provider faithfulness check). Confirm the full pipeline does not exceed 2x baseline SPARKIT cost while delivering measurable accuracy gain."
  ],
  "risks": [
    "Claim extraction LLM calls add 1 call per ingested document: with ingestion_target_docs=10-16, this adds 10-16 LLM calls per question. Using Mistral-large (cheapest available provider) at approximately $0.002 per call keeps additional cost at ~$0.03 per question, but if claim extraction uses the synthesis provider (claude-opus-4-6 at $0.015/call) the cost increase is ~$0.24 per question — a meaningful fraction of the $3.00 budget. Must hard-route claim extraction to Mistral or DeepSeek.",
    "Reflexive gap query detection depends on LLM accuracy in identifying unsupported assertions: if the LLM incorrectly classifies supported facts as unsupported (false positives), SPARKIT fires unnecessary queries and wastes budget. Must validate `_identify_unsupported_assertions` on a held-out set before enabling in production.",
    "Contrastive MCQ retrieval adds latency only when triggered by a narrow margin, but for HLE questions the narrow-margin case may be common (these are intentionally hard discriminative questions). Need a per-question latency cap so contrastive retrieval is skipped if the question is already near the max_latency_s budget.",
    "The scratch-pad synthesis draft in step 2 consumes synthesis tokens from the provider's context window before the real synthesis. For providers with tight rate limits (Gemini, Kimi), running a scratch draft may trigger 429s that delay the main synthesis. The scratch draft call should use a separate provider slot or be rate-limit-aware.",
    "Faithfulness anchor pass in strict mode (`SPARKIT_FAITHFULNESS_STRICT=1`) may over-prune correct answers that are valid logical inferences from the evidence but not word-for-word entailed by any single extracted claim. Strict mode should be disabled by default and tested carefully; the annotate mode is safer and still surfaces provenance gaps.",
    "Per-sentence LLM calls in the faithfulness anchor pass scale with answer length. A 500-word synthesis split into 15 sentences requires 15 LLM calls for faithfulness checking. These should be batched into a single prompt ('check all of the following sentences against the claim set') to keep latency and cost manageable."
  ]
}
```
