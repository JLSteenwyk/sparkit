```json
{
  "proposal_id": "SPARKIT-HDR-2026-001",
  "title": "Hypothesis-Discriminative Retrieval with Provider-Disagreement Gating and Citation-Graph Hop Expansion",
  "distinctive_angle": "Invert the standard retrieve-then-reason loop: use cheap providers to generate competing candidate hypotheses before any retrieval, identify where they diverge, then execute retrieval calibrated to the hypothesis decision boundary rather than the topic centroid. Augment with 2-hop citation-graph expansion of top-ranked papers and a provider-disagreement gate that triggers an emergency targeted retrieval round when fast models disagree—leveraging SPARKIT's unique multi-provider architecture in a way no single-model RAG system can replicate.",
  "summary": "Add four tightly coupled enhancements to the orchestration pipeline: (1) a pre-retrieval Hypothesis Generation Phase that elicits 3–5 candidate answers from two lightweight providers and identifies 'boundary pairs'—the most similar competing hypotheses that require discriminating evidence; (2) a new Discriminative Query Round inserted after the standard primary round that constructs queries explicitly targeting the mechanism difference between boundary-pair candidates; (3) a 2-Hop Citation Graph Expansion step during ingestion that fetches abstracts of references cited by the top-10 ingested papers via CrossRef/Semantic Scholar and promotes high-scoring cited papers into the evidence pool; and (4) a Provider-Disagreement Gate that fires an emergency extra retrieval round when the lightweight hypothesis providers disagree, using their divergent claims as seeds for contrastive queries. Existing _chunk_relevance_score(), _record_relevance_score(), and _build_synthesis_prompt() are extended to track and expose 'discriminative power'—how strongly a chunk separates the leading hypothesis from its nearest rival—alongside standard relevance.",
  "reasoning": "SPARKIT's current retrieval strategy targets topic relevance: queries are constructed from the question stem and known focus terms, and chunks are scored by token overlap with the question. For easy questions this is sufficient, but for HLE-level PhD questions the correct answer is almost always plausible-sounding, and the wrong answers are the closest neighbors. A topic-centroid retrieval will gather plenty of evidence about the general subject area but will chronically miss the specific comparative study or mechanistic paper that distinguishes the correct answer from its nearest-wrong neighbor. The provider-disagreement gate exploits SPARKIT's unique inventory of 8 providers: when even fast models disagree on a question it is a strong signal that the question is genuinely ambiguous and that retrieval needs to work harder. Citation-graph expansion addresses a structural gap: the papers that best distinguish subtly competing hypotheses are often methodological or review papers cited across the field, which are underrepresented in keyword-match results but appear as frequent references in the papers already retrieved. Together these changes re-orient retrieval from 'gather evidence about the topic' to 'find the paper that settles the specific dispute at hand', which is the correct objective for hard QA.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _generate_hypothesis_candidates(question, providers, n=5) to engine.py. Use two fast providers (e.g., Mistral + DeepSeek) with a short system prompt: 'List the N most plausible answers to this question as a JSON array, most likely first.' Parse results, union the lists, and identify boundary pairs by token-overlap similarity (pairs with Jaccard > 0.25 are too close to resolve without targeted evidence). Store candidates and boundary pairs in the OrchestrationState for downstream use. Fall back silently to empty lists on any provider failure so the main pipeline is unaffected.",
      "owner": "engine.py: new helper above _decompose_question()",
      "effort": "medium"
    },
    {
      "step": "Extend _decompose_retrieval() / _build_round_queries_from_plan() to inject a 'discriminative' query category. For each boundary pair (A, B), template: '{question_stem} {term_A} versus {term_B}', '{mechanism_A} compared to {mechanism_B} evidence', '{term_A} {term_B} experimental distinction'. Deduplicate against existing queries via _dedupe_queries(). Insert as a new retrieval round immediately after round 0 (primary), shifting subsequent rounds. Cap at 6 discriminative queries to avoid token explosion.",
      "owner": "engine.py: _build_round_queries_from_plan() and retrieval loop lines ~1321-1430",
      "effort": "medium"
    },
    {
      "step": "Add provider-disagreement gate in the retrieval orchestration loop. After hypothesis generation, compute pairwise token-overlap agreement across providers. If mean agreement < 0.35 AND stage_idx is at the last standard round, inject one additional 'contrastive' round seeded from the specific disagreement phrases (unique tokens in each provider's top hypothesis). This round is skipped if the engine is already at adaptive_max_rounds or the budget gate has fired.",
      "owner": "engine.py: retrieval loop, alongside adaptive retrieval logic at lines ~1398-1430",
      "effort": "medium"
    },
    {
      "step": "Implement 2-hop citation graph expansion in the ingestion phase. After _select_records_for_ingestion() selects the top-10 documents, parse each ingested record's reference list from its HTML/PDF via the ingestion_service parser (reference sections already parsed into sections dict). Extract DOIs and titles. For up to 60 candidate references, batch-fetch abstracts via existing CrossRef and Semantic Scholar adapters. Score each reference abstract with _record_relevance_score() against the question. Promote any reference scoring above the 70th-percentile threshold of the already-selected records into the ingestion pool, up to a configurable SPARKIT_CITATION_HOP_MAX_EXTRA (default 8). Guard total ingested docs at SPARKIT_INGESTION_MAX_DOCS_ABSOLUTE (default 30).",
      "owner": "engine.py: ingestion phase ~lines 1437-1550; adapters.py for batch abstract fetching",
      "effort": "high"
    },
    {
      "step": "Add discriminative_power scoring to _chunk_relevance_score(). Given the leading hypothesis token set H_top and nearest-rival token set H_rival (passed from OrchestrationState), compute discriminative_power = overlap(chunk_tokens, H_top) - overlap(chunk_tokens, H_rival). Blend into chunk score: new_score = existing_score + 0.8 * max(0, discriminative_power). Chunks that strongly mention the correct-candidate terms while discussing mechanism get prioritized for inclusion in synthesis prompt. This requires passing candidate token sets through _select_best_section_chunk().",
      "owner": "engine.py: _chunk_relevance_score() at lines ~530-536 and call sites",
      "effort": "medium"
    },
    {
      "step": "Extend _build_synthesis_prompt() to include a 'Discriminative Evidence' section (max 6 bullets) drawn from the highest-discriminative_power chunks. Place this section before the standard evidence bullets so the synthesis model sees the most decision-relevant content first. For MCQ, annotate each bullet with which option it supports or contradicts: 'SUPPORTS_A', 'CONTRADICTS_B'. This gives the scorer and judge explicit signal about which option each evidence item bears on.",
      "owner": "engine.py: _build_synthesis_prompt() at lines ~992-1032",
      "effort": "medium"
    },
    {
      "step": "Add entity synonym expansion to _rewrite_queries() in aggregator.py. Maintain a compact lookup table (chemical compound -> [IUPAC, CAS, common synonyms], gene symbol -> [full name, Entrez ID prefix, aliases], protein -> [UniProt ID prefix, aliases]). For each query token matching an entry, inject the top-2 synonyms as additional query terms. Keep total terms under 18 (existing guard). The table can be seeded with ~500 high-frequency scientific entities from the HLE bio/chem benchmark term distribution and grown incrementally. Store table in a JSON file at services/retrieval_service/app/entity_synonyms.json.",
      "owner": "aggregator.py: _rewrite_queries() and new entity_synonyms.json data file",
      "effort": "high"
    },
    {
      "step": "Add per-question ablation flags: SPARKIT_ENABLE_HYPOTHESIS_GEN (default 1), SPARKIT_ENABLE_DISCRIMINATIVE_ROUND (default 1), SPARKIT_ENABLE_CITATION_HOP (default 1), SPARKIT_ENABLE_ENTITY_SYNONYMS (default 1). This allows isolated ablation in the benchmark harness by toggling individual components per run without code changes.",
      "owner": "engine.py: env var parsing block at lines ~778-802",
      "effort": "low"
    },
    {
      "step": "Update policy.py cost estimation to account for: (a) 2 extra lightweight provider calls for hypothesis generation, (b) 1 discriminative retrieval round (same cost as existing rounds), (c) up to 60 Semantic Scholar/CrossRef abstract fetches for citation hop (free APIs but add latency). Add hypothesis_gen_cost and citation_hop_fetch_count to the observability dict returned by execute_orchestration().",
      "owner": "policy.py and engine.py observability section at lines ~2240-2267",
      "effort": "low"
    },
    {
      "step": "Write unit tests covering: hypothesis boundary pair detection correctness, discriminative query deduplication, citation hop score threshold, entity synonym expansion without token limit violations, discriminative_power chunk scoring with synthetic chunk/hypothesis fixtures, synthesis prompt discriminative section ordering. Add integration smoke test: on a 3-question HLE subset, verify that at least 1 discriminative evidence bullet appears in the final prompt.",
      "owner": "services/orchestrator/tests/test_synthesis_quality.py and new test_hypothesis_retrieval.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Discriminative hypothesis-boundary query round: for each pair of near-similar candidate answers identified in the pre-retrieval hypothesis generation step, generate queries of the form '{A} vs {B} experimental evidence', '{mechanism_A} contrasted with {mechanism_B}', targeting papers that explicitly compare the two candidates rather than just covering the topic. These are inserted as a dedicated retrieval round after round 0 and before the options round.",
    "Provider-disagreement-gated emergency contrastive round: compute cross-provider token-overlap agreement on the generated hypotheses; when mean agreement < 0.35, fire an additional retrieval round using unique discriminating tokens from each provider's divergent hypothesis as query seeds. This round is the only one in SPARKIT that uses model-generated uncertainty as a retrieval signal rather than lexical question features, targeting effort exactly where it is most needed.",
    "2-hop citation graph expansion: after ingesting the top-10 documents, parse their reference lists (already extracted by the ingestion service), batch-fetch abstracts of up to 60 cited papers via CrossRef and Semantic Scholar, score them against the question, and promote references scoring above the 70th-percentile threshold into the ingestion pool. This surfaces seminal papers and cross-cited methodological references that keyword search chronically misses because their titles do not contain the question's surface tokens.",
    "Entity-anchored synonym query expansion: maintain a domain-specific synonym table (chemical, gene, protein) and expand each query with top-2 aliases per matched entity before submitting to all six retrieval adapters. This addresses the fundamental mismatch between how a question states an entity (e.g., 'vitamin B12') and how papers index it (e.g., 'cobalamin', 'cyanocobalamin', 'CAS 68-19-9'), which currently causes correct papers to score below the retrieval threshold.",
    "Discriminative-power chunk scoring during ingestion: extend _chunk_relevance_score() to compute overlap(chunk_tokens, leading_hypothesis) - overlap(chunk_tokens, nearest_rival_hypothesis) and add this as a positive bias to chunks that selectively support the leading candidate, ensuring that the synthesis prompt receives the most decision-relevant passage rather than the most topically adjacent one."
  ],
  "evaluation_plan": [
    "Accuracy benchmark on HLE bio/chem 149 questions: run full SPARKIT pipeline with all four enhancements enabled vs. current main branch with identical provider configuration and report absolute accuracy difference. Minimum bar for promotion: +3 pct points net of noise (p < 0.10 by McNemar's test on paired question-level outcomes).",
    "Ablation matrix across HLE-149: run each of the four enhancements individually disabled (SPARKIT_ENABLE_HYPOTHESIS_GEN=0, SPARKIT_ENABLE_DISCRIMINATIVE_ROUND=0, SPARKIT_ENABLE_CITATION_HOP=0, SPARKIT_ENABLE_ENTITY_SYNONYMS=0) and report per-ablation accuracy drop to quantify each component's independent contribution. Any component contributing < 0.5 pct points in isolation can be removed to reduce cost without accuracy loss.",
    "Citation hop recall audit: for 30 manually selected HLE questions where the correct answer is known, verify whether the seminal or most-relevant paper appears in the final ingested evidence pool. Measure the rate at which it only appears via citation hop (not via direct keyword search). Target: citation hop should introduce at least one question-critical paper in >= 40% of cases where it fires.",
    "Provider-disagreement calibration check: measure whether questions where the disagreement gate fires (mean agreement < 0.35) have lower baseline accuracy than questions where it does not fire. This validates the gate as a reliable uncertainty signal. If gate-firing questions have baseline accuracy > 5 pct points higher than non-firing questions, the threshold should be tightened.",
    "Discriminative evidence coverage rate: for MCQ questions in HLE-149, measure the fraction of correct answers where at least one discriminative evidence bullet (annotated SUPPORTS_correct_option) appears in the synthesis prompt. Target >= 60% coverage. If below 40%, the boundary-pair generation or discriminative query templates need revision.",
    "Cost and latency regression: for a 20-question sample, measure mean per-question cost (USD) and end-to-end latency (seconds) before and after. Acceptable regression: <= 35% cost increase, <= 40% latency increase relative to current research_max mode. If citation hop dominates cost, SPARKIT_CITATION_HOP_MAX_EXTRA should be reduced from 8 to 4."
  ],
  "risks": [
    "Hypothesis generation using fast providers may produce confidently wrong candidates for highly specialized questions, causing the discriminative query round to target the wrong boundary pair and retrieve irrelevant comparative evidence. Mitigation: generate hypotheses from two structurally different providers (e.g., Mistral + DeepSeek) and only treat a pair as a boundary pair if both providers include both members; otherwise fall back to standard retrieval.",
    "Citation graph expansion via CrossRef and Semantic Scholar may hit rate limits when ingesting many documents per question in a concurrent benchmark run. Mitigation: batch abstract fetches with a 200ms inter-request delay (already present in adapters.py retry logic), cap at 60 reference candidates per question, and implement a skip-on-429 path so citation hop degrades gracefully to standard ingestion without blocking the pipeline.",
    "Entity synonym expansion for scientific terms risks injecting ambiguous synonyms that match unrelated papers (e.g., 'MAP' expands to 'mitogen-activated protein' but also matches 'MAP' used in cartography papers on Brave/web sources). Mitigation: restrict synonym expansion to academic adapters (arXiv, Semantic Scholar, CrossRef) only, and limit each entity to top-2 synonyms by domain-specificity score. Review the entity_synonyms.json table against false-positive rates before enabling by default.",
    "The discriminative_power chunk scoring modification to _chunk_relevance_score() introduces a dependency on OrchestrationState that currently is not threaded into that helper. Passing hypothesis token sets through the call stack requires touching _select_best_section_chunk(), _select_records_for_ingestion(), and _build_synthesis_prompt(), which risks introducing subtle bugs in the evidence selection path. Mitigation: implement as an optional additive bonus that defaults to 0 when candidate token sets are absent, so all existing tests pass without modification.",
    "Provider-disagreement gate may systematically fail to fire on questions where all cheap providers are confidently wrong in the same direction (correlated model errors), which is precisely the regime where extra retrieval is most needed. Mitigation: supplement agreement score with a 'self-consistency' check—if the hypothesis generation provider's answer contradicts retrieved evidence in round 0, also fire the gate regardless of cross-provider agreement.",
    "Adding a pre-retrieval hypothesis generation phase creates a sequential latency dependency at the start of every question: both lightweight providers must respond before the retrieval plan is built. On slow network days or provider-congestion events, this adds unbounded latency. Mitigation: apply a strict 10-second timeout to hypothesis generation calls and bypass the entire phase (not just degrade) if both calls exceed the timeout, falling back to the current retrieval decomposition path."
  ]
}
```
