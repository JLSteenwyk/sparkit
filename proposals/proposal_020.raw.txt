```json
{
  "proposal_id": "SPARKIT-HAET-001",
  "title": "Hypothesis-Anchored Evidence Triangulation (HAET): Discriminative Retrieval via Pre-Synthesis Hypothesis Priors",
  "distinctive_angle": "SPARKIT currently retrieves evidence agnostically—queries are generated from the question stem without knowing what the answer might be. HAET inverts this by generating ranked working hypotheses before any retrieval, then constructing queries specifically designed to DISCRIMINATE between competing hypotheses rather than merely gather supporting evidence. Evidence is mapped back to hypothesis coverage, and per-round hypothesis priors are updated Bayesian-style to focus subsequent retrieval rounds on whichever hypothesis has the highest unresolved uncertainty. This transforms retrieval from a document-accumulation task into a falsification-driven investigation.",
  "summary": "Add a pre-retrieval hypothesis generation stage that produces 3–5 ranked candidate answers for the question. Each retrieval round then issues three query classes: (1) support queries per hypothesis, (2) discriminative queries that would separate the top-2 hypotheses, and (3) falsification queries targeting the current leading hypothesis. After each round, a lightweight hypothesis-evidence scorer maps retrieved snippets to each hypothesis and updates a probability vector. The adaptive gating criterion is extended to also gate on hypothesis entropy (stop when one hypothesis dominates, continue when entropy is high). The synthesis stage receives the ranked hypothesis vector plus per-hypothesis evidence packs instead of a flat evidence pool. This proposal builds directly on SPARKIT's existing _decompose_retrieval, MCQ option-hypothesis query generation, adaptive gating, and claim clustering subsystems.",
  "reasoning": "HLE questions are hard precisely because the evidence landscape is ambiguous—multiple plausible answers exist, many sources partially support several options, and generic 'find me evidence about X' queries return noisy corpora that synthesizers cannot resolve. The core failure mode in SPARKIT's current architecture is that the synthesis LLM receives a flat bag of heterogeneous evidence and must simultaneously (a) decide what the answer is and (b) assess whether the evidence supports it. This conflates hypothesis selection with evidence grounding. By separating these concerns—hypothesis generation is cheap and fast; discriminative retrieval is targeted and efficient—we ensure that by the time synthesis runs, the evidence pool already contains documents that were fetched specifically because they distinguish between plausible answers. The hypothesis probability vector also feeds directly into the existing calibration formula (replacing the underinformative 'provider_config_ratio' feature with a 'hypothesis_dominance' feature) and gives the answerability gate a better signal for hard-abstain decisions. The discriminative query class is mechanically similar to the MCQ option-hypothesis queries already present in _decompose_retrieval (lines 1250–1310 in engine.py) but generalizes to free-form questions without explicit choices.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _generate_hypotheses() method to OrchestratorEngine. Uses a fast cheap model (deepseek-reasoner or gemini-3-pro-preview) with a structured prompt: 'Given this question, list exactly 5 candidate answers ranked by prior plausibility. For each: hypothesis_text, rationale (1 sentence), key discriminating fact that would confirm or deny it.' Output parsed into HypothesisPrior dataclass with fields: id, text, rationale, prior_weight (uniform 0.2 initially), evidence_snippets[], support_score float, falsification_score float. Insert this call in run() immediately after _decompose_question() completes and before _decompose_retrieval().",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend _decompose_retrieval() to accept hypothesis_priors: list[HypothesisPrior] and emit three additional query intents beyond the existing 'primary', 'methods', 'adversarial', 'reference' set. New intents: 'hypothesis_support_{i}' for the top-2 hypotheses (one query each, using the hypothesis rationale + key discriminating fact as query seed), 'discriminative_top2' (one query synthesizing the differentiating fact between hypotheses ranked 1 and 2), 'falsification_lead' (one query targeting the leading hypothesis's rationale to find contradicting evidence). Total new queries per round: 5. These slot directly into the existing RetrievalRound.queries list without changing the retrieval service interface.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _score_evidence_against_hypotheses() function. After each retrieval round, for every newly retrieved document snippet, compute token overlap between snippet text and each hypothesis's text + rationale. Assign the snippet to the hypothesis with highest overlap if overlap > 0.15, otherwise mark as 'undifferentiated'. Update HypothesisPrior.support_score += overlap for supporting snippets; update falsification_score for snippets that contain contradiction markers (reuse existing markers list from verifier.py: 'contradict', 'inconsistent', 'negative result', 'no evidence', etc.). Normalize all prior_weights by (support_score - falsification_score), re-normalize to sum=1. Compute hypothesis_entropy = -sum(p * log(p)) over prior_weights. Insert this call at the end of each retrieval round loop before the adaptive gating check.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend adaptive retrieval gating to incorporate hypothesis_entropy. Current gate: stop if new_unique_docs < SPARKIT_ADAPTIVE_MIN_NEW_DOCS AND quality_gain < SPARKIT_ADAPTIVE_MIN_QUALITY_GAIN. New gate: stop if (existing condition) AND hypothesis_entropy < SPARKIT_ADAPTIVE_MAX_ENTROPY (default: 0.8, meaning one hypothesis has >55% weight). Continue retrieval (override early stop) if hypothesis_entropy > SPARKIT_ADAPTIVE_FORCE_CONTINUE_ENTROPY (default: 1.4, meaning evidence is still very undifferentiated). Emit new fields 'hypothesis_entropy', 'lead_hypothesis_weight', 'lead_hypothesis_id' in the 'retrieval_adaptive_gate' trace stage.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Modify _build_synthesis_context() to structure evidence packs per hypothesis rather than as a flat pool. For each of the top-3 hypotheses, produce a labeled evidence pack: 'HYPOTHESIS {i}: {text} [weight: {prior_weight:.2f}] / Supporting: {top-3 snippets} / Falsifying: {top-2 contradicting snippets}'. Pass the ranked hypothesis list as a prefix to the synthesis prompt before the existing claim clusters and section summaries. Instruct the synthesis LLM to 'evaluate each hypothesis against the evidence packs and select the best-supported one, explaining why others are eliminated.' This is a prompt-engineering change only—no structural changes to the synthesis pipeline.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add 'hypothesis_dominance' calibration feature = max(prior_weights) after final retrieval round. Replace 'provider_config_ratio' (which is static and uninformative for single-provider runs) in the calibration formula with hypothesis_dominance. Adjust formula weights: base=0.25, support=0.40, hypothesis_dominance=0.15 (was config_ratio=0.15), ensemble_agreement=0.10, evidence_count=0.05, minus unsupported penalty and contradiction penalty. Update _calibrate_confidence() accordingly and add a new column 'hypothesis_dominance' to run_calibration_features via Alembic migration.",
      "owner": "orchestrator/engine.py + alembic",
      "effort": "low"
    },
    {
      "step": "Update hard-abstain answerability gate to also trigger if lead hypothesis weight < 0.35 after all retrieval rounds (evidence is too ambiguous to commit). Emit abstain_reason='hypothesis_entropy_too_high' in the existing abstain reason set. This reuses the existing answerability gate infrastructure; add one elif branch and a new constant SPARKIT_ABSTAIN_MIN_HYPOTHESIS_DOMINANCE (default: 0.35).",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add unit tests to test_synthesis_quality.py for: (1) _generate_hypotheses() returns exactly 5 items with required fields, (2) discriminative queries are emitted when hypothesis_priors are provided, (3) _score_evidence_against_hypotheses() correctly increments support/falsification scores and renormalizes weights, (4) hypothesis_entropy computation is correct for uniform vs dominated distributions, (5) synthesis prompt includes 'HYPOTHESIS {i}:' prefix blocks, (6) calibration uses hypothesis_dominance not config_ratio, (7) abstain triggers on low hypothesis dominance.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    },
    {
      "step": "Run HLE-Gold 25-question benchmark in single_openai and single_anthropic modes with HAET enabled vs. disabled (A/B). Use existing eval_service direct runner. Capture accuracy delta, per-run cost delta (expect +$0.02–0.08 per run for hypothesis generation), latency delta, and hypothesis_entropy distribution across questions to tune SPARKIT_ADAPTIVE_MAX_ENTROPY and SPARKIT_ADAPTIVE_FORCE_CONTINUE_ENTROPY defaults.",
      "owner": "eval_service + scripts_run_claudep_proposals_runner.sh",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Discriminative query generation: for each pair of top-2 hypotheses, synthesize a query from the key differentiating fact between them (e.g., if H1 says mechanism is phosphorylation and H2 says ubiquitination, the discriminative query is 'phosphorylation vs ubiquitination [protein target]'). This targets documents that directly compare or contrast competing mechanisms—the documents most useful for resolving ambiguity—rather than documents that merely mention either mechanism in isolation. These queries are mechanically generated from hypothesis rationales without LLM overhead beyond the initial _generate_hypotheses() call.",
    "Falsification-first retrieval round: designate round 2 (after an initial broad round 1) as a falsification-priority round where the lead hypothesis's core claim is negated in the query. E.g., if the leading hypothesis is 'X inhibits Y pathway', query for 'X does not inhibit Y pathway OR X activates Y pathway'. This ensures the adversarial retrieval is targeted at the specific leading candidate rather than generic contradiction of the question stem, which is what the current 'adversarial' intent does. The retrieved falsification documents, if weak or absent, provide strong evidence FOR the lead hypothesis; if strong, they correctly demote it. This extends the existing adversarial query intent in _decompose_retrieval with hypothesis-conditioned query text.",
    "Hypothesis-entropy-gated continuation: when hypothesis_entropy remains high (>1.4 bits) after the standard adaptive stopping criterion would have halted retrieval, HAET forces at least one additional discriminative round. In SPARKIT's current adaptive gating (engine.py lines 1398–1454), retrieval stops when new_doc_count AND quality_gain are both low—but this can halt prematurely if the retrieved evidence is abundant but undifferentiated (many documents relevant to the question but none resolving the key discriminating fact). The entropy continuation override ensures at least one round of targeted discriminative queries runs before stopping, trading a small cost increase for much higher evidence precision for genuinely hard questions.",
    "Per-hypothesis source diversity enforcement: the existing source-diversity cap (max_results/2 per source) is applied globally. HAET adds a per-hypothesis source diversity check: for each hypothesis, if >70% of its supporting snippets come from a single source domain, inject a source-diversifying query ('hypothesis text site:NOT dominant_domain') in the next round. This prevents a single high-domain-authority source from monopolizing the evidence for one hypothesis while competing hypotheses receive weaker multi-source backing, which would incorrectly inflate that hypothesis's prior weight.",
    "Evidence novelty scoring for hypothesis updating: extend _score_evidence_against_hypotheses() to also compute snippet novelty relative to already-ingested snippets (using token-level Jaccard distance). Novelty score weights the hypothesis update—a highly novel snippet confirming a hypothesis updates its prior more than a redundant snippet of the same content already seen in round 1. This prevents retrieval rounds from repeatedly surfacing the same high-authority source and mistaking re-confirmation for independent corroboration, which is a known bias in current lexical evidence reranking."
  ],
  "evaluation_plan": [
    "Hypothesis quality audit on HLE-Gold-25: after implementing _generate_hypotheses(), manually audit the 5 generated hypotheses for all 25 HLE-Gold questions. Score each hypothesis set on: (a) does the correct answer appear in the top-3 hypotheses (target: >80% of questions), (b) are the hypotheses semantically distinct (no two hypotheses differ only in surface form, target: avg pairwise cosine distance >0.4), (c) does the leading hypothesis (rank 1) match the correct answer (target: >50% of questions, since uniform prior should already beat random). This validates the hypothesis generation stage independently before HAET retrieval is evaluated end-to-end.",
    "Retrieval discrimination effectiveness check: for 10 HLE-Gold questions where HAET is enabled, compare the cosine similarity of the top-5 retrieved documents to the correct answer versus the top-5 documents retrieved without hypothesis conditioning. Use a sentence embedding model (e.g., Gemini embedding or OpenAI text-embedding-3-large) to compute similarity. Target: HAET top-5 documents have >15% higher mean similarity to correct answer than baseline top-5. This isolates the retrieval quality improvement from any synthesis-side effect.",
    "Hypothesis entropy convergence check: for all benchmark runs with HAET enabled, verify that hypothesis_entropy strictly decreases monotonically across retrieval rounds on at least 70% of questions. A non-monotonic entropy trajectory indicates the evidence scoring or hypothesis update logic has a bug (new evidence is destabilizing previously resolved hypotheses). Plot entropy per round for each question in the HLE-Gold eval artifacts.",
    "Calibration feature validity test: add a unit test asserting that hypothesis_dominance is positively correlated with final answer correctness across the HLE-Gold benchmark (i.e., questions where SPARKIT gets the correct answer should have higher mean hypothesis_dominance than questions where it gets it wrong). Target Pearson r > 0.25. If not achieved, hypothesis_dominance is not a valid calibration signal and should be reverted to config_ratio. Run this check as part of the CI eval_service regression suite.",
    "Cost regression guard: add an assertion in the HLE benchmark runner that mean per-run cost with HAET enabled is < 2x mean per-run cost without HAET. The hypothesis generation step (one LLM call) + five additional queries per round should add at most $0.05–0.10 per run. If cost exceeds 2x baseline, the implementation has likely introduced unnecessary LLM calls (e.g., hypothesis generation is being called per-round instead of once). Encode this as a benchmark regression check in the Makefile benchmark target.",
    "Abstain rate delta test: verify that HAET does not significantly increase the hard-abstain rate on questions SPARKIT previously answered. Run HAET on the 20 HLE-Gold questions where baseline SPARKIT produced a non-abstain answer and check that abstain rate increases by <20% (i.e., HAET should not abstain on more than 4 additional questions that baseline SPARKIT answered). If abstain rate increases more, the SPARKIT_ABSTAIN_MIN_HYPOTHESIS_DOMINANCE threshold of 0.35 is too aggressive for the question distribution."
  ],
  "risks": [
    "Hypothesis generation quality ceiling: for highly specialized HLE questions (rare biochemical mechanisms, obscure historical facts, advanced mathematics), the pre-retrieval hypothesis generator may produce 5 hypotheses that are all plausible-sounding but all wrong, meaning the correct answer never enters the hypothesis space and HAET's discriminative retrieval focuses entirely on the wrong candidates. Mitigation: always include an 'Other / None of the above' hypothesis at position 5 with an evidence pack built from the full flat retrieval pool, ensuring the synthesis stage can always fall back to unguided evidence.",
    "Hypothesis-evidence alignment brittleness: the token-overlap scorer for mapping evidence to hypotheses is lexical and will fail for paraphrased or domain-specific evidence (e.g., a hypothesis mentioning 'apoptosis' will score zero overlap with a snippet discussing 'programmed cell death'). This could systematically misattribute evidence and produce incorrect hypothesis weight updates. Mitigation: fall back to the existing claim clustering approach (which is also lexical but at the level of claim text, not hypothesis text) when token overlap for all hypotheses is below 0.05 for a given snippet, marking it as undifferentiated rather than forcing a low-confidence assignment.",
    "Latency budget overrun on difficult questions: for questions where hypothesis entropy remains high after all planned rounds (entropy-gated continuation keeps triggering), HAET could exhaust the latency budget before reaching a confident hypothesis. Current latency budget is typically 300–900s for SPARKIT runs. An unbounded entropy continuation loop could exceed this. Mitigation: cap entropy-forced continuation rounds at SPARKIT_ADAPTIVE_MAX_ENTROPY_OVERRIDE_ROUNDS (default: 2) so at most 2 extra rounds are added regardless of entropy, keeping worst-case latency bounded.",
    "Cost increase may exceed benefit for low-difficulty questions: HAET adds a fixed overhead per run (hypothesis generation call + 5 extra queries per round) regardless of question difficulty. For questions that baseline SPARKIT already answers correctly, this overhead is pure waste. Mitigation: add a cheap difficulty heuristic before _generate_hypotheses()—if the question has fewer than 3 named entities and fewer than 25 tokens, skip hypothesis generation and use the existing retrieval path. This preserves HAET's benefit for genuinely hard questions while avoiding overhead on simpler ones.",
    "Discriminative query quality depends on hypothesis specificity: if two hypotheses are semantically similar (e.g., 'X inhibits pathway A via phosphorylation' vs 'X inhibits pathway A via acetylation'), the discriminative query between them will be very specific and may return zero results from academic databases. Mitigation: compute pairwise semantic similarity between hypotheses during generation and if top-2 hypotheses have cosine similarity >0.85 (near-identical), merge them and use the 3rd hypothesis as the second discriminative target instead."
  ]
}
```
