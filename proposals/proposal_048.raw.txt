```json
{
  "proposal_id": "SPARKIT-PERF-2026-001",
  "title": "Evidence-Reactive Retrieval Chain: Citation Graph Amplification, LLM Gap-Fill Steering, and Cross-Encoder Reranking",
  "distinctive_angle": "Replace SPARKIT's pre-planned, forward-only retrieval rounds with an evidence-reactive loop: after round 1 results land, mine Semantic Scholar's citation graph from anchor papers to surface co-cited works missed by keyword search, then feed retrieved abstracts back to the synthesis LLM mid-run to identify factual gaps and emit precision queries targeting those specific unknowns—replacing the static round-2/3 structure with one steered by what was actually found.",
  "summary": "Hard questions fail because keyword-based retrieval misses papers that cite the directly relevant papers, papers that use orthogonal terminology for the same concept, and precise methodological detail buried in non-abstract sections. Three integrated mechanisms are proposed: (1) Citation graph amplification—after round 1, expand the top-3 relevance-ranked records by one Semantic Scholar citation-graph hop using GET /paper/{s2Id}/references and /citations with fields title,abstract,year,externalIds, injecting up to 8 novel deduplicated records into the round-2 evidence pool; (2) LLM gap-fill steering—feed top-5 round-1 abstract snippets to the planning provider with a structured prompt asking for 2-4 queries targeting still-unknown facts, injecting them as additional round-2 primary queries, replacing static 'limitations' and 'benchmark comparison' round-2 queries that are oblivious to what was retrieved; (3) Cross-encoder reranking—swap _record_relevance_score's token-overlap formula in _select_records_for_ingestion (engine.py:135) for semantic similarity using a sentence-transformers cross-encoder (cross-encoder/ms-marco-MiniLM-L-6-v2 as local fallback, Cohere Rerank API as primary), catching paraphrase matches and domain synonyms that score near zero under the current 2*overlap_title + 1*overlap_abstract formula.",
  "reasoning": "SPARKIT plans all retrieval queries before seeing any evidence (_build_round_queries at engine.py:194 and _heuristic_retrieval_plan at engine.py:596). This means round-2 and round-3 queries are completely orthogonal to what round-1 actually retrieved. For moderately hard questions this is acceptable because direct keyword matches surface the key papers. For HLE-level hard questions, the crucial evidence is typically (a) in papers that are not directly about the question's surface topic but are co-cited alongside it in the field's canonical references, (b) described using domain terminology that does not overlap lexically with the question's words, or (c) a specific finding buried in a paper that is broadly about something else. Citation graph amplification attacks (a): the top retrieved papers are anchors into the academic graph, and their 1-hop citation neighborhood is a highly concentrated source of topically adjacent papers that keyword search cannot reach. LLM gap-fill steering attacks (c): by asking the planning model 'given these retrieved abstracts, what specific facts are still unknown?', the system can generate queries that target exactly the missing intermediate reasoning steps required for multi-hop hard questions. Cross-encoder reranking attacks (b): the current _record_relevance_score at engine.py:121 uses set intersection of tokenized text which fails catastrophically for synonym-heavy STEM vocabulary (e.g., 'phosphorylation cascade' vs 'kinase signaling pathway'). Additionally, the Semantic Scholar adapter currently requests fields 'title,abstract,authors,year,doi,url' (adapters.py:168) but not 'paperId'—which means citation graph amplification requires only a one-line field addition to enable the entire capability on an already-integrated source.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Extend LiteratureRecord model (retrieval_service/app/models.py) with optional s2_paper_id: str | None = None field. Update search_semantic_scholar in adapters.py:168 to add 'paperId' to the fields param and populate record.s2_paper_id from item.get('paperId'). This is a non-breaking additive change.",
      "owner": "retrieval_service",
      "effort": "low"
    },
    {
      "step": "Add fetch_citation_neighbors(s2_paper_id: str, limit: int = 10, timeout_s: float = 12.0) -> list[LiteratureRecord] in adapters.py. Call GET https://api.semanticscholar.org/graph/v1/paper/{s2_paper_id}/references?fields=title,abstract,year,externalIds,url and /citations with the same fields. Normalize externalIds.DOI into record.doi. Add S2_API_KEY header support (authenticated tier: 100 req/s vs 1 req/s unauthenticated). Return up to limit records per direction.",
      "owner": "retrieval_service",
      "effort": "medium"
    },
    {
      "step": "Add _citation_amplification_round(anchor_records: list[LiteratureRecord], existing_keys: set[str], target_new: int = 8) -> list[LiteratureRecord] in engine.py. Select top-3 anchor records by _record_relevance_score that have non-null s2_paper_id. Call fetch_citation_neighbors for each anchor (references + citations). Deduplicate against existing_keys using (doi or url).lower() key. Score candidates with existing _record_relevance_score. Return top target_new by score. Guard with SPARKIT_ENABLE_CITATION_AMP env var (default 1). Add to policy.py: citation amp Semantic Scholar calls cost $0 (free API).",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add _llm_gap_fill_steering(question: str, round1_records: list[LiteratureRecord], planning_provider: str, max_queries: int = 4) -> list[str] in engine.py. Build a prompt: 'Question: {question}\\nEvidence retrieved so far:\\n{top_5_abstracts}\\n\\nIdentify 2-4 specific factual gaps not yet covered. Output a JSON array of precise scholarly search queries targeting those gaps. Each query must contain at least one term from the question.' Call generate_text with max_tokens=200. Parse JSON array. Apply coherence filter: discard queries with zero token overlap with question. Inject survivors into round-2 primary queries via _dedupe_queries. Guard with SPARKIT_ENABLE_GAP_FILL_STEERING env var (default 1).",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add _cross_encoder_rerank(question: str, records: list[LiteratureRecord]) -> list[tuple[float, LiteratureRecord]] in engine.py. Primary backend: if COHERE_API_KEY set, call Cohere Rerank API (model rerank-v3.5, documents = [title + ' ' + (abstract or '')][:512 chars each], top_n=len(records)), normalize scores to [0,1]. Fallback backend: lazy-load sentence_transformers CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2') as module-level singleton, batch predict against question. Backend controlled by SPARKIT_RERANK_BACKEND env var ('cohere'|'local'|'lexical', default 'lexical' for backward compat). Replace the sort key in _select_records_for_ingestion (engine.py:143) to use cross-encoder scores when backend != 'lexical'.",
      "owner": "orchestrator",
      "effort": "high"
    },
    {
      "step": "Wire citation amplification and gap-fill steering into execute_orchestration: after round-1 search_literature call completes, (a) collect round-1 records with s2_paper_id, (b) call _citation_amplification_round and merge results into all_records dedup pool, (c) call _llm_gap_fill_steering and prepend returned queries to round-2 primary intent_queries list. Preserve budget guard: check should_stop_early before each new network call.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add unit tests: test_citation_amplification verifies top-k anchor selection, dedup against existing_keys, Semantic Scholar endpoint shape; test_llm_gap_fill_steering verifies prompt format, JSON parse, coherence filter discard; test_cross_encoder_rerank verifies score normalization 0-1, backend fallback path, and that result ordering differs from lexical baseline on a synthetic paraphrase case.",
      "owner": "orchestrator/tests",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Citation graph amplification via Semantic Scholar /references and /citations endpoints: for the top-3 relevance-ranked round-1 records with a known paperId, fetch their 1-hop citation neighborhood (up to 10 candidates per direction per anchor = max 60 candidates), deduplicate against already-retrieved DOIs/URLs using the existing _dedupe_records key logic, score with _record_relevance_score, and inject the top-8 novel records into round-2—surfacing papers that are academically adjacent to anchor papers but would never match the question's keyword surface form.",
    "LLM gap-fill steering to replace static round-2/3 query templates: after round-1 completes, the top-5 retrieved abstracts are fed to the planning model with a structured prompt that asks it to identify specific factual unknowns and emit JSON-formatted scholarly queries targeting those gaps. These evidence-grounded queries replace the current hardcoded '{question} limitations' and '{question} benchmark comparison' round-2 templates (engine.py:197-198), which are oblivious to what was retrieved and fail to target the specific missing intermediate facts needed for multi-hop hard questions.",
    "Cross-encoder semantic reranking in _select_records_for_ingestion: replace the token-overlap formula at engine.py:121-132 (2*overlap_title + 1*overlap_abstract + 0.25*recency_bonus) with a sentence-transformers cross-encoder that jointly encodes the question and each candidate's title+abstract. The current formula produces a score of zero for any synonymous or paraphrased term (e.g., 'phosphorylation cascade' vs 'kinase signaling'), causing the system to systematically deprioritize the most topically relevant papers when domain vocabulary diverges from question surface form.",
    "Contrastive hypothesis-conditioned retrieval for MCQ: extend _build_option_hypothesis_queries (engine.py:479) to generate both a support query ('{stem} evidence supporting {choice}') and a falsification query ('{stem} evidence against {choice}' / '{stem} {choice} failure mechanism') for each answer option. The current implementation generates only '{stem} {choice}' and '{stem} evidence for {choice}', providing no retrieval signal for disconfirming evidence—which is precisely the evidence that discriminates between plausible-sounding wrong answers on hard MCQ questions."
  ],
  "evaluation_plan": [
    "A/B accuracy benchmark on HLE-25 balanced subset (existing benchmark artifact at benchmarks/results/): run baseline SPARKIT vs proposal (SPARKIT_ENABLE_CITATION_AMP=1, SPARKIT_ENABLE_GAP_FILL_STEERING=1, SPARKIT_RERANK_BACKEND=local) using the same synthesis provider on all 25 questions. Require at least 20 non-abstain answers per configuration. Declare improvement if accuracy gain >= 2pp and one-sided exact binomial test p < 0.10. Log avg_evidence_count, citation_coverage, and abstain_rate as secondary metrics.",
    "Citation amplification novelty yield audit: for each run, log how many ingested documents came exclusively from the citation graph (source traced to fetch_citation_neighbors) vs direct keyword search. A healthy citation round should yield >= 2 novel documents per run on average. If average novelty yield < 1 document/run across the HLE-25 benchmark, disable SPARKIT_ENABLE_CITATION_AMP by default—the latency overhead is not justified. Also verify no benchmark-answer-leakage domains (huggingface.co, futurehouse.org) enter via citation graph by asserting the existing domain blocklist applies to citation-sourced records.",
    "Cross-encoder reranker calibration check: on HLE-25, compute Spearman rank correlation between cross-encoder reranker scores and binary answer correctness (grouped per run: does the final synthesized answer match the gold answer?). If rho > 0.15, the reranker is providing signal beyond lexical scoring. Separately, verify citation_coverage (citations/claims) does not drop below the 0.40 abstain threshold (engine.py:183) after reranking-based record selection—cross-encoder should rerank, not discard grounded evidence.",
    "LLM gap-fill query coherence validation: for each gap-fill steering call, log the generated queries alongside the final gold answer. Post-benchmark, compute token overlap between each gap-fill query and the gold answer's key domain terms. If >= 50% of gap-fill queries have zero overlap with the gold answer's domain vocabulary across the benchmark, add a stricter coherence filter (require >= 2 question-token matches per query instead of 1). This detects hallucinated off-topic queries before they poison round-2 retrieval.",
    "Regression test on STEM-Exam-200 tuning set: run the full proposal against the existing STEM-Exam-200 baseline to verify accuracy does not drop by more than 2pp from the canonical baseline (the drift policy threshold in docs/drift-policy.md). This guards against the proposal degrading performance on moderately hard questions where the current static round structure already works well—ensuring that citation amplification and gap-fill steering add signal without introducing noise on easier questions."
  ],
  "risks": [
    "Semantic Scholar rate limits: the unauthenticated free tier allows 1 req/s. Citation amplification with 3 anchors x 2 directions = 6 requests per run; under concurrent benchmark load (e.g., 5 parallel runs) this saturates the rate limit immediately. Mitigation: add S2_API_KEY env var support to fetch_citation_neighbors (authenticated tier: 100 req/s), and implement per-domain request throttling in _get_with_retry for the semanticscholar.org host. Cache citation neighborhoods keyed by s2_paper_id in the corpus_documents table to avoid re-fetching the same anchor papers across runs.",
    "LLM gap-fill steering can hallucinate irrelevant queries that dilute round-2 retrieval. If the planning model generates queries about adjacent fields unrelated to the question, ingested documents may reduce synthesis signal-to-noise. Mitigation: the coherence filter (require >= 1 question-token overlap per query) provides a first guard; also cap injected gap-fill queries at max_queries=4 and apply _dedupe_queries normalization; monitor via the gap-fill coherence evaluation plan step above.",
    "Cross-encoder local model cold-start latency: loading cross-encoder/ms-marco-MiniLM-L-6-v2 via sentence_transformers adds ~1-3s on first inference per process (model load from disk). Subsequent calls on the same process are fast (~50-200ms for 20 documents). In serverless or short-lived orchestrator processes this overhead accumulates. Mitigation: implement lazy module-level singleton initialization guarded by a threading.Lock; set SPARKIT_RERANK_BACKEND='lexical' as default until the user explicitly opts into 'local' or 'cohere' to avoid unintended latency regressions.",
    "Citation amplification may disproportionately surface highly-cited review and survey papers (which accumulate many citations and references) rather than the primary research papers that contain the specific evidence needed for hard factual questions. Mitigation: apply a soft penalty to records whose titles contain 'review', 'survey', 'meta-analysis', or 'systematic review' by multiplying their _record_relevance_score by 0.7 when the inferred task_type is not 'comparative'—consistent with the existing task type inference at engine.py:565.",
    "The gap-fill steering LLM call adds one additional generation request per run, increasing both latency (~2-5s) and cost (~$0.002-0.01 depending on provider). For cost-sensitive benchmark configurations this may be undesirable. Mitigation: gate gap-fill steering behind SPARKIT_ENABLE_GAP_FILL_STEERING=1 (off by default) and preferentially use the cheapest available planning provider (already determined by build_provider_plan). Track the steering call cost in provider_usage as provider=planning, stage=gap_fill_steering for accurate per-run cost accounting."
  ]
}
```
