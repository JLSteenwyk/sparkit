```json
{
  "proposal_id": "SPARKIT-BCI-023",
  "title": "Bibliographic Backward-Chaining with Typed Abstract Slot Extraction and Finding-to-Option Alignment",
  "distinctive_angle": "All 21 prior proposals improve forward retrieval (query → papers) or reranking within the already-retrieved pool. This proposal exploits the citation graph in reverse: after Round 1, for each top-ranked paper that has a Semantic Scholar paper_id, fetch its reference list via the Semantic Scholar References endpoint, filter to influential cited works (citationCount > 10, year >= 2000, focus-term overlap > 0), and inject those foundational papers directly into the evidence pool without additional keyword-search cost. Simultaneously, replace SPARKIT's flat _first_sentence() claim extraction with a micro-LLM structured decomposition—{method, key_finding, quantitative_result, caveat}—and wire the quantitative_result slot into MCQ option alignment with a uniqueness-gated score boost. This targets the specific failure mode where hard HLE questions require exact numerical or mechanistic facts buried in older foundational papers that never surface in a fresh keyword search but are reliably cited by the review and primary papers that do.",
  "summary": "SPARKIT's retrieval rounds (primary, gap-fill, adversarial) are all forward searches: query terms to literature API results. For HLE-class biology and chemistry questions the definitive experimental evidence—IC50 values, rate constants, crystallographic coordinates, gene knockout phenotypes—lives predominantly in highly-cited primary papers from 2005-2018 that score poorly in fresh keyword searches because recent review papers and meta-analyses use paraphrase-heavy abstracts that dominate token-overlap rankings. Those recent papers reliably cite the foundational works in their reference lists. The backward-chain seeding round (Round 1b) fetches the reference list of the top-5 Round-1 papers from the Semantic Scholar References API (which already exists as an API source in aggregator.py), filters references by citationCount and focus-term overlap, deduplicates against existing all_records via the existing _dedupe_records() pipeline, and injects surviving foundational papers into the pool before ingestion selection. On the extraction side, _first_sentence() always emits the first 180 chars of the best chunk, which is typically methodological boilerplate or a background restatement. A single cheap micro-LLM call (claude-haiku-4-5, max_tokens=120) parses each abstract into typed slots: method, key_finding, quantitative_result, caveat. During _build_option_evidence_packs, the key_finding and quantitative_result slots are compared to each option label with a 1.5x weight multiplier over raw chunk text, enabling direct numerical and mechanistic matching against MCQ options. The combination addresses the two deepest evidence gaps in SPARKIT without changing the orchestration contract.",
  "reasoning": "Three independent failure modes compound on hard HLE questions. First, the retrieval miss: engine.py _record_relevance_score (2.0*title_overlap + 1.0*abstract_overlap + 0.25*recency) systematically favors recent review papers whose abstracts mention many topic tokens over older primary papers that use domain-precise IUPAC names and assay terminology. A 2008 Nature Chemical Biology paper reporting a 0.3 nM IC50 for a specific kinase inhibitor will score near zero if the question uses the compound's trade name rather than its systematic name, even though it is cited in all four of the top Round-1 results. Following the references of those top results with a Semantic Scholar References API call costs one HTTP round-trip and surfaces the foundational paper trivially. Semantic Scholar is already an adapter in retrieval_service/app/adapters.py and its API key is already provisioned; the references endpoint is a field-extension of the same graph API. Second, the extraction miss: _select_best_section_chunk picks the best chunk by lexical overlap then calls _first_sentence() to get the claim text. On a primary research paper, the first sentence of the Results section is almost always background or motivation; the quantitative finding appears in sentences 3-5. The micro-LLM slot extractor directly targets the key_finding and quantitative_result fields that the current logic never reaches. Third, the alignment miss: _build_option_evidence_packs scores each claim against each option by token overlap between claim_text and option label. A claim text of 'EGFR phosphorylation at Y1068 is abrogated by erlotinib at 50 nM' has near-zero token overlap with 'option B: erlotinib inhibits EGFR autophosphorylation' because the overlap set is only {erlotinib, EGFR} while the claim uses phosphorylation and abrogated. The structured key_finding slot would contain 'erlotinib blocks EGFR autophosphorylation at nanomolar concentrations', achieving 5x higher overlap. Together these three fixes directly address why SPARKIT achieves ~20% on HLE versus a theoretical ceiling of ~45-55% for frontier models with perfect retrieval, without requiring embedding infrastructure or contrastive MCQ retrieval rounds that proposals 001 and 013 already cover.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add SemanticScholarReferencesAdapter.fetch_references(paper_id: str, max_refs: int = 40) -> list[dict] in retrieval_service/app/adapters.py. Call GET https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=title,abstract,year,citationCount,externalIds,url with the existing SEMANTIC_SCHOLAR_API_KEY header. Return list of {title, abstract, year, citationCount, doi, url}. Apply the existing _make_request retry wrapper (2 attempts, 0.6s backoff) already used by SemanticScholarAdapter.search(). If paper_id is None or the call fails, return [].",
      "owner": "retrieval_service/app/adapters.py",
      "effort": "low"
    },
    {
      "step": "Add _backward_chain_seed(top_records: list[LiteratureRecord], focus_terms: list[str], min_citation_count: int, min_year: int, max_seeds: int) -> list[LiteratureRecord] in services/orchestrator/app/engine.py. For each of the top-5 records sorted by _record_relevance_score, extract the Semantic Scholar paper_id from record.metadata['semantic_scholar_id'] if present. Call SemanticScholarReferencesAdapter.fetch_references(). Filter fetched references by citationCount >= min_citation_count (env SPARKIT_BCI_MIN_CITATIONS default 10), year >= min_year (env SPARKIT_BCI_MIN_YEAR default 2000), and at least one focus_term token appearing in title+abstract. Convert surviving references to LiteratureRecord objects using the existing LiteratureRecord constructor pattern from SemanticScholarAdapter. Deduplicate the result list against top_records using the existing _dedupe_records() key logic (DOI or URL). Cap result at max_seeds (env SPARKIT_BCI_MAX_SEEDS default 12). Return empty list and log a warning if fewer than 2 source records had a paper_id.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Wire backward-chain seeding into execute_orchestration() in engine.py. After retrieval_round_1 completes and before retrieval_adaptive_gate evaluation, add a new trace stage 'retrieval_round_1b_backward_chain'. Call _backward_chain_seed with the top-5 records from round_1_records. Gate on env bool SPARKIT_BCI_ENABLED (default True) and budget check via should_stop_early(budget_state). Extend all_records with seed results (extend + dedupe pass). Record in trace: seed_count, source_paper_ids, filtered_out_count. This happens before the adaptive gate so that the gate's new_unique_docs count includes backward-chain seeds (preventing premature early-stop when seeds provide high-quality new docs).",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add extract_claim_slots(abstract_text: str, provider_client, max_tokens: int = 120) -> dict | None in services/orchestrator/app/engine.py. Send a single micro-LLM call to the cheapest available provider (prefer AnthropicClient with claude-haiku-4-5 or OpenAIClient with gpt-4o-mini, selected by checking SPARKIT_SLOT_EXTRACTOR_PROVIDER env, default 'cheapest'). Prompt: 'Return ONLY valid JSON with these keys: {\"method\": \"<one sentence, experimental approach>\", \"key_finding\": \"<one sentence, main result>\", \"quantitative_result\": \"<specific number/threshold/stat or null>\", \"caveat\": \"<one sentence, limitations or null>\"}. Abstract: {abstract_text[:600]}'. Parse the JSON response. On parse failure or provider error, return None. Total token cost per call is approximately 90 input + 80 output tokens at haiku pricing ($0.025/Mprompt, $0.125/Mcompletion), approximately $0.000020 per paper—negligible for 14 ingested papers.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify _select_best_section_chunk() in engine.py to call extract_claim_slots() when the winning chunk has word count >= 40 (sufficient context for structured extraction). If slots is not None, set the claim_text to: '{key_finding}. {quantitative_result}' (truncated to 180 chars) instead of _first_sentence(chunk). Store the full slots dict as record metadata under 'claim_slots'. Fall back to _first_sentence() when extract_claim_slots() returns None, word count < 40, or env SPARKIT_SLOT_EXTRACT=0. Add a new TraceStage artifact entry 'slot_extraction_success_rate' tracking successes/total in the ingestion stage.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Modify _build_option_evidence_packs() in engine.py to use typed slots when available. After computing the existing token overlap score between claim.claim_text and option_text, check if the claim has 'claim_slots' in its metadata. If so, compute an additional key_finding_overlap score: token_overlap(slots['key_finding'], option_tokens) * 1.5. Also compute quantitative_alignment_score: 1.0 if slots['quantitative_result'] is not None and any quantitative token (digit, percent, fold, nM, uM, kg, kDa) appears in option_text, else 0.0. Final pack_score = max(base_overlap, key_finding_overlap) + 0.3 * quantitative_alignment_score. Gate the slot-boosted scoring on env SPARKIT_SLOT_ALIGN_MCQ (default True).",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "low"
    },
    {
      "step": "Update evidence_store.py to persist claim_slots as a JSONB column on the passages table. Add an Alembic migration: ALTER TABLE passages ADD COLUMN claim_slots JSONB NULL. Update EvidenceStore.upsert_passage() to include claim_slots in the INSERT/UPDATE. Update EvidenceStore.get_top_claims() to return claim_slots alongside existing fields. This enables post-hoc analysis of extraction quality across benchmark runs without rerunning ingestion.",
      "owner": "services/orchestrator/app/evidence_store.py + alembic/versions/",
      "effort": "medium"
    },
    {
      "step": "Add benchmark diagnostic: in eval_service/app/evaluator.py, add backward_chain_coverage metric: the fraction of questions where at least one backward-chain seed paper's DOI appears in a manually-curated or LLM-judged 'relevant papers' reference list. For the HLE-25 subset, manually annotate 10 questions with the 2-3 papers most likely to contain the correct answer, then run the evaluator with and without BCI enabled and compare retrieval recall. Add this metric to the drift_thresholds.json guard: regression_guard on backward_chain_seed_rate (fraction of runs that fired at least one seed) dropping below 0.60.",
      "owner": "services/eval_service/app/evaluator.py + docs/",
      "effort": "high"
    }
  ],
  "retrieval_improvements": [
    "Backward citation seeding via Semantic Scholar References API: after Round 1, fetch reference lists of the top-5 retrieved papers and inject influential cited works (citationCount > 10, year >= 2000, focus-term overlap > 0) directly into the evidence pool. This surfaces foundational primary-research papers that contain exact quantitative results—IC50s, rate constants, structural coordinates—which score near-zero in keyword searches but are reliably cited by Round-1 papers. The infrastructure (Semantic Scholar API key, adapter pattern, _dedupe_records) already exists in SPARKIT; only the references endpoint call and a seed injection step need to be added.",
    "Typed abstract slot extraction replacing _first_sentence(): a micro-LLM call (claude-haiku, ~$0.00002/paper) decomposes each ingested abstract into {method, key_finding, quantitative_result, caveat}. The key_finding slot consistently contains the mechanistic or quantitative finding that the first sentence does not, because papers open with background restatements. This makes the evidence content semantically richer for all downstream scoring—MCQ alignment, calibration feature support_coverage, verifier stance detection—without touching any other retrieval stage.",
    "Finding-to-option quantitative alignment scoring in _build_option_evidence_packs: when claim_slots metadata is available, score key_finding against the MCQ option label with a 1.5x weight multiplier over raw claim_text overlap, and add a binary quantitative_alignment_score (0.3 bonus) when quantitative tokens from the result slot appear in the option text. This directly improves evidence pack precision for options that describe numerical effects (e.g., 'increases rate by 10-fold', '< 1 nM potency'), which is exactly the kind of discriminating fact that HLE MCQ options encode.",
    "Citation-weighted seed filtering to prioritize replication-validated evidence: when filtering backward-chain reference candidates, require citationCount > 10 as a proxy for scientific consensus (multiply-replicated findings accumulate citations). Papers with citationCount > 100 receive a seed_priority=high tag that boosts them to the front of ingestion selection. This exploits the existing _select_records_for_ingestion priority sort without modifying its logic, ensuring the most replicated foundational results are ingested first within the per-run document budget.",
    "Semantic Scholar paper_id propagation through LiteratureRecord: currently the SemanticScholarAdapter in retrieval_service/app/adapters.py populates externalIds but does not explicitly extract and store the Semantic Scholar paperId in a normalized record.metadata['semantic_scholar_id'] field. Normalizing this field for all Semantic Scholar results (and cross-referencing against DOI for records from other sources) maximizes the fraction of Round-1 records that can serve as backward-chain seeds. Add this normalization in aggregator.py._build_record() as a one-line metadata injection step."
  ],
  "evaluation_plan": [
    "Backward-chain recall audit on HLE-25: for 10 manually selected HLE bio/chem questions, pre-identify the 2-3 papers most likely to contain the correct answer (via a separate frontier-LLM-guided literature search with no cost constraints). Run SPARKIT on these questions with SPARKIT_BCI_ENABLED=0 and SPARKIT_BCI_ENABLED=1. Measure whether the pre-identified foundational papers appear in retrieved records under each condition. Target: BCI increases foundational-paper recall from <20% to >50% on these 10 questions. Record as new evaluator metric backward_chain_recall.",
    "Slot extraction quality check on ingested passages: after each benchmark run, sample 20 passages where extract_claim_slots() returned non-None. Human-verify (or LLM-grade with a structured rubric) that key_finding correctly captures the main result and quantitative_result correctly captures a specific numeric claim. Target: >85% key_finding accuracy, >90% quantitative_result precision (no hallucinated numbers). Block a BCI release if either metric drops below 80% on a 20-sample spot check.",
    "MCQ confidence margin comparison: for each MCQ question in the HLE-25 benchmark, record the margin (top_option_blended_score - second_option_blended_score) from _select_confident_blended_option under both BCI-enabled and BCI-disabled conditions. A successful BCI implementation should increase the mean margin by >= 0.04 (current threshold for commit is 0.06) and reduce the fraction of questions falling through to the judge fallback by >= 15 percentage points. Log per-question margin deltas in a new benchmark column bci_margin_delta.",
    "Claim slot usage rate in trace: add a benchmark-level aggregate metric slot_extraction_rate (fraction of ingested papers where extract_claim_slots succeeded and key_finding was used as claim_text) to the evaluator output. Require slot_extraction_rate >= 0.60 on HLE runs to confirm the micro-LLM call is not being silently skipped by fallback logic. Add a drift guard in drift_thresholds.json: regression if slot_extraction_rate drops below 0.50.",
    "End-to-end accuracy delta on HLE-bio-chem-25: run 3 full SPARKIT executions (for variance) on the 25-question HLE subset with BCI enabled vs. 3 without (using the same configs, seeds, and question order). Compare mean rubric score. Accept the BCI feature if the enabled condition shows >= 4 percentage point improvement with p < 0.15 under a paired Wilcoxon signed-rank test across the 25 questions. This is a realistic threshold given SPARKIT's known variance on small subsets.",
    "Cost regression check: measure mean per-question cost increase from backward-chain seeding (references API calls) and slot extraction (micro-LLM calls). Target: total cost increase <= $0.08 per question (Semantic Scholar references: free tier / $0.001 estimate, micro-LLM extraction for 14 papers: ~14 * $0.00002 = $0.00028, plus references fetch latency). Flag as cost regression if BCI increases per-question cost by more than 15% compared to the same run config without BCI."
  ],
  "risks": [
    "Semantic Scholar paper_id coverage gap: SPARKIT's aggregator already calls Semantic Scholar as one of six sources, but records from arXiv, Crossref, OpenAlex, and EuropePMC may not carry a semantic_scholar_id in their metadata. If fewer than 3 of the top-5 Round-1 papers have a resolvable paper_id, the backward-chain round yields zero seeds and contributes only latency overhead. Mitigation: add a DOI-to-semantic-scholar-id resolution step (Semantic Scholar supports GET /paper/{doi} lookup by external ID) to upgrade non-SS records before the seed fetch; fall back gracefully to no-op when the lookup fails.",
    "Micro-LLM claim slot hallucination risk: asking claude-haiku-4-5 to extract quantitative_result from text it did not ingest fully (only 600 chars of abstract) risks confabulation of specific numbers. A hallucinated IC50 or gene name in quantitative_result that matches an MCQ option via quantitative_alignment_score could cause a confident wrong answer. Mitigation: (1) restrict quantitative_result to tokens that appear verbatim in the input abstract text (add a post-hoc verbatim-grounding check: reject quantitative_result if none of its numeric tokens appear in abstract_text); (2) set the quantitative_alignment_score bonus to 0.3 (not 1.0) so it cannot override strong lexical evidence for a different option.",
    "Backward-chain seeding loop depth: foundational papers from 2005 also have references; a recursive crawl would explode in cost and latency. Mitigation: strictly limit to one hop (references of Round-1 papers only, never references of references). The SPARKIT_BCI_MAX_SEEDS cap (default 12) and budget guard check before the seeding step provide a hard ceiling on both API calls and injected records.",
    "Semantic Scholar rate limiting: the free tier allows 100 requests/second with an API key but 10/second without. Each backward-chain seeding event makes up to 5 references API calls. At concurrent benchmark runs, this could hit rate limits. Mitigation: apply the existing retry/backoff wrapper already used in SemanticScholarAdapter (2 attempts, 0.6s backoff); add a SPARKIT_BCI_RATE_LIMIT_DELAY_S env (default 0.2s) sleep between reference fetches within a single run; share the existing SS adapter rate-limit state.",
    "Slot extraction latency regression: 14 sequential micro-LLM calls during ingestion (one per paper) at ~1.2s each adds ~17s to the total pipeline latency. This conflicts with tight latency budgets (SPARKIT currently runs HLE questions in ~421s adaptive mode). Mitigation: batch all abstract texts into a single multi-item LLM call using a structured prompt that returns a JSON array of slot objects; also set extract_claim_slots to run only when remaining latency budget > 30s (checked via should_stop_early), skipping to _first_sentence() fallback otherwise.",
    "Evidence store schema migration breakage: adding a claim_slots JSONB column requires an Alembic migration that must be applied before any BCI-enabled run. Existing CI benchmark infrastructure that spins up a fresh PostgreSQL container will need the migration applied. Mitigation: add the Alembic migration to the Makefile db-migrate target and ensure it is idempotent (IF NOT EXISTS guard in the raw SQL); test the migration on a populated evidence_store snapshot before merging."
  ]
}
```
