```json
{
  "proposal_id": "sparkit-evidence-gap-amplification-v1",
  "title": "LLM-Driven Evidence-Gap Requerying with Semantic Cross-Encoder Re-ranking and Citation Graph Traversal",
  "distinctive_angle": "SPARKIT's round-2+ queries are pre-planned heuristically before any evidence is seen. This proposal makes round-2 query generation evidence-conditional: after round 1 a fast LLM reads all collected abstracts, identifies specific factual gaps relative to the question, and generates targeted follow-up queries. This is combined with embedding-based semantic re-ranking to replace the current lexical token-overlap scorer in aggregator.py, and Semantic Scholar citation-graph traversal to surface foundational and recent citing papers that keyword search structurally cannot find.",
  "summary": "Three tightly coupled retrieval upgrades: (1) post-round-1 LLM gap analysis that produces precise, evidence-aware queries for subsequent rounds instead of the current generic 'gap_fill' and 'adversarial' intents; (2) embedding semantic re-ranking of all candidate records before ingestion selection, replacing the _record_relevance_score() token-overlap heuristic; (3) citation-graph amplification via Semantic Scholar /paper/{id}/references and /paper/{id}/citations for the top-5 round-1 papers, injecting foundational and high-recency citing works that keyword search structurally misses. Together these address the three deepest evidence-quality failure modes visible in the current codebase.",
  "reasoning": "Hard HLE-style questions fail for three identifiable reasons in the current SPARKIT pipeline. First, round-2+ queries are generated by _heuristic_retrieval_plan() before any evidence is seen (engine.py:596-652), so they cannot be corrected when round 1 misses the key concept. An LLM that has read the round-1 evidence can identify exactly what is absent and construct semantically precise queries—closing the gap without additional retrieval rounds. Second, _record_relevance_score() (engine.py:121-134) scores by token overlap plus a recency scalar; this systematically under-ranks papers that use domain synonyms or mathematical notation in their abstracts, causing correct evidence to be filtered before ingestion. Embedding similarity captures semantic equivalence that lexical overlap cannot. Third, the retrieval adapters return papers by keyword match; for frontier scientific questions the most authoritative evidence often lives in a paper's references (foundational methods) or in a paper that cites it (recent replication or refutation). Semantic Scholar's free citation API exposes this graph without cost, and the current codebase has no citation-chain traversal at all. Combining these three fixes directly improves the quality and completeness of the evidence that reaches synthesis, which is the primary driver of answer correctness on hard questions.",
  "expected_impact": {
    "accuracy_delta_pct_points": 7,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add embed_records() helper in aggregator.py using OpenAI text-embedding-3-small (or local sentence-transformer as fallback) to score each candidate record's abstract+title against the question embedding. Replace the token_overlap term in _record_relevance_score() with cosine similarity from this embedding. Cache embeddings keyed by (doi or url, question_hash) in Redis/Postgres to avoid re-embedding on retry.",
      "owner": "retrieval_service/app/aggregator.py",
      "effort": "medium"
    },
    {
      "step": "Add gap_analysis_queries() function in engine.py that is called after round-1 ingestion completes. It receives the question text + all round-1 abstract snippets, calls generate_text() on the cheapest available provider (haiku/flash) with a structured prompt requesting JSON {gaps: [{missing_fact: str, targeted_query: str}], max 4 gaps}, and merges the returned targeted_query strings into the query plan for round 2, replacing the pre-built 'gap_fill' intent queries. Wrap in try/except so heuristic fallback is preserved.",
      "owner": "services/orchestrator/app/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add fetch_citation_neighbors() in retrieval_service/app/adapters/semantic_scholar.py that accepts a Semantic Scholar paper_id and returns up to 8 references (most-cited) and up to 8 citing papers (most-recent) via GET https://api.semanticscholar.org/graph/v1/paper/{id}/references and /citations with fields=title,abstract,year,externalIds. Called for the top-5 papers from round 1 (by embedding relevance score). Inject returned records into the round-2 candidate pool before deduplication.",
      "owner": "services/retrieval_service/app/adapters/semantic_scholar.py",
      "effort": "medium"
    },
    {
      "step": "Add SPARKIT_ENABLE_GAP_REQUERY (default 1), SPARKIT_ENABLE_SEMANTIC_RERANK (default 1), SPARKIT_ENABLE_CITATION_GRAPH (default 0, off by default behind flag) to configuration.md and engine.py/aggregator.py. Gate each feature independently so A/B evaluation is clean.",
      "owner": "services/orchestrator/app/engine.py + docs/configuration.md",
      "effort": "low"
    },
    {
      "step": "Add embedding cost accounting in policy.py: text-embedding-3-small costs $0.02/1M tokens; estimate tokens per abstract (avg 150) * candidate pool size (default 40 records) * 4 rounds = ~24k tokens per run = $0.00048. Add as 'embedding' line item in provider_usage.",
      "owner": "services/orchestrator/app/policy.py",
      "effort": "low"
    },
    {
      "step": "Write unit tests in test_synthesis_quality.py: (a) mock embedding returns and verify semantic re-ranking changes record order vs. lexical baseline; (b) mock gap_analysis_queries() and verify round-2 query plan contains LLM-generated queries; (c) mock fetch_citation_neighbors() and verify neighbor records appear in candidate pool after dedup. Add integration smoke test against one HLE question using SPARKIT_ENABLE_CITATION_GRAPH=1.",
      "owner": "services/orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Replace lexical token-overlap relevance scoring (_record_relevance_score in engine.py:121-134) with embedding cosine similarity using OpenAI text-embedding-3-small, eliminating systematic under-ranking of papers that use domain synonyms, mathematical notation, or non-overlapping but semantically equivalent terminology.",
    "Add post-round-1 LLM gap analysis: after ingesting round-1 evidence, call a fast model with all collected abstracts + the original question to identify specific missing facts and generate 2-4 precisely targeted follow-up queries. These replace the heuristic 'gap_fill' intent queries in _build_round_queries_from_plan() (engine.py:753-842), making round 2 evidence-conditional rather than pre-planned.",
    "Add Semantic Scholar citation-graph traversal for top-5 round-1 papers: fetch their references (foundational methods papers) and their citing papers (recent replications, refutations, or extensions). This surfaces evidence that keyword search structurally cannot find—papers whose titles and abstracts do not contain the query terms but are directly cited by retrieved papers.",
    "Gate semantic re-ranking output against the existing adaptive retrieval quality metric (selected_quality in engine.py:1375): log embedding-score vs. token-overlap-score agreement rate per run to detect cases where the two rankings diverge significantly, which signals domain-specific vocabulary drift and can trigger query expansion."
  ],
  "evaluation_plan": [
    "Run HLE Gold Bio/Chem benchmark with SPARKIT_ENABLE_SEMANTIC_RERANK=1 vs. baseline (=0), holding all other config constant. Measure per-question accuracy delta, answer confidence ECE, and mean ingested-document relevance score. Flag as regression if accuracy drops >1 point or ECE worsens.",
    "For gap-requery: run benchmark with SPARKIT_ENABLE_GAP_REQUERY=1 vs. baseline. Log the gap_analysis_queries() output per question to failures_gap_requery.json. Measure whether questions that previously had 'sparse retrieval' abstain triggers (support_coverage < 0.40) are now answered. Target: reduce hard-abstain rate by >=20% without reducing precision.",
    "For citation-graph: sample 25 HLE questions that fail in the baseline and enable SPARKIT_ENABLE_CITATION_GRAPH=1. Check whether any citation-neighbor records were injected into the candidate pool (log injected_neighbor_count in TraceStage artifact). For those questions, measure whether citation neighbors contributed to the final citations list and whether answer accuracy improved vs. baseline.",
    "Add per-run embedding-coverage metric: track what fraction of ingested documents scored higher under semantic ranking than lexical ranking. If fraction < 0.10 consistently, the embedding is not adding information and the feature can be disabled to save latency.",
    "Regression guard: assert that total per-run cost increase from embedding calls stays below $0.005 (well within the $0.02/1M token pricing for ~24k tokens/run) and that mean latency increase from the gap-analysis LLM call stays below 4 seconds (haiku-class model on a 400-token prompt is <2s typical)."
  ],
  "risks": [
    "Gap analysis LLM call adds a synchronous serial step between round 1 and round 2, increasing total latency by 2-5 seconds. If the budget guard fires before round 2 starts (policy.py:160-170), gap requery is silently skipped—this must be logged as a TraceStage artifact to prevent silent degradation.",
    "Semantic Scholar citation API is rate-limited (~100 req/s unauthenticated). Fetching citations for 5 papers per run is safe at low concurrency, but parallel benchmark runs (e.g., 10 concurrent HLE evaluations) could hit rate limits. Implement per-adapter semaphore with configurable SEMANTIC_SCHOLAR_CITATION_CONCURRENCY (default 2).",
    "LLM-generated gap queries may hallucinate specific claims or paper titles as search terms, producing queries that retrieve irrelevant documents. The gap_analysis_queries() prompt must explicitly instruct the model to output only factual knowledge gaps expressed as topic phrases, not as asserted facts or specific citation references. Add a post-parse validator that rejects gap queries containing quotation-wrapped multi-word phrases longer than 6 tokens.",
    "Embedding model cost and latency are near-zero at current scale but become material at high throughput. Cache embeddings keyed by (content_hash, model_id) in the existing Postgres store to avoid re-embedding duplicate abstracts across questions.",
    "Citation-graph injection can introduce off-topic papers if the round-1 top-5 papers are themselves off-topic (a risk on questions where round 1 retrieval is poor). Gate citation injection behind a minimum embedding relevance threshold (e.g., only inject neighbors of papers with embedding_score >= 0.65) to prevent noise amplification."
  ]
}
```
