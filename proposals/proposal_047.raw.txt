Now I have full context of the SPARKIT architecture. Let me construct the precise JSON proposal.

```json
{
  "proposal_id": "SPARKIT-PROP-001",
  "title": "Per-Option Falsifiability Retrieval with Citation-Ancestry Amplification and Paragraph-Level Passage Re-ranking",
  "distinctive_angle": "Invert the retrieval epistemology: for each MCQ option, actively retrieve disconfirming evidence (not just supporting), then select the option whose disconfirmation signal is weakest while its confirmation signal is strongest. Augment the Semantic Scholar adapter to traverse citation ancestry chains, pulling the references of top-hit papers to reach canonical, higher-precision sources that keyword queries miss. Replace section-level evidence with paragraph-level chunked passages scored against each specific option text.",
  "summary": "SPARKIT's current retrieval is symmetric — all intents (primary, options, adversarial, reference) query for confirming evidence, with adversarial queries generic to the question, not per-option. The blended MCQ scoring (0.7*LLM_net + 0.3*lexical in engine.py:1690) treats all options identically in the evidence pass. This proposal adds three orthogonal capability layers: (1) Per-option falsifiability queries injected into RetrievalPlan.intent_queries at planning time, scored as a 'falsification deficit' dimension in blended_scores; (2) Semantic Scholar citation-ancestry amplification — after aggregator.py's primary search, the top-3 papers with S2 paperIds have their /graph/v1/paper/{id}/references fetched, adding depth-2 canonical papers that surface-level keyword queries miss; (3) Paragraph-level evidence chunking — the ingestion section text (currently selected as a full section, up to SPARKIT_INGESTION_MAX_CHARS=10000 chars) is split into 512-char overlapping windows, each scored against the specific option label+text using engine.py's existing _record_relevance_score token-overlap logic, yielding tighter, higher-precision passages for synthesis.",
  "reasoning": "Hard questions on HLE-style benchmarks (chemistry mechanisms, exact biomedical facts, numerical thresholds) fail SPARKIT in two specific modes: (a) retrieval misses the right paper because surface keyword overlap is low — the canonical paper uses different terminology, and only its citation descendants use the familiar terms; (b) MCQ scoring is fooled by options that have superficially similar supporting evidence but differ in a single technically critical qualifier. Falsifiability queries exploit MCQ structure — the wrong answers have specific scientific counter-evidence that exists in the literature (e.g., if the answer is 'compound X inhibits pathway Y', there will be papers showing 'compound X does NOT inhibit pathway Y under these conditions' for each wrong option, but not for the correct one). Citation ancestry catches the canonical paper (e.g., the 1987 paper that defined the mechanism being asked about) which is cited by 50 modern papers but not returned directly by keyword search. Paragraph chunking prevents dilution — currently a 10000-char Results section is treated as one evidence unit, hiding the 3-sentence paragraph that directly answers the question.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add search_semantic_scholar_references(paper_id, limit=15) to services/retrieval_service/app/adapters.py. Call GET https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=title,abstract,year,authors,externalIds,openAccessPdf&limit={limit}. Extract paperId from existing search_semantic_scholar results via item.get('paperId'). Store paperId on LiteratureRecord by extending its schema with an optional s2_paper_id: str | None field in services/retrieval_service/app/models.py.",
      "owner": "retrieval-service",
      "effort": "medium"
    },
    {
      "step": "Add citation amplification pass in services/retrieval_service/app/aggregator.py inside search_literature(), after the primary ranked+deduped list is assembled. For the top-3 records where s2_paper_id is not None, call search_semantic_scholar_references in parallel (threadpool, 8s timeout per call). Merge returned references into combined, re-run _relevance_score ranking and deduplication. Cap amplified records at 10 total to avoid noise. Gate behind SPARKIT_ENABLE_CITATION_AMPLIFICATION env var (default 1).",
      "owner": "retrieval-service",
      "effort": "medium"
    },
    {
      "step": "Add per-option falsifiability query generation in engine.py's _heuristic_retrieval_plan() and _plan_retrieval_llm() functions. For each answer_choice label+text, generate 2 queries: '{option_text} incorrect evidence', '{option_text} contradicted by'. Add these to intent_queries as 'falsify' key (list of all per-option disproof queries). In _build_round_queries_from_plan(), route 'falsify' queries into a new retrieval_round_falsify stage for standard mode, and retrieval_falsify stage for research_max mode. Track returned records separately in records_by_round under their stage key.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add falsification_scores dict to the MCQ scoring block in engine.py (around line 1686). For each option label, count how many falsify-round records have token overlap > 0.15 with the option text — this is the falsification_hit_count. Compute falsification_deficit = 1.0 / (1.0 + falsification_hit_count) so options with zero falsifying evidence score 1.0 and options with strong falsifying evidence score low. Modify blended score formula from '0.7 * LLM_net + 0.3 * lexical' to '0.55 * LLM_net + 0.25 * lexical + 0.20 * falsification_deficit'. Expose falsification_scores and falsification_deficit in the synthesis stage artifact dict alongside existing blended_scores.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add paragraph-level chunking in the ingestion phase of execute_orchestration() in engine.py (around line 1500-1520). After the best_section and section_text are selected, split section_text into overlapping 512-char windows with 64-char stride. Score each chunk against the question + answer_choices values using _tokenize() token overlap. Select the top-2 chunks by score. Store both chunks as separate passages in evidence_store.upsert_passage(), each linked to the same claim. Update _build_option_dossiers() in engine.py to receive the chunk-level passages instead of full section passages, enabling tighter snippet extraction for support_snippets and counter_snippets.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add SPARKIT_ENABLE_CITATION_AMPLIFICATION, SPARKIT_CITATION_AMP_TOP_K (default 3), SPARKIT_CITATION_AMP_MAX_REFS (default 10), and SPARKIT_PARA_CHUNK_SIZE (default 512), SPARKIT_PARA_CHUNK_STRIDE (default 64) to the configuration documentation in docs/configuration.md. Add smoke tests: test that falsify intent queries are generated when answer_choices is non-empty; test that citation amplification merges records correctly; test that paragraph chunking produces chunks of correct size and the top-2 chunks are selected by score.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Run A/B comparison on HLE-gold-149: baseline SPARKIT routed_frontier_plus vs. SPARKIT with all three features enabled. Track: overall accuracy, accuracy on MCQ-only subset, average evidence_count per run, average support_coverage, average falsification_deficit of selected vs. rejected options, and total cost per run. Persist results in benchmarks/ with a new manifest tag 'falsify-citamp-parachunk-v1'.",
      "owner": "eval",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Citation ancestry amplification via Semantic Scholar /graph/v1/paper/{id}/references: extends the retrieval frontier from surface keyword matches to canonical papers 1-2 citation hops deeper, which are often more authoritative and specifically define the mechanism or fact being tested. The existing search_semantic_scholar adapter in adapters.py already retrieves paperId metadata; this adds a second API call per top-hit to fetch its reference list, merged into the aggregator's combined record pool before deduplication.",
    "Per-option falsifiability retrieval: instead of one generic adversarial round ('{lead} contradictory findings'), generate 2 targeted disproof queries per MCQ option and execute them as a dedicated falsify retrieval stage. Records returned by falsify queries that have high token overlap with a specific option are strong signals that option is wrong — the literature actively contradicts it. This transforms the adversarial intent from a generic credibility check into a discriminative per-option signal.",
    "Paragraph-level passage chunking replacing section-level evidence selection: the current ingestion selects an entire section (up to 10000 chars) as the evidence unit. Splitting into 512-char overlapping windows with token-overlap scoring against the question+answer_choices produces 2-3 sentence spans that are directly question-answering, reducing noise in claim_texts fed to synthesis and making support_snippets/counter_snippets in option dossiers more precise.",
    "Option-text-aware relevance scoring in _rank_and_select_records() (engine.py line 144): currently boost_terms combines focus_terms and question segments. Extend to also include answer_choices.values() as boost terms so that records mentioning specific option text receive a relevance bonus, surfacing papers that directly compare or evaluate the answer candidates.",
    "Temporal stratification of retrieval results: for questions involving specific compounds, mechanisms, or numerical thresholds, the decisive paper is often the original publication (pre-2010), not a recent review. Add a recency_penalty toggle that deprioritizes post-2020 papers when the question contains year-specific cues or when the LLM retrieval planner tags the question as 'historical fact' task_type, improving selection of canonical sources over secondary citations."
  ],
  "evaluation_plan": [
    "MCQ option discrimination metric: for each HLE-gold MCQ question, record the blended_score gap between the selected option and the second-best option (margin). Compare margin distributions before and after the proposal. A wider margin distribution indicates the system is more confident and correct, not just guessing. Track correlation between margin > 0.15 and answer correctness.",
    "Falsification signal calibration check: for each MCQ question where the system selected the correct answer, verify that the correct option's falsification_deficit is higher than the average falsification_deficit of eliminated options. This validates that the falsify queries are returning disconfirming evidence for wrong options and not for the correct one. Target: correct option should have falsification_deficit > eliminated options in >= 60% of cases.",
    "Citation amplification coverage check: for each run with citation amplification enabled, log how many of the final ingested documents came from the amplification pass (not the primary search). Track 'amplification_hit_rate' = amplified_docs_ingested / total_docs_ingested. A rate > 10% indicates the feature is contributing new evidence. Also verify that amplified records score higher on _record_relevance_score than the median primary-search record, confirming quality improvement.",
    "Paragraph chunk precision audit: for 20 randomly sampled hard questions, manually inspect the top-2 selected paragraph chunks vs. the original full-section evidence. Score whether the chunk contains the specific fact needed (1) or is generic background (0). Compare this precision score against the section-level baseline. Target: chunk precision >= 0.65 vs. section precision baseline of ~0.35.",
    "End-to-end accuracy regression on STEM-Exam-200: run the full benchmark on the existing STEM-Exam-200 set to confirm no accuracy regression on easier questions while measuring uplift on the hard subset (rubric_score < 0.4 on baseline). Cost and latency per question must be tracked against the SPARKIT_MAX_COST_USD and SPARKIT_MAX_LATENCY_S policy guards to ensure budget compliance.",
    "Abstain rate impact check: verify that the hard abstain rate (questions returning 'insufficient evidence') does not increase significantly (< +3 percentage points) after adding falsify queries, since falsify records should not negatively contaminate support_coverage or push contradiction_flags above the abstain threshold in the quality gates logic."
  ],
  "risks": [
    "Semantic Scholar API rate limits: the /graph/v1/paper/{id}/references endpoint has strict rate limits (100 req/5min unauthenticated). Citation amplification on top-3 papers per retrieval round in a multi-round workflow could hit limits under parallel benchmark runs. Mitigation: use the S2_API_KEY env var if available (same pattern as existing adapters), add per-request jitter (0.2-0.5s), and gate amplification to only fire on the primary retrieval round (not gap-fill or adversarial rounds).",
    "Falsification query pollution: if falsify-round records happen to use option-specific language that is common in general scientific writing (e.g., 'protein X does not bind Y' is a common negative phrasing in many unrelated papers), falsification_hit_counts could be inflated for all options, collapsing falsification_deficit differences and degrading the signal. Mitigation: require minimum token overlap threshold of 0.20 (not 0.15) and require at least 2 tokens of overlap beyond single-word matches before counting a falsification hit.",
    "Paragraph chunking context loss: some experimental findings span multiple paragraphs and lose meaning when split at 512-char boundaries. A single paragraph may say 'X inhibits Y' but the preceding paragraph establishes the critical context 'under condition Z'. Splitting at 512 chars with 64-char stride may select the first paragraph and miss the context. Mitigation: use a larger stride overlap (128 chars) and include the section header (if present) as a prefix to every chunk, preserving document-level context in the claim text.",
    "Latency increase from additional retrieval stages and citation API calls: the falsify retrieval stage adds 1 extra round equivalent, and citation amplification adds 3 serial/parallel API calls per round. In research_max mode with 5 rounds, this could add 15-25s of wall-clock latency. Mitigation: run citation amplification calls in a threadpool with a hard 8s timeout per paper; skip amplification if the budget state already shows cost or latency above 70% of limits (integrating with the existing should_stop_early() policy guard in policy.py).",
    "Blended score weight regression: changing the blended formula from 0.7/0.3 (LLM/lexical) to 0.55/0.25/0.20 (LLM/lexical/falsification_deficit) could degrade performance on questions where the falsification signal is uninformative (e.g., very niche topics with sparse literature). Mitigation: make the falsification weight configurable via SPARKIT_MCQ_FALSIFY_WEIGHT (default 0.20) and fall back to the existing 0.7/0.3 split when the falsify stage returned zero records (falsification_deficit would be uniformly 1.0, making the weight irrelevant, but the formula should be re-normalized to sum to 1.0 in that degenerate case)."
  ]
}
```
