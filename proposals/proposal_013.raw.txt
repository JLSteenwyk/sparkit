Now I have everything I need for a deep, architecture-specific proposal. Let me compose it.

```json
{
  "proposal_id": "SPARKIT-PROP-022",
  "title": "Option-Pair Contrastive Retrieval with Per-Option Divergence Promotion and Saturated-Dossier Fill",
  "distinctive_angle": "SPARKIT's retrieval is question-centric: all MCQ options compete for the same evidence pool retrieved against the question stem, then evidence is sorted by per-option token overlap inside _build_option_evidence_packs and _build_option_dossiers. This means a paper equally relevant to all four options scores high for all four, providing zero discriminative signal. This proposal replaces that symmetry-blind selection with three complementary mechanisms: (1) pairwise contrastive queries that seek evidence explicitly distinguishing specific option pairs rather than supporting individual options, (2) a record-level discriminativeness bonus inside _select_records_for_ingestion that boosts records with unequal per-option overlap, and (3) zero-dossier fill—detecting options whose dossier_score is 0 after evidence pack construction and running a targeted three-query mini-retrieval for only those options before synthesis fires.",
  "summary": "In hard STEM MCQs options often share vocabulary and describe related phenomena (e.g., four reaction mechanisms for the same substrate), so broad retrieval finds the right papers but cannot surface the specific passage that distinguishes option A from B. The current _build_option_hypothesis_queries at engine.py:479-486 emits only support queries ({stem} {choice}, {stem} evidence for {choice}), never contrastive queries. _select_records_for_ingestion at engine.py:135-168 sorts by _record_relevance_score (question-level token overlap) and source diversity—it has no signal about whether a record favors one option over another. _build_option_dossiers at engine.py:391-433 already penalizes cross-option overlap in its scoring formula (support_score = 2.0*own - 1.2*other) but operates entirely on the pre-selected evidence pool, so if discriminative papers were never retrieved or selected they cannot rescue the dossier. This proposal closes the pipeline gap by injecting discriminativeness awareness at the retrieval and selection stages, not just the dossier-building stage.",
  "reasoning": "Audit of engine.py reveals that three pipeline stages are unaware of per-option discrimination needs. First, _build_round_queries_from_plan (line 753) for research_max emits retrieval_option_hypotheses from _build_option_hypothesis_queries—pure support queries. For a question whose options are A: SN2, B: radical, C: E2, D: acid-catalysed, the queries become 'stem SN2', 'stem evidence for SN2', etc., which all retrieve the same corpus of organic chemistry papers; none specifically retrieves 'SN2 vs radical selectivity comparison experimental'. Second, _select_records_for_ingestion sorts by _record_relevance_score (2*title_overlap + 1*abstract_overlap + 0.25*recency) and picks the top-N after source diversity balancing. A review paper that mentions all four mechanisms scores higher than a primary research paper that specifically demonstrates SN2 selectivity—the review paper is selected first, diluting the evidence pool with ambiguous encyclopedic content. Third, the adaptive retrieval gate (lines 1430-1454) measures low_novelty as new_unique_docs < adaptive_min_new_docs and low_gain as quality_gain < adaptive_min_quality_gain where quality_gain is a question-level relevance delta—it does not measure whether the new docs add per-option discriminative power. Together these three deficits mean that for the hardest questions (where all options are plausible and share vocabulary), the system retrieves the right domain papers, selects the broadest review papers, and synthesizes from an ambiguous evidence pool. Pairwise contrastive queries bypass all three stages: they retrieve papers that exist in the literature specifically to compare mechanisms (e.g., 'nucleophilic vs radical addition comparison kinetics'), record discriminativeness bonuses shift selection toward primary research papers, and zero-dossier fill closes the final gap when a specific option has no supporting evidence in the pool.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _record_discriminativeness_bonus(record: LiteratureRecord, answer_choices: dict[str, str], stem_tokens: set[str], epsilon: float = 1e-3) -> float to engine.py. Tokenize record title+abstract. For each option label compute overlap = len(option_tokens & record_tokens). Compute max_overlap = max(overlaps), mean_overlap = sum(overlaps)/len(overlaps). Return (max_overlap - mean_overlap) / (mean_overlap + epsilon). This is 0 for a record equally relevant to all options and positive for a record concentrated on one option. Cap return at 5.0.",
      "owner": "engine.py",
      "effort": "low"
    },
    {
      "step": "Modify _select_records_for_ingestion signature to accept answer_choices: dict[str, str] | None = None and a discriminativeness_weight: float = 0.0 parameter (default 0 preserves existing behavior). When answer_choices is provided and non-empty, add discriminativeness_weight * _record_discriminativeness_bonus(item, answer_choices, stem_tokens) to the sort key in the scored list. Expose weight via _env_float('SPARKIT_MCQ_DISCRIMINATIVENESS_WEIGHT', default=0.35, minimum=0.0). Update all three call sites in execute_orchestration to pass retrieval_plan.answer_choices and the env weight for MCQ questions.",
      "owner": "engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_contrastive_option_queries(stem: str, answer_choices: dict[str, str], focus_terms: list[str], max_pairs: int = 3, max_items: int = 8) -> list[str] to engine.py. For each pair of options (A, B) sorted by label, generate query '{choice_A_key_phrase} versus {choice_B_key_phrase} {shared_anchor}' where choice_X_key_phrase = first 4 tokens of _tokenize(choice_X) joined by space, and shared_anchor = first focus_term or first stem token. Limit to max_pairs pairs (prioritize pairs where choice texts share fewest tokens, i.e., most distinct). Dedupe via _dedupe_queries(queries, max_items=max_items). Return empty list when len(answer_choices) < 2.",
      "owner": "engine.py",
      "effort": "low"
    },
    {
      "step": "In _build_round_queries_from_plan (line 753), for research_max mode insert a new 'retrieval_contrastive' round after 'retrieval_option_hypotheses', populated by _build_contrastive_option_queries(plan segments[0], plan.answer_choices, plan.focus_terms). Gate on _env_bool('SPARKIT_MCQ_CONTRASTIVE_RETRIEVAL', default=True) and non-empty answer_choices. For non-research_max modes, insert it after 'retrieval_round_option_hypotheses' only if answer_choices is non-empty and env bool is set.",
      "owner": "engine.py",
      "effort": "medium"
    },
    {
      "step": "Add zero-dossier fill logic in execute_orchestration immediately after the existing _build_option_dossiers call (around line 1643). Detect which labels have dossier_score == 0 in option_dossiers. For each zero-dossier label (up to 2), gate on _env_bool('SPARKIT_MCQ_FILL_ZERO_DOSSIER', default=True) and budget check via should_stop_early. Generate 3 targeted queries: '{stem_question} {choice_text}', '{choice_text} experimental evidence', '{choice_text} mechanism specificity'. Call search_literature for each (per_source=2), extend all_records with deduped new records, re-ingest new records via the existing ingestion block, and rebuild option_dossiers. Record fill results in a new TraceStage 'mcq_zero_dossier_fill' with artifacts: filled_labels, fill_queries, new_docs.",
      "owner": "engine.py",
      "effort": "high"
    },
    {
      "step": "Add three new env vars following the existing _env_float/_env_bool/_env_int pattern: SPARKIT_MCQ_DISCRIMINATIVENESS_WEIGHT (float, default 0.35), SPARKIT_MCQ_CONTRASTIVE_RETRIEVAL (bool, default True), SPARKIT_MCQ_FILL_ZERO_DOSSIER (bool, default True). Document in a comment block near the existing SPARKIT_MCQ_BLEND_MARGIN env var at engine.py:1705.",
      "owner": "engine.py",
      "effort": "low"
    },
    {
      "step": "Add tests to test_synthesis_quality.py: (a) test_record_discriminativeness_bonus_favors_option_specific_records: create 2 records—one mentioning all 4 option terms equally, one mentioning only option A terms—assert bonus for option-A record > bonus for the generic record by at least 0.5; (b) test_contrastive_queries_are_distinct_from_hypothesis_queries: assert that for a 4-choice MCQ, _build_contrastive_option_queries returns queries not already in _build_option_hypothesis_queries output; (c) test_zero_dossier_fill_triggers_on_empty_dossier: mock search_literature to return 1 new record when called for a zero-dossier option, assert option_dossiers after fill has dossier_score > 0 for that label.",
      "owner": "services/orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Pairwise contrastive queries ('option_A_phrase versus option_B_phrase shared_anchor') surface comparison papers—systematic reviews of competing mechanisms, papers reporting selectivity experiments—that never appear in hypothesis queries ('{stem} {choice}') because comparison papers often do not mention any individual mechanism in their titles with the specificity needed to clear token overlap thresholds in _record_relevance_score.",
    "Record discriminativeness bonus in _select_records_for_ingestion (SPARKIT_MCQ_DISCRIMINATIVENESS_WEIGHT=0.35) demotes review papers that discuss all options equally and promotes primary research papers that report experimental results for one specific mechanism. This shifts the ingested evidence pool from encyclopedic but ambiguous to specific but directly diagnostic, improving the dossier signal-to-noise ratio without increasing retrieval volume.",
    "Zero-dossier fill detects the structural failure mode where a specific option has zero evidence after the full retrieval pipeline (dossier_score == 0 in _build_option_dossiers), then runs a last-resort targeted mini-retrieval of exactly 3 queries for that option. This directly rescues the case where the correct answer is option D but all retrieved papers focus on the incorrect options A/B/C, which is a known failure mode on hard MCQs where one option is counter-intuitive or domain-specific.",
    "Contrastive queries at per_source=2 per query access all five live retrieval adapters (arxiv, crossref, semantic_scholar, openalex, europe_pmc) with semantically novel queries, expanding the effective search space beyond what the current retrieval_round_option_hypotheses achieves with its {stem} {choice_text} templates that are lexically similar to the primary round queries and frequently deduplicate to the same result set.",
    "The discriminativeness bonus acts as a diversity mechanism orthogonal to the existing source-diversity first pass in _select_records_for_ingestion: where the existing first pass ensures no single API dominates, the bonus ensures no single MCQ option topic dominates the evidence pool—both diversities are necessary for hard multi-option STEM questions."
  ],
  "evaluation_plan": [
    "Run HLE-25 bio+chem subset in research_max mode with SPARKIT_MCQ_CONTRASTIVE_RETRIEVAL=1 SPARKIT_MCQ_DISCRIMINATIVENESS_WEIGHT=0.35 SPARKIT_MCQ_FILL_ZERO_DOSSIER=1 versus baseline with all three flags disabled. Report per-question binary accuracy, mean accuracy delta, and 95% bootstrap CI over the 25-question sample. Target: +4 pct pts mean accuracy delta.",
    "Evidence concentration audit: for each run, compute 'option evidence Gini coefficient'—for each ingested record, measure per-option discriminativeness_bonus, compute Gini across all records in the pool. Higher Gini = more concentrated, discriminative pool. Assert that the treatment run shows Gini > 1.10 * baseline Gini on average across all MCQ questions, indicating the discriminativeness bonus and contrastive retrieval successfully shifted the pool toward option-specific evidence.",
    "Zero-dossier fill precision: across all questions where the fill round triggers, compute fraction of newly ingested records (from fill) where the targeted option's dossier_score increases by at least 1.0 after fill. Target >50% fill precision to confirm fill queries are landing on relevant documents rather than adding noise.",
    "Regression suite: run all existing tests in test_synthesis_quality.py—specifically test_option_dossiers_capture_support_and_counter, test_select_option_from_dossiers_requires_margin, test_select_confident_blended_option_requires_margin—with the new code paths; assert zero regressions. Additionally assert that when answer_choices is empty (non-MCQ call), _select_records_for_ingestion produces identical output with and without the discriminativeness_weight parameter.",
    "Budget ceiling compliance: for 10 questions with max_cost_usd=3.0 in research_max mode with all three flags enabled, assert spent_usd at end of execute_orchestration never exceeds 3.0. Additionally measure mean latency delta per question and assert it stays below 20s increase over baseline (the contrastive round adds ~4 queries * 5 sources = ~20 API calls which at 0.5s each is ~10s parallelized to ~2-3s, plus fill adds up to ~15 API calls per zero-dossier option).",
    "Discriminativeness bonus calibration: for a synthetic test with 4 records where record_0 mentions only option_A terms, records_1-3 mention all option terms equally, assert _select_records_for_ingestion with SPARKIT_MCQ_DISCRIMINATIVENESS_WEIGHT=0.35 ranks record_0 first even when records_1-3 have higher question-level token overlap, confirming the bonus overrides the relevance score when discriminativeness is sufficiently high."
  ],
  "risks": [
    "When MCQ option texts are numerically differentiated (e.g., A: 3.2 eV, B: 3.4 eV, C: 3.6 eV, D: 3.8 eV), the contrastive queries collapse to near-identical strings ('3 versus 3 shared_anchor') and _dedupe_queries reduces them to 1-2 unique queries that are no different from hypothesis queries; the discriminativeness bonus similarly cannot distinguish records since all four options are single-token. A guard checking min_edit_distance(choice_A_tokens, choice_B_tokens) >= 2 before emitting a pair would suppress useless contrastive queries in this case.",
    "The zero-dossier fill adds up to 15 additional API calls per zero-dossier option (3 queries * 5 sources). If multiple options have dossier_score == 0 (common when evidence retrieval fails broadly for a domain), fill could trigger for 3-4 options, adding ~60 API calls and potentially triggering should_stop_early before synthesis on tight-budget runs. The 'up to 2' cap in the implementation plan partially mitigates this but should be configurable via SPARKIT_MCQ_FILL_ZERO_DOSSIER_MAX_OPTIONS (default 2).",
    "The discriminativeness bonus promotes records with high max/mean overlap ratio, which can favor records that are UNIQUELY irrelevant to 3 options but also uniquely relevant to 1 option. For questions where option texts are very short (1-2 tokens), a record that happens to contain option A text for unrelated reasons could get promoted. Adding a minimum absolute overlap floor (own_overlap >= 2 before applying the bonus) mitigates false promotions.",
    "Contrastive queries require at least 2 non-empty option texts to generate meaningful pairs. If _split_question_and_choices fails to parse choices (possible for non-standard MCQ formatting), _build_contrastive_option_queries returns [] and the round is silently skipped—this is safe but means the improvement is inactive for questions that need it. The existing _question_has_answer_choices guard at engine.py:231 can gate the contrastive round to prevent no-op API overhead.",
    "The per-option discriminativeness bonus modifies _select_records_for_ingestion which is also used in the adaptive retrieval gate comparison (lines 1417-1427) via _avg_relevance. Adding the bonus to selection but not to relevance scoring could create a subtle mismatch: selected_now uses discriminativeness-boosted selection while prev_selected_quality uses question-level avg_relevance. This could cause the adaptive gate to under-report quality_gain and run extra retrieval rounds unnecessarily. The fix is to compute quality_gain on the question-level score only, separate from the discriminativeness-augmented selection."
  ]
}
```
