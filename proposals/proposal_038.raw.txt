```json
{
  "proposal_id": "SPARKIT-2026-EGM-001",
  "title": "Cascading Belief-Conditioned Retrieval via Epistemic Gap Mining and Contrastive Document Discrimination",
  "distinctive_angle": "Replace SPARKIT's fully upfront query planning and fixed adversarial rounds with a two-phase loop: after round 1 retrieval, run a lightweight preliminary synthesis to mine explicit epistemic gaps (specific facts needed but not yet evidenced), then generate belief-conditioned gap queries and score all follow-up documents by their discriminative power—how sharply they shift confidence across competing answer hypotheses—rather than by bare keyword overlap. This transforms retrieval from topic coverage into targeted uncertainty reduction.",
  "summary": "SPARKIT's current multi-round retrieval uses pre-planned intent-based queries (primary, methods, adversarial, reference) and scores documents by token overlap with the question. For hard questions, critical evidence is often non-obvious: it requires knowing what is still unknown after seeing partial evidence and prioritizing documents that distinguish plausible competing answers. This proposal adds: (1) an epistemic gap analysis LLM call after round 1 that produces a structured list of specific unresolved knowledge gaps; (2) a gap-query generator that converts those gaps into targeted retrieval queries, replacing or supplementing the planned adversarial/reference rounds 3-4; (3) a contrastive discriminative scoring function applied to all ingested documents that rewards evidence uniquely useful for distinguishing between competing answer options; (4) an updated adaptive gating condition that stops retrieval when expected discriminative gain falls below a threshold rather than merely counting new documents; and (5) cross-hypothesis evidence triangulation that prevents any single source type from dominating high-confidence claims.",
  "reasoning": "Hard benchmark questions like HLE-gold require integrating evidence that is domain-specific, temporally precise, and mechanistically detailed. SPARKIT's current pipeline makes two structural errors: (a) query planning is fully upfront before any evidence is seen, meaning planned queries cannot adapt to what is actually missing after partial retrieval; (b) document scoring is question-centric (relevance to query surface terms) rather than hypothesis-discriminating (ability to distinguish the correct from incorrect answers). The current adversarial rounds use static keyword templates like 'contradictory findings' rather than probing actual gaps in the evidence collected. By mining epistemic gaps from partial evidence, the system generates queries for exactly what is missing, not what the surface question suggests. By scoring documents for discriminative power—how much they change the likelihood of one hypothesis over another—the system selects evidence that actually moves the answer needle. The current MCQ lexical blending (0.70 LLM + 0.30 lexical) is similarly improved by substituting discriminative power for raw lexical overlap, making the blending function hypothesis-aware. These changes compound: better targeted retrieval feeds higher-quality evidence into synthesis, reducing noise and increasing signal per ingested document while adaptive gating becomes more precise, stopping when uncertainty is resolved rather than when a doc-count threshold is met.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "mixed",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add run_epistemic_gap_analysis(question, decomposition, evidence_bullets, provider) to engine.py: fires after round 1 ingestion using the planning provider with a structured prompt requesting up to 6 specific unresolved knowledge gaps as JSON objects, each with fields gap_id, gap_description, and gap_type from the enum [factual, mechanistic, quantitative, temporal]. Returns empty list on LLM failure so downstream pipeline degrades gracefully to existing planned queries.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add generate_gap_queries(gaps, question, decomposition) in engine.py: converts each gap into 1-2 targeted retrieval query strings using gap-type-specific string templates. Template examples: factual → '{entity} confirmed {property} study evidence'; quantitative → '{metric} measured {system} experimental value'; mechanistic → '{process} mechanism {system} pathway confirmed'; temporal → '{phenomenon} {year_range} timeline evidence. Gap queries replace or supplement the existing planned adversarial and reference round query lists (rounds 3-4 in research_max mode).",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add score_discriminative_power(record, hypotheses, existing_evidence_tokens) in engine.py: for MCQ questions, computes per-document discriminative power as max(0, overlap_with_best_matching_hypothesis - max_overlap_with_any_other_hypothesis) normalized to [0,1], where overlap is the Jaccard coefficient between document tokens and hypothesis text tokens. For non-MCQ, computes information novelty as (1 - Jaccard(new_doc_tokens, union_of_all_ingested_tokens)), rewarding evidence diversity.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Update section chunk scoring in select_best_section: replace the current flat weighting (question tokens 1.4x, focus terms 1.9x) with a three-component normalized score: relevance_component = 0.55 * (question_token_overlap * 1.4 + focus_term_overlap * 1.9), discriminative_component = 0.30 * discriminative_power_score, recency_component = 0.15 * recency_bonus. This makes chunk selection reward both topical match and hypothesis discrimination.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Update MCQ blended scoring: replace the 0.30 * lexical_score component with 0.20 * lexical_score + 0.10 * avg_discriminative_power_of_option_citations, where avg_discriminative_power_of_option_citations is the mean discriminative_power score of all documents cited as supporting that option. Preserves the existing 0.70 LLM weight. Update the minimum margin threshold check from 0.06 to 0.07 to account for the tighter discriminative signal.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Update adaptive retrieval gating: add a secondary stop condition alongside the existing new-doc and quality-gain conditions. Track per-round mean discriminative power of newly ingested documents. If mean_discriminative_power < SPARKIT_ADAPTIVE_MIN_DISCRIMINATIVE_GAIN (default 0.04) for two consecutive rounds AND the existing new-doc condition is also met, trigger early stop. Record this in the retrieval_adaptive_gate trace stage with reason discriminative_gain_exhausted.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add cross-source claim triangulation guard to high-confidence claim acceptance: claims with base confidence >= 0.78 that support a single MCQ option must be backed by documents from at least SPARKIT_CROSS_SOURCE_MIN_TYPES (default 2) distinct retrieval source types before they receive full confidence weight. Claims backed by only one source type receive a confidence penalty of -0.08. Source type is derived from the existing record source field (arxiv, crossref, semantic_scholar, openalex, europepmc, brave).",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add config env vars with documented defaults: SPARKIT_ENABLE_GAP_MINING=1, SPARKIT_GAP_MINING_MAX_GAPS=6, SPARKIT_ADAPTIVE_MIN_DISCRIMINATIVE_GAIN=0.04, SPARKIT_DISCRIMINATIVE_POWER_WEIGHT=0.10, SPARKIT_CROSS_SOURCE_MIN_TYPES=2. Add gap_mining and discriminative_scoring trace stage records to observability, logging gap count, gap types, per-round mean discriminative power, and whether gap sub-rounds fired.",
      "owner": "orchestrator/engine.py, docs/",
      "effort": "low"
    },
    {
      "step": "Add unit tests in test_synthesis_quality.py: (a) mock LLM returning structured gap JSON and verify generate_gap_queries produces correctly typed query strings for each gap_type; (b) score_discriminative_power with synthetic MCQ hypotheses where one doc clearly favors one hypothesis; (c) verify blended MCQ score update integrates discriminative component correctly; (d) verify adaptive gate fires early when discriminative power metric drops below threshold for two consecutive rounds.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Gap-conditioned query generation: after round 1 ingestion, an LLM call identifies up to 6 specific unresolved epistemic gaps typed as factual, mechanistic, quantitative, or temporal. Each gap type generates a structured retrieval query from a domain-aware template that targets the missing evidence precisely. For example, a question about an enzyme kinetics constant generates a quantitative gap query like 'Km value cytochrome P450 substrate experimental measurement' rather than the generic static template 'contradictory findings [topic]'. This replaces static adversarial round queries whose keyword coverage is question-agnostic.",
    "Discriminative document scoring: every retrieved document beyond round 1 is scored for discriminative power—the margin between its token overlap with the best-matching answer hypothesis and its maximum overlap with any other hypothesis for MCQ questions. Documents that confirm all hypotheses equally (low discrimination) are downweighted in chunk selection. Documents that provide evidence specific to one hypothesis are upweighted. This scoring dimension is incorporated into section chunk selection alongside the existing relevance and recency components, replacing the flat keyword-weighting scheme that treats all retrieved content as equally useful.",
    "Information novelty weighting for open-ended queries: for non-MCQ questions where competing hypotheses are not enumerated, each new document is scored by its Jaccard novelty relative to the union of all already-ingested evidence token sets. Documents with high overlap to already-ingested content are deprioritized during the target-doc selection pass, enforcing semantic diversity across the ingested corpus. This directly addresses the failure mode where academic databases return clusters of highly-citing papers around a few landmark studies, creating an evidence echo chamber that confirms a popular but possibly incorrect interpretation.",
    "Gap saturation tracking with targeted sub-rounds: after each round where gap queries fired, compute a gap saturation score per epistemic gap as the fraction of the gap's characteristic terms found in ingested evidence. Gaps with saturation below 0.50 after their targeted retrieval round trigger a focused sub-round scoped to that specific gap. The sub-round relaxes source-type constraints to include source types not yet sampled for that gap, expanding beyond arxiv and Semantic Scholar to Europe PMC for biological mechanisms or Crossref for exact measurement citations. Sub-rounds count against the existing adaptive max-rounds budget and only fire if cost/latency reserve allows.",
    "Cross-hypothesis evidence triangulation: high-confidence claims (base confidence >= 0.78) supporting any single MCQ option must be backed by documents from at least two distinct retrieval source types before receiving full confidence weight. A claim backed by three arXiv papers counts as one source type, not three independent confirmations. This prevents the existing 50% per-source dominance cap from being satisfied at the record level while still having a single academic index dominate the semantic content of ingested evidence. Claims with only single-source backing receive a confidence penalty and are flagged in the verification stage for deeper contradiction probing."
  ],
  "evaluation_plan": [
    "Gap coverage rate on HLE-gold benchmark: for each question where gap mining fires (SPARKIT_ENABLE_GAP_MINING=1), measure what fraction of identified epistemic gaps have at least one ingested document with gap-term token overlap >= 0.40. Log gap coverage per question. Target: >= 70% mean gap coverage rate across the HLE-gold subset, confirming that gap-targeted queries successfully locate relevant documents.",
    "Discriminative power vs. answer accuracy correlation: run the full HLE-gold benchmark and log per-question mean discriminative power of ingested documents. Compute Spearman rank correlation between mean_discriminative_power per question and binary correct/incorrect outcome. Target: Spearman rho >= 0.25, confirming that questions with higher-quality discriminative retrieval achieve better accuracy outcomes.",
    "A/B accuracy comparison on HLE-gold 25-question subset: run both SPARKIT_ENABLE_GAP_MINING=0 and SPARKIT_ENABLE_GAP_MINING=1 configurations on the identical question set with three repeated trials each to reduce variance. Compute mean accuracy and 95% confidence intervals. Target: gap-mining configuration achieves >= 4 percentage point accuracy improvement with non-overlapping confidence intervals at trial N=3.",
    "MCQ blended-score margin improvement for correct options: on HLE-gold MCQ questions, measure the blended score margin between the correct answer option and the second-best option for each question. Compare gap-mining-enabled vs. disabled runs. Target: mean margin increases from the current baseline of approximately 0.08 to >= 0.12 on HLE hard questions, indicating that discriminative retrieval concentrates supporting evidence on the correct option rather than distributing it across all options.",
    "Adaptive gating efficiency: with discriminative-power-based gating active, measure mean retrieval rounds per question and mean ingested document count per question versus baseline. Target: equal or fewer mean rounds with equal or higher accuracy, confirming that discriminative gating terminates retrieval more intelligently without sacrificing evidence coverage.",
    "Calibration ECE delta: compare Expected Calibration Error before and after enabling gap mining on the HLE-gold benchmark, using the existing calibration.py infrastructure. Higher-quality evidence resulting from discriminative retrieval should reduce overconfidence on hard questions. Target: ECE reduction >= 0.02 absolute points, indicating improved probability calibration alongside accuracy gains."
  ],
  "risks": [
    "Gap analysis LLM call adds one extra provider round-trip after round 1 ingestion, adding approximately 2-5 seconds and $0.01-0.05 per question in cost. On tight latency budgets this may trigger early termination before synthesis. Mitigation: use the fastest available provider (claude-sonnet-4-6 or gemini-3-pro-preview) with a hard 500-token output cap; launch the gap analysis call concurrently with round 2 retrieval startup if the event loop allows; gate entirely on SPARKIT_ENABLE_GAP_MINING=0 for requests with max_latency_s < 60.",
    "Gap analysis prompt can hallucinate gaps for well-evidenced questions, generating spurious retrieval queries that waste budget and return irrelevant documents. This degrades performance on questions that round 1 already answers well. Mitigation: require each gap to state a specific missing entity or measurement, not vague uncertainty phrases; cap at 6 gaps maximum; skip gap query generation if any gap description is fewer than 5 tokens or more than 30 tokens (likely malformed); degrade to standard planned queries on any LLM parse failure.",
    "Discriminative power scoring for MCQ requires hypothesis text tokens at retrieval time. If the question decomposition stage fails to produce structured answer options (e.g., non-MCQ or decomposition timeout), discriminative power falls back to Jaccard novelty, which is a weaker proxy and may not improve accuracy for open-ended hard questions. Mitigation: detect task_type == multiple_choice before enabling full discriminative scoring; log fallback activation in the discriminative_scoring trace stage; measure accuracy impact separately for MCQ vs. non-MCQ questions across evaluations.",
    "Cross-source triangulation requirement (>= 2 distinct source types for high-confidence claims) could increase minimum retrieval budget for questions where one source type dominates the relevant literature, such as highly specialized clinical questions indexed only in Europe PMC. In edge cases no source type combination meets the threshold, forcing a confidence penalty on all high-confidence claims and potentially triggering the answerability gate. Mitigation: make the threshold configurable via SPARKIT_CROSS_SOURCE_MIN_TYPES; implement graceful fallback where if fewer than SPARKIT_CROSS_SOURCE_MIN_TYPES source types are available in the total retrieved set, the triangulation requirement is waived with a logged warning rather than penalizing all claims.",
    "Gap sub-rounds add potentially 1-2 extra retrieval rounds beyond the current adaptive max of 5, conflicting with SPARKIT_ADAPTIVE_MAX_ROUNDS budget controls and potentially breaching max_cost_usd or max_latency_s constraints for cost-sensitive requests. Mitigation: count gap sub-rounds against the existing adaptive max-rounds counter; only fire a gap sub-round if the existing reserve-cost check (already in engine.py) confirms sufficient budget remains for at least one full synthesis pass after retrieval; disable gap sub-rounds if SPARKIT_ENABLE_GAP_MINING=1 but constraints.max_cost_usd < 1.50."
  ]
}
```
