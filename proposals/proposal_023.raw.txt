```json
{
  "proposal_id": "SPARKIT-HVS-001",
  "title": "Hypothesis-Driven Contrastive Evidence Triangulation with Semantic Claim Graph",
  "distinctive_angle": "Reframe hard-question answering as probabilistic hypothesis testing: generate competing hypotheses from initial evidence, run a targeted discrimination retrieval round with contrastive pair queries, build a claim-evidence graph with semantic deduplication and cross-source triangulation confidence, then score each hypothesis by total evidence consistency rather than term frequency before synthesis. This converts SPARKIT from a retrieve-then-synthesize pipeline into a retrieve-hypothesize-discriminate-synthesize loop that explicitly resolves ambiguity rather than averaging over it.",
  "summary": "After initial retrieval rounds, SPARKIT currently assembles a flat evidence pile and synthesizes in one pass. This proposal adds three new stages: (1) Hypothesis Generation—enumerate competing answer candidates (MCQ options or free-form claims) and for each pair generate a discriminating retrieval query of the form 'evidence that H_i is true but H_j is false'; (2) Semantic Evidence Graph Construction—deduplicate claims semantically via embedding cosine similarity rather than DOI alone, build a bipartite claim-to-source graph, and compute cross-source triangulation confidence using a Herfindahl-adjusted source diversity formula; (3) Hypothesis Scoring—score each hypothesis by how well it explains the full evidence graph, incorporating both triangulated support density and contrastive evidence absence, then inject the ranked hypothesis contest into the synthesis prompt as a structured prior.",
  "reasoning": "Hard HLE bio/chem questions fail under the current architecture for four compounding reasons: (a) purely lexical relevance scoring misses semantically equivalent papers that use IUPAC systematic names, gene symbols, or domain jargon with zero surface token overlap with the query; (b) MCQ evidence for similar options heavily overlaps, keeping dossier margins below the 0.06 blended-selection threshold and forcing fallback to less-grounded lexical scoring; (c) the heuristic verifier misclassifies methodological caveats and limitations sections as contradictions via string-match, applying coarse confidence penalties to correct claims; (d) the same fact cited across three papers appears as three independent claims, inflating support_coverage and misleading the abstention gate. Hypothesis-driven discrimination addresses all four: contrastive queries explicitly target papers that tested alternative mechanisms; semantic deduplication fixes inflation by merging claims above cosine threshold 0.85; cross-source triangulation uses source independence rather than raw count as a confidence signal; and hypothesis scoring rewards evidence uniquely consistent with one answer rather than shared across all options.",
  "expected_impact": {
    "accuracy_delta_pct_points": 8,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add hypothesis generator in engine.py immediately after round-1 retrieval completes: call the planning provider with a short prompt to enumerate 2–4 competing hypotheses (directly mapped to MCQ options for multiple_choice task_type, or inferred alternative claims for mechanism/factual). For each ordered pair (H_i, H_j), generate a discriminating query string: 'evidence that {H_i} and not {H_j}'. Store as retrieval_plan.discrimination_queries.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add discrimination retrieval round to the multi-round loop in engine.py: after adaptive gating would normally stop, check if any dossier margin is below 0.06. If so, execute discrimination_queries through the existing aggregator with intent='discriminating', capped at max 2 records per source per pair. Guard with SPARKIT_ENABLE_DISCRIMINATION_ROUND=1 env flag.",
      "owner": "retrieval_service",
      "effort": "low"
    },
    {
      "step": "Integrate text-embedding-3-small (OpenAI API) or all-MiniLM-L6-v2 (local via sentence-transformers) into the retrieval_service aggregator: replace _relevance_score token-overlap with cosine similarity between the question embedding and each paper's title+abstract embedding. Cache embeddings keyed by DOI or URL using a module-level LRU dict to avoid redundant API calls across retrieval rounds. Fall back to lexical scoring if embedding API call fails or if SPARKIT_MAX_EMBEDDING_COST_USD budget is exhausted.",
      "owner": "retrieval_service",
      "effort": "medium"
    },
    {
      "step": "Add query expansion via entity normalization for bio/chem questions: before issuing retrieval queries, detect chemical names (regex for IUPAC patterns, CAS number format) and gene/protein symbols in the question. Look up synonyms from PubChem REST API (/compound/name/{name}/synonyms/JSON) and UniProt ID mapping. Append top-3 synonyms to each sub-query. Cache lookups in a session-scoped dict. Guard with SPARKIT_ENABLE_ENTITY_EXPANSION=1.",
      "owner": "retrieval_service",
      "effort": "medium"
    },
    {
      "step": "Implement semantic claim deduplication in engine.py _build_claim_clusters: after extracting all claims, encode claim texts via the same embedding model, compute pairwise cosine similarity, and merge claims with similarity above 0.85 into a canonical claim with source_count and source_ids fields. Update support_coverage computation to count canonical claims, not raw claims, preventing inflation. Add entity guard: do not merge claims containing distinct chemical identifiers (CAS, InChI).",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Compute cross-source triangulation confidence for each canonical claim: triangulation_score = log(1 + source_count) * (1 - HHI) where HHI is the Herfindahl-Hirschman Index over source_type (arXiv, Semantic Scholar, Europe PMC, etc.). Claims from 3 independent source types score ~1.1x base; single-source claims score 0.0 triangulation bonus. Add source_authority_weight (arXiv=0.8, SemanticScholar=1.0, EuropePMC=1.2, Crossref=1.0) as a multiplier.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Replace heuristic verifier.py contradiction detection with LLM stance classifier: batch all canonical claims against each hypothesis in a single prompt call to the cheapest available provider (DeepSeek-reasoner at $0.28/M tokens, timeout 15s). Output structured JSON per claim: {claim_id, hypothesis_id, stance: supports|contradicts|neutral, confidence: 0.0–1.0}. Chunk into batches of 20 claims if total exceeds 20. Fall back to heuristic string-match markers on timeout or parse failure.",
      "owner": "orchestrator",
      "effort": "medium"
    },
    {
      "step": "Add hypothesis scorer in engine.py: for each hypothesis H, score = sum(triangulation_score * stance_confidence for supporting canonical claims) - 0.5 * sum(triangulation_score * stance_confidence for contradicting canonical claims). Rank hypotheses by score. Store ranked list as hypothesis_contest in run metadata.",
      "owner": "orchestrator",
      "effort": "high"
    },
    {
      "step": "Inject hypothesis contest into synthesis prompt: add a HYPOTHESIS CONTEST section above EVIDENCE in the synthesis prompt template, showing ranked hypotheses with their scores, triangulated support counts, and top 2 discriminating evidence snippets. For MCQ questions, this maps directly to option ranking. For free-form, it surfaces the most evidence-consistent claim as the synthesis anchor.",
      "owner": "orchestrator",
      "effort": "low"
    },
    {
      "step": "Extend test_synthesis_quality.py with tests for: (a) semantic claim deduplication—merged canonical count <= raw claim count on fixture with 3 near-duplicate claims; (b) triangulation score ordering—3-source claim scores higher than 1-source claim; (c) hypothesis scorer—on a fixture where claims strongly support H_A and contradict H_B, scorer ranks H_A above H_B; (d) stance classifier output schema validation against expected JSON structure; (e) entity non-merge guard—claims containing distinct CAS numbers are not merged even if cosine similarity is 0.87.",
      "owner": "eval",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Replace token-overlap _relevance_score with cosine similarity between text-embedding-3-small (or local all-MiniLM-L6-v2) embeddings of the question and each retrieved paper's title+abstract. This catches papers using IUPAC systematic names, gene symbols, or domain jargon that share zero surface tokens with the query—the primary failure mode on HLE bio/chem. Embeddings are cached by DOI/URL across retrieval rounds so incremental cost is one API call per new document, not per round.",
    "Add a dedicated discrimination retrieval round that issues contrastive pair queries: for each pair of competing hypotheses (H_i, H_j), query string = 'evidence distinguishing {H_i} from {H_j}'. This explicitly surfaces papers that experimentally tested alternative mechanisms or compared competing compounds—exactly the evidence that separates correct from incorrect MCQ options—without relying on surface similarity to any single option text. Only triggered when initial dossier margin is below 0.06, avoiding cost on easy questions.",
    "Implement query expansion via chemical and biological entity normalization: for questions containing compound names or gene symbols, query PubChem REST API and UniProt ID mapping to retrieve top-3 synonyms and CAS numbers, then append them to retrieval sub-queries before federation. This directly addresses the HLE bio/chem failure mode where systematic IUPAC names have zero overlap with paper titles that use trade names or abbreviations.",
    "Add novelty-weighted re-ranking after retrieval aggregation: before record selection for ingestion, compute pairwise cosine distances among all candidate documents and demote any document whose embedding is within cosine distance 0.1 of an already-selected document by multiplying its relevance score by 0.5. This reduces the dominance of highly-cited review papers and increases effective source diversity beyond the current per-source cap, improving triangulation confidence for the evidence graph.",
    "Introduce intent-conditioned query suffixes based on the task_type classification from the decompose step: for mechanism questions append 'molecular mechanism pathway reaction intermediate'; for numerical questions append 'quantitative measurement experimental value reported'; for comparative questions generate one query per comparison axis (e.g., 'compound A vs compound B selectivity'). This leverages the existing task-type infrastructure in engine.py to steer retrieval vocabulary toward the specific evidence type each question demands."
  ],
  "evaluation_plan": [
    "Measure claim deduplication compression ratio on HLE-gold 149 questions: compute raw_claim_count / canonical_claim_count per run. Current system is expected to show ratio 1.6–2.2 due to the same fact cited across review papers. After implementing semantic deduplication, verify ratio drops below 1.2 on average while support_coverage is unchanged or improves, confirming inflation is removed without losing genuine evidence breadth.",
    "Run discrimination retrieval ablation on HLE-gold bio/chem: compare SPARKIT accuracy with SPARKIT_ENABLE_DISCRIMINATION_ROUND=0 vs =1 across all 149 questions. Primary metric: accuracy on the subset where initial dossier margin was below 0.06 (currently triggering lexical fallback). Expected: +5–12% accuracy improvement on that hard subset with discrimination round enabled.",
    "Verify semantic reranking recall@10 vs lexical recall@10: for HLE questions where a gold-answer paper DOI is known, check whether that paper appears in the top-10 retrieval candidates under both scoring methods. Target: embedding-based recall@10 exceeds lexical recall@10 by at least 8 percentage points on chemistry-specific questions where IUPAC names are present in the question stem.",
    "Stance classifier accuracy audit: manually annotate 50 claim-hypothesis pairs sampled from five distinct HLE runs as supports/contradicts/neutral. Measure precision, recall, and macro-F1 of the LLM classifier against the annotations. Accept classifier if macro-F1 >= 0.75; revert to heuristic string-match verifier and flag for re-tuning if F1 < 0.65.",
    "Run end-to-end cost regression with all features enabled: measure per-question total cost (synthesis tokens + Brave requests + embedding API calls + stance classifier call) across 20 HLE questions. Verify total cost stays below 2.0x current standard-mode cost. If exceeded, reduce discrimination round to max_pairs=2 and max_results_per_pair=2, and switch entity expansion to local RDKit synonym lookup rather than PubChem API.",
    "Calibration ECE delta test: after implementing hypothesis scoring as a synthesis prior, compute Expected Calibration Error before and after on HLE-gold using the existing calibration.py infrastructure. Hypothesis-aware synthesis should improve confidence-accuracy alignment, particularly for questions where the correct answer has high triangulation score but where the current system assigns low confidence due to heuristic contradiction penalties."
  ],
  "risks": [
    "Embedding API latency adds 0.5–2s per document batch per round. With 10–14 ingested docs per question across 3–5 rounds, this could push retrieval latency above current provider timeouts. Mitigation: batch all documents across all rounds into a single API call at the end of aggregation, cache results in a session-scoped dict keyed by DOI/URL, and implement graceful fallback to lexical scoring when SPARKIT_MAX_EMBEDDING_COST_USD is exceeded.",
    "Contrastive pair queries for MCQ with 4–5 options generate O(n^2) = 6–10 additional retrieval queries. At 0.5s average retrieval latency, this adds 3–5s per question. Mitigation: restrict discrimination round to top-2 plausible options only (filtered by initial dossier score), and only trigger the round when dossier margin is below 0.06 via the existing adaptive gating infrastructure.",
    "LLM stance classifier adds one provider API call per question. DeepSeek-reasoner at $0.28/M tokens is cheap but carries a 45s timeout risk under load. Mitigation: set stance classifier call timeout to 15s, use a single-shot batch prompt for all claims, fall back to heuristic marker scoring on timeout or malformed JSON, and log classifier_fallback_used in run telemetry.",
    "Semantic deduplication at cosine threshold 0.85 may incorrectly merge distinct claims about structurally similar compounds (e.g., two enantiomers, two drug analogs) in fine-grained chemistry questions. Mitigation: add a chemical entity non-merge guard that skips merging any two claims containing different CAS numbers, InChI strings, or SMILES substrings, even if embedding similarity exceeds threshold.",
    "Cross-source triangulation confidence under-rewards highly specific single-source claims from authoritative studies (e.g., a landmark RCT or definitive crystallography result). A seminal paper with source_count=1 would receive zero triangulation bonus despite high authority. Mitigation: add source_authority_weight per domain (Europe PMC clinical=1.5, arXiv preprint=0.8) and use weighted_source_count = sum(authority_weight_i) instead of raw source_count in the triangulation formula.",
    "Hypothesis scorer LLM batch call may exceed context limits for cheap providers when questions generate 40+ canonical claims. At ~50 tokens per claim-hypothesis pair and 40 claims against 4 hypotheses, a single prompt reaches ~8000 tokens—within range but risky for claim-dense runs. Mitigation: chunk claim-hypothesis pairs into batches of 20 pairs maximum, run sequentially, and take the max-confidence stance per (claim_id, hypothesis_id) pair across chunks before scoring."
  ]
}
```
