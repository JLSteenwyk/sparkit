Now I have a thorough understanding of the full SPARKIT architecture. Let me synthesize the proposal.

```json
{
  "proposal_id": "SPARKIT-SERCG-001",
  "title": "Semantic Retrieval Reranking with Contrastive Option-Pair Gap Filling",
  "distinctive_angle": "Every relevance signal in SPARKIT today is purely lexical (token overlap). The proposal replaces the ingestion-gate scoring with a bi-encoder semantic reranker, then adds a novel 'contrastive retrieval' round that generates queries specifically designed to find evidence discriminating between *pairs* of similar MCQ options—the exact failure mode of lexical scoring—while simultaneously replacing heuristic contradiction detection with embedding-based stance classification.",
  "summary": "SPARKIT's entire relevance stack from aggregator._relevance_score through engine._record_relevance_score and engine._select_best_section_chunk relies on token overlap. For hard STEM questions with domain-specific vocabulary, this systematically under-ranks semantically relevant documents that use synonyms, IUPAC names, acronyms, or paraphrase-heavy abstracts. This proposal (1) inserts a bi-encoder reranking step at the ingestion gate to replace token-overlap scoring, (2) adds a post-Round-1 'contrastive pair' retrieval round that asks the LLM to generate queries for each differing option-pair combination rather than each option independently, (3) replaces verifier.py's heuristic contradiction-marker detection with cosine-similarity-based stance classification between adversarial-round claims and support-round claims, and (4) promotes discriminative evidence (high relevance to exactly one option) to a higher weight in MCQ dossier construction.",
  "reasoning": "The core problem on HLE-class hard questions is not retrieval volume but retrieval precision: SPARKIT retrieves 14-18 records per round, but _select_records_for_ingestion ranks them by title/abstract word overlap against the question text. For a question asking about (1S,2R)-2-((tert-butyldimethylsilyl)oxy)cyclopent-1-en-1-yl chemistry, the correct paper may use systematic IUPAC terms with zero shared tokens. A bi-encoder (e.g., nomic-embed-text-v1.5 or text-embedding-3-small) encodes semantic intent and recovers these misses. The contrastive pair queries address the second failure mode: when options A and B are both plausible, queries like '{stem} {option_A}' and '{stem} {option_B}' retrieve largely overlapping evidence. Generating a query like 'does [mechanism X] favor [compound A] over [compound B] and why' forces retrieval of discriminating evidence. Stance classification with embeddings catches paraphrased contradictions that string-matching misses (e.g., 'no measurable effect' is not caught by 'contradict' but is cosine-close to 'null result' embeddings). Together these three changes attack the three biggest evidence-quality gaps without changing SPARKIT's orchestration contract.",
  "expected_impact": {
    "accuracy_delta_pct_points": 6,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add a lightweight embedding model client to services/orchestrator/app/providers.py (or a new embeddings.py module). Support OpenAI text-embedding-3-small as the default (already API-key-gated) with a local fallback via sentence-transformers nomic-embed-text-v1.5 when SPARKIT_EMBEDDING_PROVIDER=local. Cache embeddings of retrieved abstracts in-process (dict keyed by record identity) within a single run to avoid redundant API calls.",
      "owner": "platform-engineer",
      "effort": "medium"
    },
    {
      "step": "Replace _record_relevance_score in engine.py with a two-stage scorer: (1) keep existing token-overlap score as a cheap first-pass pre-filter that retains the top 2*target_docs candidates, (2) re-rank those candidates with cosine similarity between the embedding of '{question} {focus_terms}' and the embedding of '{title} {abstract[:400]}'. Guard with SPARKIT_ENABLE_SEMANTIC_RERANK env bool (default true) and fall back to lexical-only if embedding call fails.",
      "owner": "platform-engineer",
      "effort": "medium"
    },
    {
      "step": "Replace _select_best_section_chunk scoring in engine.py (currently 1.4*question_overlap + 1.9*focus_term_overlap + length_factor) with cosine similarity between the query embedding and the chunk embedding. Pre-compute chunk embeddings lazily on the chunks list. This ensures the ingested section is semantically closest to the question, not just keyword-dense.",
      "owner": "platform-engineer",
      "effort": "low"
    },
    {
      "step": "Add a new retrieval round 'retrieval_round_contrastive_pairs' after retrieval_round_option_hypotheses (MCQ only). For each pair of KEPT options after the elimination step, call the planning provider with a short prompt: 'Generate 2 queries to find evidence specifically distinguishing [{choice_A}] from [{choice_B}] in the context of: {stem}'. Collect up to max(4, len(kept_options)*(len(kept_options)-1)) contrastive queries, deduped via _dedupe_queries. Run these through search_literature with a reduced per_source budget. Wire into _build_round_queries_from_plan by returning this round as the second round for modes SINGLE and ROUTED when answer_choices is non-empty. Guard with SPARKIT_ENABLE_CONTRASTIVE_ROUND (default true).",
      "owner": "ml-engineer",
      "effort": "medium"
    },
    {
      "step": "Modify _build_option_dossiers in engine.py to accept an optional discriminative_boost: float parameter. After building per-option scored_support lists, compute a 'uniqueness score' for each snippet: score_for_target_option / (sum_of_scores_across_all_options + 1e-9). Multiply support_score by (1.0 + discriminative_boost * uniqueness) before appending to scored_support. Set discriminative_boost=2.0 as default, making evidence unique to one option triple its effective weight in dossier selection. This directly improves _select_option_from_dossiers margins on hard MCQ.",
      "owner": "ml-engineer",
      "effort": "low"
    },
    {
      "step": "Replace heuristic contradiction marker scanning in verifier.py with embedding-based stance classification. After retrieving adversarial-round records, embed each adversarial abstract. Embed each support-round claim_text. Compute cosine similarity between each (adversarial_abstract, support_claim) pair. If similarity > SPARKIT_STANCE_SIM_THRESHOLD (default 0.72), treat the adversarial document as semantically related to that claim. Then pass the pair to a short LLM stance prompt ('Does passage P support, contradict, or is neutral to claim C? Answer one word.') to classify stance. Only apply contradiction penalty when stance='contradict'. This eliminates false positives from keyword matches on unrelated papers.",
      "owner": "ml-engineer",
      "effort": "high"
    },
    {
      "step": "Add _semantic_claim_dedup in engine.py: before building claim clusters (_build_claim_clusters), embed all claim_texts. Cluster with a simple greedy algorithm—add a claim only if its cosine similarity to all already-accepted claims is below SPARKIT_CLAIM_DEDUP_THRESHOLD (default 0.88). This prevents the same fact reported across 3 papers from inflating unsupported_claims and support_coverage metrics. Guard with SPARKIT_ENABLE_CLAIM_DEDUP (default true).",
      "owner": "ml-engineer",
      "effort": "medium"
    },
    {
      "step": "Add new unit tests to test_synthesis_quality.py: (a) test that _semantic_claim_dedup removes near-duplicate claims while keeping semantically distinct ones, (b) test that contrastive queries are generated for kept option pairs and absent for single-option cases, (c) test that discriminative_boost increases dossier margin when one option has uniquely relevant evidence, (d) test that the embedding reranker is invoked during _select_records_for_ingestion when SPARKIT_ENABLE_SEMANTIC_RERANK=1. Add integration benchmark run comparing HLE-25 balanced subset accuracy before/after the changes.",
      "owner": "ml-engineer",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Bi-encoder semantic reranking at the ingestion gate: replace token-overlap _record_relevance_score with cosine similarity between the query+focus_terms embedding and title+abstract embeddings, recovering semantically relevant papers that share no surface tokens with the question (critical for IUPAC nomenclature, synonym-heavy domains, and paraphrased abstracts on HLE-class chemistry and biology questions).",
    "Contrastive option-pair retrieval round for MCQ: after elimination, generate LLM-crafted queries that specifically ask 'why X rather than Y' for each pair of remaining options, forcing retrieval of discriminating evidence rather than general per-option evidence—directly attacking the failure mode where all options retrieve the same papers because their surface text is topically similar.",
    "Section-chunk embedding reranking: replace the lexical _select_best_section_chunk scorer (1.4*question_overlap + 1.9*focus_term_overlap) with cosine similarity between the question embedding and each parsed section chunk, ensuring the 1200-char ingested window is semantically closest to the question rather than highest in focus-term density.",
    "Semantic claim deduplication before cluster construction: embed all extracted claim_texts and apply greedy cosine-similarity deduplication at threshold 0.88, preventing the same factual assertion cited across multiple papers from being counted as independent evidence and artificially inflating support_coverage.",
    "Embedding-gated adversarial contradiction detection: filter adversarial-round records by semantic similarity to support claims before applying any stance classification, eliminating false contradiction flags from topically adjacent but claim-unrelated papers—producing a cleaner contradiction_flags signal that more reliably triggers the abstention and confidence penalty logic."
  ],
  "evaluation_plan": [
    "Run the HLE-25 balanced benchmark (bio13 + chem12) with semantic reranking enabled vs. the baseline. Report top-1 accuracy delta, mean rank of the correct evidence document pre- and post-reranking (requires gold-labeled relevant docs), and fraction of questions where the ingested top-1 document changed after the semantic reranker replaced the lexical scorer.",
    "Measure retrieval P@5 on HLE-25: for each question, manually or programmatically label whether the top-5 ingested documents contain a citation that grounds the correct answer. Compare P@5 before and after the contrastive pair round and the semantic reranker. Track separately for bio vs. chem subsets since domain-specific synonym coverage differs.",
    "Evaluate claim deduplication impact: count mean unique claims per run before/after _semantic_claim_dedup at threshold 0.88. Check that support_coverage values do not systematically collapse (which would cause false abstentions) and that calibration ECE on the HLE-25 subset improves (confidence should track accuracy more closely).",
    "A/B test contradiction detection precision: for 20 HLE questions where verifier.py currently fires contradiction_flags >= 2, manually verify whether each flagged adversarial paper genuinely contradicts a support claim. Compare precision before (heuristic markers) and after (embedding-gated + LLM stance classification). Report false positive rate reduction.",
    "Measure discriminative_boost impact on dossier margin: log the margin (top_score - second_score from _select_option_from_dossiers) per MCQ question before/after enabling discriminative_boost=2.0. Track the fraction of questions where _select_option_from_dossiers succeeds (returns non-None) vs. falls back to blended scoring—larger fraction succeeding with dossier method is a positive signal since it is the more evidence-grounded path.",
    "Latency and cost regression test: run 5 HLE questions through each new stage and record per-stage duration and embedding token cost. Verify total per-question cost increase is bounded by SPARKIT_MAX_EMBEDDING_COST_USD (new env var, default 0.05) and that the embedding step does not exceed 8s per round before the budget guard kicks in."
  ],
  "risks": [
    "Embedding API latency adds to per-round wall time. text-embedding-3-small is fast (~100ms per batch) but if the embedding call is serial per-record rather than batched, it could add 1-3s per retrieval round. Mitigation: batch all candidates from a round into a single embedding API call; implement the local sentence-transformers fallback for zero-latency operation in offline runs.",
    "Contrastive pair round LLM call may generate queries that are too narrow or chemically nonsensical for multi-step synthesis questions where options are structurally similar (e.g., two regioisomers of the same compound). Mitigation: cap contrastive queries at 4 pairs maximum, dedup against existing round queries via _dedupe_queries, and gate the round behind SPARKIT_ENABLE_CONTRASTIVE_ROUND=1 (default) with easy disabling.",
    "Semantic claim deduplication at threshold 0.88 may over-dedup in subdomains where multiple papers independently confirming the same result is itself meaningful (convergent evidence increases confidence). Mitigation: track dedup_count per claim and expose it in calibration features; reduce threshold to 0.92 if support_coverage drops more than 15% relative on HLE-25.",
    "Embedding-gated stance classification in verifier.py adds an LLM call per (adversarial_doc, support_claim) pair. With depth=2 and 6 adversarial docs × 8 claims = 48 pair checks, this could exceed budget. Mitigation: limit to top-4 adversarial docs by embedding similarity to any claim (gate out unrelated docs before LLM call), keeping the maximum stance-check LLM calls to 4*4=16 per run.",
    "The local sentence-transformers fallback requires a model download (~90MB for nomic-embed-text-v1.5) at first use. This is acceptable for server deployments but may surprise developers running tests. Mitigation: gate local embedding on SPARKIT_EMBEDDING_PROVIDER=local explicitly; default to OpenAI embedding and make the local option opt-in."
  ]
}
```
