Now I have full picture of the codebase. Generating the proposal.

```json
{
  "proposal_id": "SPARKIT-PROP-2026-EDC-001",
  "title": "Evidence-Deficit-Closure Retrieval: Sub-Claim Gap Targeting, Citation Graph Snowballing, and LLM Cross-Encoder Passage Reranking",
  "distinctive_angle": "Treats each retrieval round as a knowledge-gap-closing operation rather than independent keyword queries. After every round, a sub-claim gap analyzer maps each ResearchPlan.sub_claim against the accumulated abstract pool to identify which claims are still evidentially unresolved, then issues targeted gap-filling queries for only those unresolved claims. Simultaneously, the top-scoring retrieved papers seed a citation-graph snowball via Semantic Scholar's /graph/v1/paper/{id}/references and /citations APIs, reaching seminal works that keyword search misses due to terminology mismatch. Before synthesis, a batched LLM cross-encoder reranking pass replaces the token-overlap _select_best_section_chunk heuristic with claim-to-passage semantic relevance scores, ensuring only high-quality excerpts enter the synthesis prompt.",
  "summary": "Three coordinated, minimally-invasive improvements targeting the three dominant failure modes on hard HLE-Gold bio/chem questions: (1) retrieval rounds that repeat similar keyword clusters without closing specific evidential gaps in the ResearchPlan.sub_claims; (2) highly-cited authoritative papers that keyword search misses because they use domain-specific nomenclature absent from the question surface form; (3) coarse passage selection via token-overlap _record_relevance_score that feeds synthesis irrelevant text even after ingesting the correct papers. Each improvement slots into an existing engine.py extension point with minimal architectural change. The gap analyzer hooks into _build_round_queries_from_plan() as a new post-round intent. Citation snowballing adds a search_semantic_scholar_citations() adapter in retrieval_service/adapters.py. LLM reranking replaces _select_best_section_chunk() with a batched provider call that scores passage-subclaim pairs before building the synthesis prompt.",
  "reasoning": "On HLE-Gold PhD-level chemistry and biology questions, SPARKIT's primary accuracy ceiling is set by evidence quality, not synthesis capability. The current _build_round_queries_from_plan() issues statically-named rounds (retrieval_round_1, _gap_fill, _adversarial) with fixed query suffix patterns like 'review', 'limitations', 'benchmark comparison'. These are question-agnostic and produce overlapping result sets when the knowledge gap is actually a specific missing numeric measurement or mechanism. The ResearchPlan already contains sub_claims from LLM decomposition but these are never used to drive retrieval targeting - they exist only for calibration scoring. Connecting sub-claims to retrieval targeting closes the gap. Citation graph snowballing is motivated by the Semantic Scholar adapter already being present in adapters.py; it already fetches paper metadata but never traverses the reference graph. The Semantic Scholar Graph API provides /paper/{paperId}/references with full paper metadata at no extra monetary cost (free tier). For hard technical questions, the key paper is often cited by multiple retrieved papers but is not itself at the top of keyword search because its title uses systematic nomenclature rather than question keywords. LLM passage reranking addresses the chunk selection bottleneck: _select_best_section_chunk currently uses the same token-overlap formula as document-level scoring, meaning a passage that precisely answers a sub-claim but uses different vocabulary scores lower than a tangentially related passage that shares many surface tokens with the question. A single batched LLM reranking call using the planning provider (already instantiated per-run) over the top-20 candidate passages adds one extra provider call per synthesis but dramatically improves what enters the 10,000-char synthesis context window.",
  "expected_impact": {
    "accuracy_delta_pct_points": 10,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add _analyze_evidence_gaps(question, sub_claims, records) in engine.py. Takes the ResearchPlan.sub_claims list and the accumulated LiteratureRecord pool after each retrieval round. For each sub-claim, checks token overlap against all record abstracts. Sub-claims with zero or near-zero max overlap (< 2 token matches) are flagged as 'unresolved'. Returns a list of (sub_claim_text, urgency_score) pairs. Urgency is 1.0 for zero coverage, 0.5 for weak coverage. Called at the top of the adaptive loop in execute_orchestration() before deciding whether to issue another round.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Extend _build_round_queries_from_plan() in engine.py with a new 'gap_targeted' intent. When _analyze_evidence_gaps() returns unresolved sub-claims, generate one targeted query per unresolved claim by prepending its key noun phrases to the original focus_terms. Cap at 6 gap-targeted queries per round to avoid API rate limits. These queries are injected into the next retrieval round as a 'retrieval_gap_targeted' stage name, replacing the static 'retrieval_round_2_gap_fill' when sub-claim data is available. Add env flag SPARKIT_ENABLE_GAP_TARGETING (default 1) to gate the feature.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add search_semantic_scholar_citations(paper_id, limit) in retrieval_service/adapters.py. Calls the Semantic Scholar Graph API endpoint /graph/v1/paper/{paperId}/references?fields=title,abstract,authors,year,externalIds,url with httpx using the existing _get_with_retry() pattern. Returns a list of LiteratureRecord objects. Add a parallel search_semantic_scholar_references() variant for the /citations endpoint. Both functions honor the existing SEMANTIC_SCHOLAR_API_KEY env var if set.",
      "owner": "retrieval_service/adapters.py",
      "effort": "medium"
    },
    {
      "step": "Add _citation_snowball(records, question, top_k_seeds, max_hops) in engine.py. Takes the current accumulated records, scores them by _record_relevance_score(), takes the top top_k_seeds (default 3) papers that have a Semantic Scholar paper ID (extractable from their URL or DOI via the S2 lookup). Calls search_semantic_scholar_citations() and search_semantic_scholar_references() for each seed. Deduplicates against the existing record pool. Called once after retrieval_round_1 when adaptive retrieval is enabled. Controlled by env flag SPARKIT_ENABLE_CITATION_SNOWBALL (default 1). Seeds are limited to papers with relevance_score > 3.0 to avoid snowballing low-quality documents.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _llm_rerank_passages(question, sub_claims, candidate_passages, provider_client) in engine.py. Constructs a batched prompt listing up to 20 (passage_index, title, excerpt) tuples and asks the provider to score each on a 0-10 integer scale for relevance to each unresolved sub-claim. Parses scores from structured output lines 'PASSAGE_N: SCORE'. Returns top-K passages sorted by max score across sub-claims, filtered to score >= 6. Replaces the token-overlap selection in _select_best_section_chunk() when sub-claims are available. Uses the planning provider from the run's ProviderPlan to avoid adding a new provider dependency. Add env flag SPARKIT_ENABLE_PASSAGE_RERANKING (default 1) with a SPARKIT_RERANKING_MIN_CANDIDATES threshold (default 8) below which reranking is skipped.",
      "owner": "orchestrator/engine.py",
      "effort": "high"
    },
    {
      "step": "Add answer-conditioned falsification retrieval in execute_orchestration(). After the first synthesis draft is produced (before run_verifier()), extract the top 2 concrete claims from the draft answer using a lightweight regex for quoted measurements, named entities, and modal assertions ('X causes Y', 'X is the primary Z'). Issue 2-3 falsification queries of the form 'evidence against [claim]' and 'replication failure [claim keyword]' through the existing search_literature() path. Feed results directly to run_verifier() as additional adversarial_records. This makes verification answer-conditioned rather than purely query-plan-conditioned.",
      "owner": "orchestrator/engine.py + verifier.py",
      "effort": "medium"
    },
    {
      "step": "Extend test_synthesis_quality.py with unit tests for _analyze_evidence_gaps() covering: (a) empty sub-claims returns no gaps, (b) sub-claim with zero abstract matches is flagged with urgency 1.0, (c) sub-claim with 3+ token matches is not flagged. Add integration test for _citation_snowball() using mocked search_semantic_scholar_citations() that returns 5 records. Add unit test for _llm_rerank_passages() using mocked provider returning well-formed score lines, verifying that passages scoring < 6 are excluded.",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    }
  ],
  "retrieval_improvements": [
    "Sub-claim gap targeting: After each retrieval round, _analyze_evidence_gaps() identifies ResearchPlan.sub_claims with zero or near-zero abstract coverage in the accumulated record pool and generates targeted queries for only those unresolved claims. This replaces the static suffix-based 'gap_fill' round (which appends fixed strings like 'limitations' and 'benchmark comparison' regardless of what is actually missing) with dynamic, claim-driven query construction. For a chemistry question asking about a specific reaction mechanism, this ensures SPARKIT issues queries for the specific intermediate species rather than generic 'reaction review' queries.",
    "Citation graph snowballing via Semantic Scholar Graph API: The top-3 highest-relevance papers from the primary retrieval round seed a one-hop traversal of their reference and citation graphs using the Semantic Scholar /graph/v1/paper/{id}/references and /citations endpoints (free, no extra cost beyond existing S2 budget). This multi-hop expansion reliably surfaces seminal foundational papers and recent follow-on work that keyword search misses due to nomenclature mismatch - a critical gap for HLE-Gold questions that use systematic IUPAC chemical names or gene symbols that appear rarely in paper titles.",
    "LLM cross-encoder passage reranking before synthesis: _llm_rerank_passages() replaces the token-overlap _select_best_section_chunk() heuristic with a single batched LLM call that scores each candidate passage (up to 20) on a 0-10 integer scale of relevance to each open sub-claim. Passages scoring below 6 are excluded from the synthesis context window. This directly addresses the case where SPARKIT ingests the correct paper but the synthesis prompt receives the abstract section (which overlaps with question keywords) rather than the Results or Supplementary Methods section (which contains the specific quantitative answer).",
    "Answer-conditioned falsification queries: After the first synthesis draft, concrete claims are extracted via regex and used to issue targeted falsification retrieval queries ('evidence against [specific claim]', 'replication failure [claim entity]'). These results augment the adversarial_records fed to run_verifier(), making contradiction detection specific to what SPARKIT actually claims rather than running the same generic adversarial queries regardless of the draft answer. This catches cases where the draft answer confidently states an outdated consensus position that has been experimentally refuted."
  ],
  "evaluation_plan": [
    "Per-sub-claim resolution rate on HLE-Gold-25: After each SPARKIT run, compute what fraction of ResearchPlan.sub_claims have at least one retrieved passage with token overlap >= 3 against the sub-claim text. Track this metric across runs with and without gap targeting enabled. A correctly functioning gap-targeting implementation should increase mean sub-claim resolution rate from the current baseline by at least 15 percentage points on HLE-Gold bio/chem questions.",
    "Citation snowball discovery rate: For each HLE-Gold question where SPARKIT retrieves at least one Semantic Scholar paper, count how many of the snowballed citation/reference papers are novel (not already in the retrieval pool). Log this as 'snowball_novel_docs' in the existing run_observability_metrics table. Threshold: snowball should add >= 2 novel documents per run on at least 60% of questions where it is triggered. If novel doc rate is < 2 consistently, seed relevance threshold (currently 3.0) should be lowered.",
    "Passage reranking score distribution audit: After each run with LLM reranking enabled, log the full distribution of per-passage scores (0-10) to the trace artifacts. The mean score of passages that enter the synthesis prompt should be >= 7.0. If mean is < 6.5, the reranking prompt needs revision. Also check that the reranking call does not silently fail (score parsing errors): the fallback to token-overlap should fire in < 5% of runs, tracked as 'reranking_fallback_rate' in observability.",
    "Accuracy delta on HLE-Gold-25 controlled comparison: Run the full HLE-Gold-25 benchmark (25 bio/chem questions) with gap targeting, citation snowball, and LLM reranking all disabled (SPARKIT_ENABLE_GAP_TARGETING=0, SPARKIT_ENABLE_CITATION_SNOWBALL=0, SPARKIT_ENABLE_PASSAGE_RERANKING=0) as the control. Run again with all three enabled. Use the existing rubric evaluator in eval_service/app/evaluator.py to score both runs. The hypothesis is >= 8 percentage points accuracy improvement. Report per-question win/loss breakdown to identify failure modes.",
    "Cost and latency regression check: After enabling all three improvements, the per-run cost (tracked in run_observability_metrics via estimate_generation_cost() and estimate_brave_search_cost()) should not exceed 2x the baseline cost for the same question on the same mode. LLM reranking adds one extra provider call; citation snowball adds up to 6 Semantic Scholar API calls (free). If cost exceeds 2x, the passage candidate cap (currently 20) and seed count (currently 3) should be reduced. P90 latency should remain under 3 minutes for ROUTED mode.",
    "Falsification retrieval contradiction rate: After enabling answer-conditioned falsification queries, track what fraction of runs have their draft answer's top claim contradicted by at least one falsification retrieval result (i.e., run_verifier() flags >= 1 new contradiction from the falsification records that was NOT flagged by the generic adversarial records). On hard questions, this rate should be > 20% - if it is near zero, the claim extraction regex is too conservative and needs expansion to cover more linguistic patterns."
  ],
  "risks": [
    "Semantic Scholar rate limiting: The citation snowball makes up to 6 additional API calls per run (3 seeds × 2 directions). Semantic Scholar's unauthenticated tier allows 100 requests/5min. Under concurrent benchmark runs (e.g., 25-question HLE-Gold all at once), this could trigger 429 errors. Mitigation: use the existing _get_with_retry() pattern with 2 retries and exponential backoff already in adapters.py; add SPARKIT_CITATION_SNOWBALL_MAX_SEEDS env var (default 2, not 3) for burst scenarios; fall back gracefully when S2 is unavailable without failing the run.",
    "LLM reranking prompt injection via paper excerpts: Malicious or adversarially-crafted paper abstracts could contain text that confuses the reranking prompt (e.g., 'PASSAGE_1: 10\\nPASSAGE_2: 10' embedded in an abstract). Mitigation: sanitize candidate passage text by stripping lines matching the score pattern regex before inserting into the reranking prompt; validate that parsed scores are integers in 0-10 range and that the count of parsed scores matches the count of candidates; fall back to token-overlap on validation failure.",
    "Gap targeting generating low-quality queries for very technical sub-claims: When a sub-claim contains specialized nomenclature (e.g., 'rate constant for S_N2 displacement of tosylate by azide in DMSO at 25°C'), the gap-targeted query may be too specific for any database API to match. This could waste a retrieval round returning zero results. Mitigation: cap gap-targeted queries at the first 8 whitespace-separated tokens of the sub-claim; if the gap-targeted round returns 0 results, log 'gap_targeting_zero_hit' and fall back to the existing heuristic gap-fill queries; do not count zero-hit rounds against the SPARKIT_ADAPTIVE_MAX_ROUNDS budget.",
    "LLM reranking latency adding unacceptable overhead for time-constrained runs: The reranking call over 20 passages adds one serial provider call (estimated 5-15s for the planning provider). For questions with max_latency_s constraints (stored in the BudgetState), this may cause early stop triggering. Mitigation: skip LLM reranking when remaining_budget_s < 30 (check via should_stop_early() before calling _llm_rerank_passages()); also skip when the candidate passage count is below SPARKIT_RERANKING_MIN_CANDIDATES (default 8) since the benefit is minimal.",
    "Citation snowball introducing benchmark-contaminated papers: The Semantic Scholar reference graph may surface papers that are themselves part of or derived from HLE-Gold sources. The existing _DISALLOWED_EVIDENCE_DOMAINS filter blocks huggingface.co and futurehouse.org but may not catch all HLE-associated domains. Mitigation: apply the existing _is_disallowed_record() check to all snowballed records before adding them to the pool; also filter snowballed records where the abstract contains 'humanity's last exam' or 'HLE' to avoid training signal leakage."
  ]
}
```
