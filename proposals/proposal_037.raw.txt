Now I have everything I need. Let me produce the strict JSON proposal.

```json
{
  "proposal_id": "SCAR-PR-001",
  "title": "Speculative Claim Anchoring with Proof-or-Refute Retrieval",
  "distinctive_angle": "Flip SPARKIT's retrieval direction from question-driven to claim-driven: after round-1 retrieval, prompt the synthesis provider to emit a speculative pre-answer as 3-5 explicit numbered falsifiable claims, then drive a dedicated retrieval sub-round ('claim_probes') where each claim generates both a support-probe query and a refutation-probe query. Retrieved evidence is organized per-claim and passed to a new claim-adjudication synthesis prompt that explicitly marks each claim as CONFIRMED, REVISED, CONTRADICTED, or UNCERTAIN — replacing the current free-form evidence-bullet synthesis.",
  "summary": "SPARKIT retrieves evidence based on the original question text. For HLE-class questions, the evidence that actually answers the question often uses domain-specific terminology tied to the answer claim rather than the question stem — meaning the critical papers are routinely missed in rounds 1-3. SCAR-PR inserts a new pipeline phase between retrieval_round_1 and retrieval_round_2_gap_fill. It calls the synthesis provider with a reduced 280-token budget to generate a speculative pre-answer with labeled claims (e.g., 'CLAIM_1: The IC50 of compound X against enzyme Y is approximately 5 nM under conditions Z'). Each claim drives two new retrieval queries: a support probe ('evidence that [claim_text]') and a refutation probe ('evidence contradicting [claim_text]'). These queries are dispatched through the existing search_literature() aggregator as a new intent 'claim_verification' round. The retrieved evidence is then scored per-claim using token overlap against claim text (vs. question text in the current pipeline), and organized into per-claim dossiers. The final synthesis prompt is replaced by a structured claim-adjudication prompt that iterates over each claim, presenting its support and refutation evidence, and requiring explicit CONFIRMED / REVISED / CONTRADICTED / UNCERTAIN verdicts with brief rationale. This creates a structured proof-or-refute loop that forces the synthesizer to reconcile its parametric priors with retrieved evidence, rather than free-associating from a pile of evidence bullets.",
  "reasoning": "1. SPARKIT's current relevance scoring (_record_relevance_score in engine.py:121, _relevance_score in aggregator.py:78) is pure token overlap between query text and title/abstract. For hard scientific questions, the question text contains general framing terms while the critical papers use answer-specific technical vocabulary. A question like 'What is the dissociation constant of protein X binding ligand Y?' will tokenize to {'dissociation', 'constant', 'protein', 'binding', 'ligand'} — but the key paper's title might be 'Thermodynamic characterization of X-Y complex formation: Kd of 3.2 nM' — which has low overlap with the question tokens but perfect overlap with the speculative claim tokens ('Kd', '3.2', 'nM', 'complex'). Claim-conditional queries recover these papers. 2. SPARKIT's synthesis prompt (_build_synthesis_prompt in engine.py:993) passes claim_texts[:8] as free-form evidence bullets with no structure forcing the model to confront conflicting evidence. When retrieved evidence partially contradicts the model's prior, the model can ignore the conflicting evidence and synthesize a fluent but incorrect answer. The claim-adjudication prompt forces explicit CONFIRMED/CONTRADICTED verdicts per claim, preventing this selective attention failure. 3. MCQ option-hypothesis queries (_build_option_hypothesis_queries, engine.py:479) already implement a weaker version of this insight for multiple-choice — generating per-option queries. SCAR-PR extends the same claim-targeted retrieval logic to free-text questions and to both support and refutation directions, unifying the retrieval strategy across question types. 4. The current verifier (run_verifier) uses heuristic marker-counting (8 contradiction markers) on adversarial documents, not claim-specific evidence. By explicitly retrieving refutation evidence per claim, SCAR-PR replaces the heuristic verifier with evidence-grounded claim-level contradiction detection, fixing a known weakness documented in the audit summary. 5. Speculative pre-answers are already implicitly generated by the model during synthesis — SCAR-PR makes this latent computation explicit and uses it to drive targeted retrieval before the final answer is committed, recovering the marginal value of that computation rather than discarding it.",
  "expected_impact": {
    "accuracy_delta_pct_points": 5,
    "cost_impact": "increase",
    "latency_impact": "increase",
    "confidence": "medium"
  },
  "implementation_plan": [
    {
      "step": "Add SPARKIT_ENABLE_SCAR env var (default 0) and read it in execute_orchestration() alongside existing adaptive retrieval flags. Gate all new SCAR code paths on this flag so the baseline is unaffected.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Add _generate_speculative_claims(question, claim_texts, provider, max_tokens=280) -> list[str] function that prompts the provider with: 'Based on the question and retrieved evidence, write 3-5 specific, falsifiable scientific claims that would constitute a correct answer. Format each as CLAIM_N: [assertion]. Be precise — include specific values, mechanisms, or findings if known.' Parse CLAIM_N: lines with regex fallback to sentence-split. Filter claims shorter than 12 tokens or longer than 80 tokens.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _claims_to_probe_queries(claims: list[str]) -> list[str] that for each claim generates two queries: f'evidence that {claim}' (support probe) and f'contradicting evidence {claim}' (refutation probe). Run through existing _dedupe_queries() with max_items=12. Add 'claim_verification' as a new key in RetrievalPlan.intent_queries dict.",
      "owner": "orchestrator/engine.py",
      "effort": "low"
    },
    {
      "step": "Insert SCAR retrieval sub-round in execute_orchestration() between retrieval_round_1 and retrieval_round_2_gap_fill: (1) call _generate_speculative_claims() using provider_plan.synthesis and the round-1 claim_texts, (2) call _claims_to_probe_queries() to build probe queries, (3) call search_literature() with the probe queries as a new 'retrieval_round_1.5_claim_probes' round, (4) append to all_records with existing deduplication logic. Store speculative_claims in plan stage artifacts for traceability.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _score_claim_evidence(claim: str, claim_texts: list[str]) -> float that computes token overlap between claim tokens and evidence tokens (reusing _tokenize()), then add _build_per_claim_dossiers(speculative_claims, claim_texts, section_summaries, top_k=4) -> list[dict] that for each speculative claim collects its top_k supporting and top_k contradicting evidence snippets using the existing _build_option_dossiers() logic adapted for free-text claims.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add _build_claim_adjudication_prompt(question, speculative_claims, per_claim_dossiers, claim_texts) -> str as a new synthesis prompt variant. Format: system='You are a strict scientific claim adjudicator. For each claim, review its support and refutation evidence, then emit a verdict: CONFIRMED, REVISED [new text], CONTRADICTED, or UNCERTAIN. Then write a final synthesized answer based on the verdicts.' Structure: claim block with dossier snippets, general evidence bullets, final answer request. Used only when SPARKIT_ENABLE_SCAR=1.",
      "owner": "orchestrator/engine.py",
      "effort": "medium"
    },
    {
      "step": "Add claim_adjudication_score to CalibrationFeatures: fraction of speculative claims marked CONFIRMED or REVISED (vs CONTRADICTED or UNCERTAIN). Update calibrate_answer() to use this as an additional feature with weight 0.10, reducing provider_config_ratio weight from 0.15 to 0.05 to maintain the same raw confidence range.",
      "owner": "orchestrator/calibration.py",
      "effort": "low"
    },
    {
      "step": "Add 'retrieval_round_claim_probes' stage to observability with: speculative_claim_count, probe_query_count, new_unique_docs_from_probes, avg_claim_evidence_alignment_score. This enables A/B analysis of whether probe rounds contribute meaningfully.",
      "owner": "orchestrator/observability.py",
      "effort": "low"
    },
    {
      "step": "Add unit tests in test_synthesis_quality.py for: _generate_speculative_claims() parsing (valid CLAIM_N format, fallback to sentence split, length filtering), _claims_to_probe_queries() (correct support+refutation pair generation, dedup), _build_per_claim_dossiers() (correct snippet assignment by token overlap to claim vs other claims), _build_claim_adjudication_prompt() (contains all claims, dossiers, verdict instructions).",
      "owner": "orchestrator/tests/test_synthesis_quality.py",
      "effort": "medium"
    },
    {
      "step": "Run HLE-25 gold subset with SPARKIT_ENABLE_SCAR=1 against same questions as existing hle25_direct_openai_gpt52 baseline. Compare accuracy, abstain rate, ECE, Brier score, and cost. Publish manifest in benchmarks/results/hle25_scar_pr_v1_<timestamp>/.",
      "owner": "benchmarks",
      "effort": "low"
    }
  ],
  "retrieval_improvements": [
    "Claim-targeted support probes: after round-1, generate 'evidence that [CLAIM_N]' queries for each speculative claim. These queries use answer-domain terminology (specific values, mechanisms, compound names) rather than question-stem terminology, recovering papers that current question-driven queries miss due to vocabulary mismatch between question and answer-bearing documents.",
    "Claim-targeted refutation probes: generate 'contradicting evidence [CLAIM_N]' queries for each speculative claim, dispatched through the same search_literature() aggregator. This replaces the current heuristic verifier (8 marker words) with genuinely adversarial retrieval targeted at the specific claims being made, surfacing actual contradicting papers rather than any paper with the word 'contradictory' in it.",
    "Per-claim evidence alignment reranking: after probe retrieval, rerank the probe-round records by token overlap between each record's title+abstract and the specific claim text (instead of the question text as in the current _record_relevance_score()). This shifts the relevance signal from question-relevance to claim-relevance, which is the correct target for verification retrieval.",
    "PubMed E-utilities as a new retrieval adapter (search_pubmed): add a new adapter in services/retrieval_service/app/adapters.py calling https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi + efetch.fcgi (free, no API key required). Register it in the aggregator.py adapter list with domain filter: activate when question tokens include any of {'enzyme', 'protein', 'gene', 'cell', 'clinical', 'drug', 'receptor', 'pathway', 'molecular', 'biochemical', 'pharmacological', 'metabolism'}. PubMed covers 37M+ biomedical abstracts including chemistry, biology, and pharmacology — the dominant domains in HLE bio/chem questions.",
    "Extended synthesis context from section chunks: the current _build_section_summaries() (engine.py:942) extracts only _first_sentence() (max 220 chars) per section bucket, discarding numerical values and findings that appear mid-section. When SCAR is active, replace the first-sentence extraction with the full best_text chunk from _select_best_section_chunk() (already computed at ingestion time, stored in ClaimEvidence.section_text), capped at 400 chars rather than 220. Pass these extended summaries to _build_claim_adjudication_prompt() instead of first-sentence summaries."
  ],
  "evaluation_plan": [
    "Accuracy benchmark: run HLE-25 gold questions with SCAR enabled vs disabled on identical provider config (openai/gpt-5.2, single mode). Compare rubric_score distributions. Expected improvement is concentrated on free-text questions where speculative claims are specific; MCQ questions should show neutral-to-slight improvement since option-hypothesis queries already provide partial claim targeting.",
    "Claim retrieval precision metric: for each SCAR run, compute the fraction of probe-round documents that contain at least one token from the corresponding speculative claim (beyond shared question tokens). This measures whether probe queries actually retrieved claim-relevant evidence vs coincidental matches. Target: >60% claim-token overlap in probe documents vs ~30% baseline for gap_fill documents.",
    "Calibration quality: compare ECE and Brier scores between SCAR and baseline on HLE-25. Explicit claim adjudication (CONFIRMED/CONTRADICTED/UNCERTAIN verdicts) should reduce overconfidence on wrong answers because the synthesizer is forced to acknowledge contradicted claims, yielding lower confidence on those questions.",
    "Novelty of probe rounds: track new_unique_docs_from_probes in observability stage artifacts. If probe queries recover <2 new unique documents on average, the retrieval improvement is negligible and the overhead is not justified. Target: >3 new unique docs per question on average.",
    "Speculative claim calibration: after graded runs, compute what fraction of speculative claims in pre-answers were directionally correct vs the final graded answer. This reveals whether the speculative pre-answer quality is sufficient to generate useful probe queries. If >50% of speculative claims are directionally correct on graded-correct questions and <30% on graded-wrong questions, the speculative step is discriminative.",
    "Cost/latency overhead: measure mean added cost (1 extra LLM call at 280 tokens + N claim-probe retrieval queries) and mean added latency per question. SCAR is justified only if accuracy_delta_pct_points / cost_overhead_ratio > baseline cost efficiency. Report as cost-per-correct-answer for SCAR vs baseline."
  ],
  "risks": [
    "Cascading hallucination: if the speculative pre-answer contains confidently wrong claims, support probes may retrieve papers that appear supportive via surface lexical overlap (e.g., a paper about a different enzyme with similar Kd values), reinforcing the wrong answer. Mitigation: always dispatch both support AND refutation probes; require the adjudication prompt to explicitly consider contradicting evidence before marking CONFIRMED; retain the existing adversarial round (retrieval_round_3_adversarial) unchanged.",
    "Speculative claim degeneracy: for questions where the model has very low parametric knowledge, the speculative pre-answer may produce vague or question-restatement claims ('CLAIM_1: The answer depends on the conditions') rather than specific falsifiable assertions. Vague claims generate unfocused probe queries with no benefit over existing gap_fill queries. Mitigation: add claim specificity filter (reject claims with fewer than 4 non-stopword tokens or claims that are substring subsets of the original question); fall back to standard gap_fill round when fewer than 2 valid claims are extracted.",
    "Cost amplification: each question adds 1 LLM call (speculative pre-answer, 280 out-tokens) plus N×2 retrieval queries (N=3-5 claims, 2=support+refute = 6-10 extra queries). At current Brave pricing ($0.005/request) and provider costs, this is approximately $0.05-0.15 additional cost per question on top of the current ~$1-3 per question. For bulk HLE benchmarking at 149 questions, this adds $7-22 total. Acceptable for research runs but should be flagged in cost accounting.",
    "Sequential latency bottleneck: the speculative pre-answer call must complete before probe queries are dispatched, adding one full LLM round-trip (typically 5-15s for gpt-5.2 at 280 tokens) before the claim_probes retrieval round can begin. Unlike the existing parallel retrieval rounds, this is inherently sequential. Total added latency: 10-25s per question. Mitigation: run the speculative pre-answer call in parallel with retrieval_round_option_hypotheses (for MCQ) or retrieval_round_2_gap_fill (for free-text) using Python asyncio, so the latency only adds time equal to max(speculative_call_latency, gap_fill_latency) - gap_fill_latency.",
    "Adjudication prompt complexity regression: the new claim-adjudication synthesis prompt is significantly more structured than the current _build_synthesis_prompt() (engine.py:993). Highly structured prompts can cause some models (especially smaller/faster ones) to produce malformed outputs that fall back to the deterministic answer synthesis. Mitigation: implement robust fallback — if the adjudication prompt fails to produce at least one CONFIRMED/REVISED/CONTRADICTED/UNCERTAIN verdict, fall back to the existing _build_synthesis_prompt() unchanged.",
    "Verdict extraction brittleness for MCQ: when SCAR is active on MCQ questions, the claim-adjudication prompt output may conflict with the existing MCQ option-scoring path (_parse_mcq_option_scores, engine.py:278). Need to ensure SCAR is applied only to the free-text synthesis path and that MCQ questions continue to use the option-elimination + scoring flow unchanged, with SCAR speculative claims used only to augment the retrieval phase (claim_probes round) and not to replace the MCQ adjudication logic."
  ]
}
```
